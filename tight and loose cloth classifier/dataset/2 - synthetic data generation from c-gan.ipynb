{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a990f18-032f-42f3-84a5-2739271f32a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to S3...\n",
      "Connected to S3\n",
      "Using device: cuda\n",
      "Loading images from S3...\n",
      "Total images loaded: 100\n",
      "Dataset ready\n",
      "Models initialized\n",
      "Starting DCGAN training...\n",
      "Epoch [1/300] Batch [0/7] D Loss: 1.4071 G Loss: 3.3136\n",
      "Epoch [1/300] Batch [5/7] D Loss: 2.4201 G Loss: 2.3094\n",
      "Epoch 1 completed\n",
      "Epoch [2/300] Batch [0/7] D Loss: 1.8791 G Loss: 2.7638\n",
      "Epoch [2/300] Batch [5/7] D Loss: 1.8761 G Loss: 2.5804\n",
      "Epoch 2 completed\n",
      "Epoch [3/300] Batch [0/7] D Loss: 1.2539 G Loss: 2.8439\n",
      "Epoch [3/300] Batch [5/7] D Loss: 1.2673 G Loss: 3.0392\n",
      "Epoch 3 completed\n",
      "Epoch [4/300] Batch [0/7] D Loss: 1.0817 G Loss: 2.7394\n",
      "Epoch [4/300] Batch [5/7] D Loss: 1.2848 G Loss: 2.6787\n",
      "Epoch 4 completed\n",
      "Epoch [5/300] Batch [0/7] D Loss: 1.1077 G Loss: 2.5044\n",
      "Epoch [5/300] Batch [5/7] D Loss: 1.4107 G Loss: 2.9071\n",
      "Epoch 5 completed\n",
      "Epoch [6/300] Batch [0/7] D Loss: 1.5348 G Loss: 2.1644\n",
      "Epoch [6/300] Batch [5/7] D Loss: 1.4229 G Loss: 2.1121\n",
      "Epoch 6 completed\n",
      "Epoch [7/300] Batch [0/7] D Loss: 1.3412 G Loss: 2.5834\n",
      "Epoch [7/300] Batch [5/7] D Loss: 1.1456 G Loss: 2.8035\n",
      "Epoch 7 completed\n",
      "Epoch [8/300] Batch [0/7] D Loss: 1.1925 G Loss: 2.4099\n",
      "Epoch [8/300] Batch [5/7] D Loss: 1.7160 G Loss: 3.1225\n",
      "Epoch 8 completed\n",
      "Epoch [9/300] Batch [0/7] D Loss: 1.6533 G Loss: 2.6801\n",
      "Epoch [9/300] Batch [5/7] D Loss: 1.0046 G Loss: 3.1527\n",
      "Epoch 9 completed\n",
      "Epoch [10/300] Batch [0/7] D Loss: 1.8965 G Loss: 4.1828\n",
      "Epoch [10/300] Batch [5/7] D Loss: 1.5230 G Loss: 3.1257\n",
      "Epoch 10 completed\n",
      "Epoch [11/300] Batch [0/7] D Loss: 1.3849 G Loss: 3.1244\n",
      "Epoch [11/300] Batch [5/7] D Loss: 1.3770 G Loss: 3.7223\n",
      "Epoch 11 completed\n",
      "Epoch [12/300] Batch [0/7] D Loss: 1.1924 G Loss: 2.9103\n",
      "Epoch [12/300] Batch [5/7] D Loss: 1.5236 G Loss: 2.6331\n",
      "Epoch 12 completed\n",
      "Epoch [13/300] Batch [0/7] D Loss: 1.2200 G Loss: 3.2125\n",
      "Epoch [13/300] Batch [5/7] D Loss: 1.6970 G Loss: 2.6870\n",
      "Epoch 13 completed\n",
      "Epoch [14/300] Batch [0/7] D Loss: 1.1555 G Loss: 2.9150\n",
      "Epoch [14/300] Batch [5/7] D Loss: 1.6537 G Loss: 2.8865\n",
      "Epoch 14 completed\n",
      "Epoch [15/300] Batch [0/7] D Loss: 1.6032 G Loss: 2.4687\n",
      "Epoch [15/300] Batch [5/7] D Loss: 1.2224 G Loss: 2.2145\n",
      "Epoch 15 completed\n",
      "Epoch [16/300] Batch [0/7] D Loss: 1.4904 G Loss: 2.1425\n",
      "Epoch [16/300] Batch [5/7] D Loss: 1.1189 G Loss: 2.3200\n",
      "Epoch 16 completed\n",
      "Epoch [17/300] Batch [0/7] D Loss: 0.8823 G Loss: 2.6037\n",
      "Epoch [17/300] Batch [5/7] D Loss: 1.2202 G Loss: 2.8595\n",
      "Epoch 17 completed\n",
      "Epoch [18/300] Batch [0/7] D Loss: 1.2275 G Loss: 2.5617\n",
      "Epoch [18/300] Batch [5/7] D Loss: 1.1778 G Loss: 2.4555\n",
      "Epoch 18 completed\n",
      "Epoch [19/300] Batch [0/7] D Loss: 0.8402 G Loss: 2.5421\n",
      "Epoch [19/300] Batch [5/7] D Loss: 1.3374 G Loss: 2.1507\n",
      "Epoch 19 completed\n",
      "Epoch [20/300] Batch [0/7] D Loss: 1.1372 G Loss: 2.8813\n",
      "Epoch [20/300] Batch [5/7] D Loss: 0.7352 G Loss: 2.7147\n",
      "Epoch 20 completed\n",
      "Epoch [21/300] Batch [0/7] D Loss: 1.6030 G Loss: 2.5314\n",
      "Epoch [21/300] Batch [5/7] D Loss: 0.9912 G Loss: 2.8193\n",
      "Epoch 21 completed\n",
      "Epoch [22/300] Batch [0/7] D Loss: 1.1592 G Loss: 3.0740\n",
      "Epoch [22/300] Batch [5/7] D Loss: 1.0232 G Loss: 3.0190\n",
      "Epoch 22 completed\n",
      "Epoch [23/300] Batch [0/7] D Loss: 1.5639 G Loss: 2.8550\n",
      "Epoch [23/300] Batch [5/7] D Loss: 1.2327 G Loss: 2.5031\n",
      "Epoch 23 completed\n",
      "Epoch [24/300] Batch [0/7] D Loss: 1.2139 G Loss: 2.7559\n",
      "Epoch [24/300] Batch [5/7] D Loss: 0.9539 G Loss: 2.6507\n",
      "Epoch 24 completed\n",
      "Epoch [25/300] Batch [0/7] D Loss: 0.7438 G Loss: 2.9294\n",
      "Epoch [25/300] Batch [5/7] D Loss: 0.9237 G Loss: 2.9340\n",
      "Epoch 25 completed\n",
      "Epoch [26/300] Batch [0/7] D Loss: 0.9473 G Loss: 2.8972\n",
      "Epoch [26/300] Batch [5/7] D Loss: 0.9881 G Loss: 2.5567\n",
      "Epoch 26 completed\n",
      "Epoch [27/300] Batch [0/7] D Loss: 1.5341 G Loss: 2.7479\n",
      "Epoch [27/300] Batch [5/7] D Loss: 1.0312 G Loss: 2.3545\n",
      "Epoch 27 completed\n",
      "Epoch [28/300] Batch [0/7] D Loss: 0.9883 G Loss: 2.7500\n",
      "Epoch [28/300] Batch [5/7] D Loss: 1.2244 G Loss: 2.4756\n",
      "Epoch 28 completed\n",
      "Epoch [29/300] Batch [0/7] D Loss: 1.6209 G Loss: 1.9571\n",
      "Epoch [29/300] Batch [5/7] D Loss: 0.9024 G Loss: 2.5807\n",
      "Epoch 29 completed\n",
      "Epoch [30/300] Batch [0/7] D Loss: 1.0143 G Loss: 2.7927\n",
      "Epoch [30/300] Batch [5/7] D Loss: 0.8893 G Loss: 2.4746\n",
      "Epoch 30 completed\n",
      "Epoch [31/300] Batch [0/7] D Loss: 1.1585 G Loss: 3.7104\n",
      "Epoch [31/300] Batch [5/7] D Loss: 1.0379 G Loss: 2.4835\n",
      "Epoch 31 completed\n",
      "Epoch [32/300] Batch [0/7] D Loss: 1.5469 G Loss: 3.8833\n",
      "Epoch [32/300] Batch [5/7] D Loss: 0.9836 G Loss: 2.3424\n",
      "Epoch 32 completed\n",
      "Epoch [33/300] Batch [0/7] D Loss: 1.0727 G Loss: 2.1438\n",
      "Epoch [33/300] Batch [5/7] D Loss: 0.9674 G Loss: 2.3123\n",
      "Epoch 33 completed\n",
      "Epoch [34/300] Batch [0/7] D Loss: 0.9794 G Loss: 2.1021\n",
      "Epoch [34/300] Batch [5/7] D Loss: 1.1134 G Loss: 1.8430\n",
      "Epoch 34 completed\n",
      "Epoch [35/300] Batch [0/7] D Loss: 0.8125 G Loss: 2.5001\n",
      "Epoch [35/300] Batch [5/7] D Loss: 0.7620 G Loss: 3.0091\n",
      "Epoch 35 completed\n",
      "Epoch [36/300] Batch [0/7] D Loss: 0.9097 G Loss: 2.5156\n",
      "Epoch [36/300] Batch [5/7] D Loss: 0.9376 G Loss: 3.0985\n",
      "Epoch 36 completed\n",
      "Epoch [37/300] Batch [0/7] D Loss: 0.7866 G Loss: 3.0638\n",
      "Epoch [37/300] Batch [5/7] D Loss: 1.1368 G Loss: 3.1751\n",
      "Epoch 37 completed\n",
      "Epoch [38/300] Batch [0/7] D Loss: 0.9954 G Loss: 3.1127\n",
      "Epoch [38/300] Batch [5/7] D Loss: 0.8224 G Loss: 2.5851\n",
      "Epoch 38 completed\n",
      "Epoch [39/300] Batch [0/7] D Loss: 0.8047 G Loss: 2.9696\n",
      "Epoch [39/300] Batch [5/7] D Loss: 0.7988 G Loss: 2.9842\n",
      "Epoch 39 completed\n",
      "Epoch [40/300] Batch [0/7] D Loss: 1.2483 G Loss: 3.0575\n",
      "Epoch [40/300] Batch [5/7] D Loss: 1.4119 G Loss: 1.5488\n",
      "Epoch 40 completed\n",
      "Epoch [41/300] Batch [0/7] D Loss: 0.7421 G Loss: 2.8492\n",
      "Epoch [41/300] Batch [5/7] D Loss: 1.0873 G Loss: 2.1821\n",
      "Epoch 41 completed\n",
      "Epoch [42/300] Batch [0/7] D Loss: 1.1485 G Loss: 2.3625\n",
      "Epoch [42/300] Batch [5/7] D Loss: 0.9124 G Loss: 2.2334\n",
      "Epoch 42 completed\n",
      "Epoch [43/300] Batch [0/7] D Loss: 0.7817 G Loss: 2.8928\n",
      "Epoch [43/300] Batch [5/7] D Loss: 0.7449 G Loss: 3.0775\n",
      "Epoch 43 completed\n",
      "Epoch [44/300] Batch [0/7] D Loss: 0.6110 G Loss: 3.2977\n",
      "Epoch [44/300] Batch [5/7] D Loss: 1.0922 G Loss: 3.1512\n",
      "Epoch 44 completed\n",
      "Epoch [45/300] Batch [0/7] D Loss: 0.9462 G Loss: 3.5438\n",
      "Epoch [45/300] Batch [5/7] D Loss: 0.8160 G Loss: 3.0352\n",
      "Epoch 45 completed\n",
      "Epoch [46/300] Batch [0/7] D Loss: 1.2964 G Loss: 2.4402\n",
      "Epoch [46/300] Batch [5/7] D Loss: 1.2336 G Loss: 2.8152\n",
      "Epoch 46 completed\n",
      "Epoch [47/300] Batch [0/7] D Loss: 0.9486 G Loss: 3.1050\n",
      "Epoch [47/300] Batch [5/7] D Loss: 0.7854 G Loss: 2.5854\n",
      "Epoch 47 completed\n",
      "Epoch [48/300] Batch [0/7] D Loss: 0.8665 G Loss: 2.3588\n",
      "Epoch [48/300] Batch [5/7] D Loss: 0.8611 G Loss: 1.9730\n",
      "Epoch 48 completed\n",
      "Epoch [49/300] Batch [0/7] D Loss: 0.8406 G Loss: 3.5713\n",
      "Epoch [49/300] Batch [5/7] D Loss: 1.5219 G Loss: 3.3905\n",
      "Epoch 49 completed\n",
      "Epoch [50/300] Batch [0/7] D Loss: 1.0339 G Loss: 2.9696\n",
      "Epoch [50/300] Batch [5/7] D Loss: 0.8293 G Loss: 3.4642\n",
      "Epoch 50 completed\n",
      "Epoch [51/300] Batch [0/7] D Loss: 0.8015 G Loss: 3.1984\n",
      "Epoch [51/300] Batch [5/7] D Loss: 0.6774 G Loss: 3.6259\n",
      "Epoch 51 completed\n",
      "Epoch [52/300] Batch [0/7] D Loss: 0.7417 G Loss: 2.8216\n",
      "Epoch [52/300] Batch [5/7] D Loss: 1.1751 G Loss: 2.7122\n",
      "Epoch 52 completed\n",
      "Epoch [53/300] Batch [0/7] D Loss: 0.7009 G Loss: 3.2088\n",
      "Epoch [53/300] Batch [5/7] D Loss: 1.0174 G Loss: 1.9750\n",
      "Epoch 53 completed\n",
      "Epoch [54/300] Batch [0/7] D Loss: 0.8233 G Loss: 3.1847\n",
      "Epoch [54/300] Batch [5/7] D Loss: 0.7178 G Loss: 2.8079\n",
      "Epoch 54 completed\n",
      "Epoch [55/300] Batch [0/7] D Loss: 0.6447 G Loss: 2.6088\n",
      "Epoch [55/300] Batch [5/7] D Loss: 0.7274 G Loss: 2.5504\n",
      "Epoch 55 completed\n",
      "Epoch [56/300] Batch [0/7] D Loss: 0.4666 G Loss: 3.5490\n",
      "Epoch [56/300] Batch [5/7] D Loss: 1.2782 G Loss: 2.3014\n",
      "Epoch 56 completed\n",
      "Epoch [57/300] Batch [0/7] D Loss: 1.1666 G Loss: 3.5797\n",
      "Epoch [57/300] Batch [5/7] D Loss: 1.0004 G Loss: 2.4268\n",
      "Epoch 57 completed\n",
      "Epoch [58/300] Batch [0/7] D Loss: 0.9693 G Loss: 3.1886\n",
      "Epoch [58/300] Batch [5/7] D Loss: 0.9437 G Loss: 2.7861\n",
      "Epoch 58 completed\n",
      "Epoch [59/300] Batch [0/7] D Loss: 0.9614 G Loss: 2.4268\n",
      "Epoch [59/300] Batch [5/7] D Loss: 0.7153 G Loss: 2.2602\n",
      "Epoch 59 completed\n",
      "Epoch [60/300] Batch [0/7] D Loss: 0.8735 G Loss: 2.2200\n",
      "Epoch [60/300] Batch [5/7] D Loss: 0.8658 G Loss: 3.0206\n",
      "Epoch 60 completed\n",
      "Epoch [61/300] Batch [0/7] D Loss: 0.6603 G Loss: 3.6557\n",
      "Epoch [61/300] Batch [5/7] D Loss: 0.7067 G Loss: 2.1798\n",
      "Epoch 61 completed\n",
      "Epoch [62/300] Batch [0/7] D Loss: 1.2118 G Loss: 3.6302\n",
      "Epoch [62/300] Batch [5/7] D Loss: 0.7502 G Loss: 3.0408\n",
      "Epoch 62 completed\n",
      "Epoch [63/300] Batch [0/7] D Loss: 0.9435 G Loss: 3.2433\n",
      "Epoch [63/300] Batch [5/7] D Loss: 0.9396 G Loss: 2.6651\n",
      "Epoch 63 completed\n",
      "Epoch [64/300] Batch [0/7] D Loss: 1.1698 G Loss: 2.4087\n",
      "Epoch [64/300] Batch [5/7] D Loss: 0.5978 G Loss: 3.3352\n",
      "Epoch 64 completed\n",
      "Epoch [65/300] Batch [0/7] D Loss: 0.9519 G Loss: 3.0898\n",
      "Epoch [65/300] Batch [5/7] D Loss: 0.9196 G Loss: 2.6524\n",
      "Epoch 65 completed\n",
      "Epoch [66/300] Batch [0/7] D Loss: 2.2253 G Loss: 4.1976\n",
      "Epoch [66/300] Batch [5/7] D Loss: 1.2265 G Loss: 2.9266\n",
      "Epoch 66 completed\n",
      "Epoch [67/300] Batch [0/7] D Loss: 1.0246 G Loss: 2.4087\n",
      "Epoch [67/300] Batch [5/7] D Loss: 0.7552 G Loss: 2.4340\n",
      "Epoch 67 completed\n",
      "Epoch [68/300] Batch [0/7] D Loss: 0.9569 G Loss: 2.6627\n",
      "Epoch [68/300] Batch [5/7] D Loss: 0.8473 G Loss: 3.1018\n",
      "Epoch 68 completed\n",
      "Epoch [69/300] Batch [0/7] D Loss: 0.7636 G Loss: 3.3747\n",
      "Epoch [69/300] Batch [5/7] D Loss: 0.9406 G Loss: 3.3011\n",
      "Epoch 69 completed\n",
      "Epoch [70/300] Batch [0/7] D Loss: 0.6130 G Loss: 2.9701\n",
      "Epoch [70/300] Batch [5/7] D Loss: 0.6425 G Loss: 2.6364\n",
      "Epoch 70 completed\n",
      "Epoch [71/300] Batch [0/7] D Loss: 0.9465 G Loss: 3.0436\n",
      "Epoch [71/300] Batch [5/7] D Loss: 0.9866 G Loss: 3.5187\n",
      "Epoch 71 completed\n",
      "Epoch [72/300] Batch [0/7] D Loss: 1.0188 G Loss: 2.8892\n",
      "Epoch [72/300] Batch [5/7] D Loss: 0.5648 G Loss: 3.0566\n",
      "Epoch 72 completed\n",
      "Epoch [73/300] Batch [0/7] D Loss: 0.7074 G Loss: 3.9604\n",
      "Epoch [73/300] Batch [5/7] D Loss: 0.7320 G Loss: 3.0712\n",
      "Epoch 73 completed\n",
      "Epoch [74/300] Batch [0/7] D Loss: 0.9263 G Loss: 3.3163\n",
      "Epoch [74/300] Batch [5/7] D Loss: 0.8224 G Loss: 2.7502\n",
      "Epoch 74 completed\n",
      "Epoch [75/300] Batch [0/7] D Loss: 1.2366 G Loss: 3.4454\n",
      "Epoch [75/300] Batch [5/7] D Loss: 0.6090 G Loss: 3.3679\n",
      "Epoch 75 completed\n",
      "Epoch [76/300] Batch [0/7] D Loss: 0.6730 G Loss: 2.7893\n",
      "Epoch [76/300] Batch [5/7] D Loss: 0.7043 G Loss: 3.3435\n",
      "Epoch 76 completed\n",
      "Epoch [77/300] Batch [0/7] D Loss: 0.7989 G Loss: 2.9827\n",
      "Epoch [77/300] Batch [5/7] D Loss: 0.9392 G Loss: 2.6238\n",
      "Epoch 77 completed\n",
      "Epoch [78/300] Batch [0/7] D Loss: 0.6879 G Loss: 3.1722\n",
      "Epoch [78/300] Batch [5/7] D Loss: 0.6905 G Loss: 2.7794\n",
      "Epoch 78 completed\n",
      "Epoch [79/300] Batch [0/7] D Loss: 0.7312 G Loss: 2.6703\n",
      "Epoch [79/300] Batch [5/7] D Loss: 0.9821 G Loss: 3.0682\n",
      "Epoch 79 completed\n",
      "Epoch [80/300] Batch [0/7] D Loss: 0.8142 G Loss: 2.9738\n",
      "Epoch [80/300] Batch [5/7] D Loss: 0.9208 G Loss: 2.6031\n",
      "Epoch 80 completed\n",
      "Epoch [81/300] Batch [0/7] D Loss: 0.7092 G Loss: 3.2250\n",
      "Epoch [81/300] Batch [5/7] D Loss: 0.5747 G Loss: 3.2750\n",
      "Epoch 81 completed\n",
      "Epoch [82/300] Batch [0/7] D Loss: 1.2237 G Loss: 3.8593\n",
      "Epoch [82/300] Batch [5/7] D Loss: 0.7132 G Loss: 3.0518\n",
      "Epoch 82 completed\n",
      "Epoch [83/300] Batch [0/7] D Loss: 1.2541 G Loss: 3.9944\n",
      "Epoch [83/300] Batch [5/7] D Loss: 0.8910 G Loss: 2.9964\n",
      "Epoch 83 completed\n",
      "Epoch [84/300] Batch [0/7] D Loss: 0.8257 G Loss: 3.4381\n",
      "Epoch [84/300] Batch [5/7] D Loss: 0.5464 G Loss: 4.2874\n",
      "Epoch 84 completed\n",
      "Epoch [85/300] Batch [0/7] D Loss: 0.5905 G Loss: 2.9309\n",
      "Epoch [85/300] Batch [5/7] D Loss: 0.7561 G Loss: 2.8414\n",
      "Epoch 85 completed\n",
      "Epoch [86/300] Batch [0/7] D Loss: 0.8363 G Loss: 2.4903\n",
      "Epoch [86/300] Batch [5/7] D Loss: 0.9018 G Loss: 2.1618\n",
      "Epoch 86 completed\n",
      "Epoch [87/300] Batch [0/7] D Loss: 0.4875 G Loss: 3.3402\n",
      "Epoch [87/300] Batch [5/7] D Loss: 0.8397 G Loss: 2.5127\n",
      "Epoch 87 completed\n",
      "Epoch [88/300] Batch [0/7] D Loss: 0.5947 G Loss: 3.2182\n",
      "Epoch [88/300] Batch [5/7] D Loss: 0.8008 G Loss: 2.5660\n",
      "Epoch 88 completed\n",
      "Epoch [89/300] Batch [0/7] D Loss: 0.6301 G Loss: 3.6731\n",
      "Epoch [89/300] Batch [5/7] D Loss: 0.7553 G Loss: 2.8200\n",
      "Epoch 89 completed\n",
      "Epoch [90/300] Batch [0/7] D Loss: 0.8264 G Loss: 2.5290\n",
      "Epoch [90/300] Batch [5/7] D Loss: 0.8683 G Loss: 2.2657\n",
      "Epoch 90 completed\n",
      "Epoch [91/300] Batch [0/7] D Loss: 0.8056 G Loss: 2.6022\n",
      "Epoch [91/300] Batch [5/7] D Loss: 0.5684 G Loss: 2.9292\n",
      "Epoch 91 completed\n",
      "Epoch [92/300] Batch [0/7] D Loss: 0.6509 G Loss: 3.2714\n",
      "Epoch [92/300] Batch [5/7] D Loss: 0.4777 G Loss: 3.4118\n",
      "Epoch 92 completed\n",
      "Epoch [93/300] Batch [0/7] D Loss: 0.6456 G Loss: 2.6967\n",
      "Epoch [93/300] Batch [5/7] D Loss: 0.6386 G Loss: 2.7195\n",
      "Epoch 93 completed\n",
      "Epoch [94/300] Batch [0/7] D Loss: 0.5428 G Loss: 3.1774\n",
      "Epoch [94/300] Batch [5/7] D Loss: 0.6674 G Loss: 3.4330\n",
      "Epoch 94 completed\n",
      "Epoch [95/300] Batch [0/7] D Loss: 0.6920 G Loss: 3.4722\n",
      "Epoch [95/300] Batch [5/7] D Loss: 0.6313 G Loss: 3.2161\n",
      "Epoch 95 completed\n",
      "Epoch [96/300] Batch [0/7] D Loss: 1.0542 G Loss: 2.5233\n",
      "Epoch [96/300] Batch [5/7] D Loss: 0.8052 G Loss: 3.0376\n",
      "Epoch 96 completed\n",
      "Epoch [97/300] Batch [0/7] D Loss: 1.0278 G Loss: 2.8367\n",
      "Epoch [97/300] Batch [5/7] D Loss: 0.8402 G Loss: 2.8335\n",
      "Epoch 97 completed\n",
      "Epoch [98/300] Batch [0/7] D Loss: 0.7149 G Loss: 2.5564\n",
      "Epoch [98/300] Batch [5/7] D Loss: 1.2865 G Loss: 5.6201\n",
      "Epoch 98 completed\n",
      "Epoch [99/300] Batch [0/7] D Loss: 0.8799 G Loss: 3.3497\n",
      "Epoch [99/300] Batch [5/7] D Loss: 0.6802 G Loss: 3.0071\n",
      "Epoch 99 completed\n",
      "Epoch [100/300] Batch [0/7] D Loss: 1.2202 G Loss: 3.4367\n",
      "Epoch [100/300] Batch [5/7] D Loss: 1.1419 G Loss: 2.3690\n",
      "Epoch 100 completed\n",
      "Epoch [101/300] Batch [0/7] D Loss: 0.7201 G Loss: 3.5266\n",
      "Epoch [101/300] Batch [5/7] D Loss: 0.7118 G Loss: 2.7529\n",
      "Epoch 101 completed\n",
      "Epoch [102/300] Batch [0/7] D Loss: 0.6036 G Loss: 2.8349\n",
      "Epoch [102/300] Batch [5/7] D Loss: 0.8063 G Loss: 2.4658\n",
      "Epoch 102 completed\n",
      "Epoch [103/300] Batch [0/7] D Loss: 0.6321 G Loss: 2.5842\n",
      "Epoch [103/300] Batch [5/7] D Loss: 0.6464 G Loss: 2.7258\n",
      "Epoch 103 completed\n",
      "Epoch [104/300] Batch [0/7] D Loss: 0.8520 G Loss: 3.2121\n",
      "Epoch [104/300] Batch [5/7] D Loss: 0.6012 G Loss: 2.8152\n",
      "Epoch 104 completed\n",
      "Epoch [105/300] Batch [0/7] D Loss: 0.7032 G Loss: 3.9603\n",
      "Epoch [105/300] Batch [5/7] D Loss: 0.9511 G Loss: 2.9047\n",
      "Epoch 105 completed\n",
      "Epoch [106/300] Batch [0/7] D Loss: 1.1854 G Loss: 3.3465\n",
      "Epoch [106/300] Batch [5/7] D Loss: 0.9898 G Loss: 3.5536\n",
      "Epoch 106 completed\n",
      "Epoch [107/300] Batch [0/7] D Loss: 1.3093 G Loss: 4.0595\n",
      "Epoch [107/300] Batch [5/7] D Loss: 0.7779 G Loss: 2.0821\n",
      "Epoch 107 completed\n",
      "Epoch [108/300] Batch [0/7] D Loss: 0.8702 G Loss: 5.1014\n",
      "Epoch [108/300] Batch [5/7] D Loss: 1.0266 G Loss: 2.9096\n",
      "Epoch 108 completed\n",
      "Epoch [109/300] Batch [0/7] D Loss: 0.9899 G Loss: 3.3140\n",
      "Epoch [109/300] Batch [5/7] D Loss: 0.5218 G Loss: 3.1627\n",
      "Epoch 109 completed\n",
      "Epoch [110/300] Batch [0/7] D Loss: 0.5125 G Loss: 2.6778\n",
      "Epoch [110/300] Batch [5/7] D Loss: 0.5814 G Loss: 3.0103\n",
      "Epoch 110 completed\n",
      "Epoch [111/300] Batch [0/7] D Loss: 0.8295 G Loss: 2.1988\n",
      "Epoch [111/300] Batch [5/7] D Loss: 0.9188 G Loss: 2.9056\n",
      "Epoch 111 completed\n",
      "Epoch [112/300] Batch [0/7] D Loss: 0.7873 G Loss: 3.4561\n",
      "Epoch [112/300] Batch [5/7] D Loss: 0.7068 G Loss: 3.7795\n",
      "Epoch 112 completed\n",
      "Epoch [113/300] Batch [0/7] D Loss: 0.7648 G Loss: 3.9411\n",
      "Epoch [113/300] Batch [5/7] D Loss: 0.5171 G Loss: 3.6969\n",
      "Epoch 113 completed\n",
      "Epoch [114/300] Batch [0/7] D Loss: 1.0441 G Loss: 4.1340\n",
      "Epoch [114/300] Batch [5/7] D Loss: 1.0228 G Loss: 2.8170\n",
      "Epoch 114 completed\n",
      "Epoch [115/300] Batch [0/7] D Loss: 1.0971 G Loss: 3.0067\n",
      "Epoch [115/300] Batch [5/7] D Loss: 0.8035 G Loss: 3.0859\n",
      "Epoch 115 completed\n",
      "Epoch [116/300] Batch [0/7] D Loss: 0.7853 G Loss: 3.0660\n",
      "Epoch [116/300] Batch [5/7] D Loss: 0.9152 G Loss: 2.8919\n",
      "Epoch 116 completed\n",
      "Epoch [117/300] Batch [0/7] D Loss: 0.9165 G Loss: 4.9004\n",
      "Epoch [117/300] Batch [5/7] D Loss: 0.8347 G Loss: 2.3512\n",
      "Epoch 117 completed\n",
      "Epoch [118/300] Batch [0/7] D Loss: 0.9982 G Loss: 3.4648\n",
      "Epoch [118/300] Batch [5/7] D Loss: 0.8158 G Loss: 3.2695\n",
      "Epoch 118 completed\n",
      "Epoch [119/300] Batch [0/7] D Loss: 1.1235 G Loss: 5.2808\n",
      "Epoch [119/300] Batch [5/7] D Loss: 0.6716 G Loss: 3.3332\n",
      "Epoch 119 completed\n",
      "Epoch [120/300] Batch [0/7] D Loss: 0.7719 G Loss: 2.8195\n",
      "Epoch [120/300] Batch [5/7] D Loss: 1.0248 G Loss: 2.6129\n",
      "Epoch 120 completed\n",
      "Epoch [121/300] Batch [0/7] D Loss: 0.6440 G Loss: 2.8529\n",
      "Epoch [121/300] Batch [5/7] D Loss: 0.7793 G Loss: 2.9698\n",
      "Epoch 121 completed\n",
      "Epoch [122/300] Batch [0/7] D Loss: 0.4798 G Loss: 3.2467\n",
      "Epoch [122/300] Batch [5/7] D Loss: 0.7254 G Loss: 2.2656\n",
      "Epoch 122 completed\n",
      "Epoch [123/300] Batch [0/7] D Loss: 1.6918 G Loss: 4.3493\n",
      "Epoch [123/300] Batch [5/7] D Loss: 0.6388 G Loss: 3.0116\n",
      "Epoch 123 completed\n",
      "Epoch [124/300] Batch [0/7] D Loss: 1.8623 G Loss: 4.3387\n",
      "Epoch [124/300] Batch [5/7] D Loss: 0.7697 G Loss: 2.6607\n",
      "Epoch 124 completed\n",
      "Epoch [125/300] Batch [0/7] D Loss: 1.2577 G Loss: 3.9626\n",
      "Epoch [125/300] Batch [5/7] D Loss: 0.8698 G Loss: 3.2010\n",
      "Epoch 125 completed\n",
      "Epoch [126/300] Batch [0/7] D Loss: 0.5524 G Loss: 3.2112\n",
      "Epoch [126/300] Batch [5/7] D Loss: 1.1657 G Loss: 2.0412\n",
      "Epoch 126 completed\n",
      "Epoch [127/300] Batch [0/7] D Loss: 0.8668 G Loss: 3.1758\n",
      "Epoch [127/300] Batch [5/7] D Loss: 0.5192 G Loss: 2.9304\n",
      "Epoch 127 completed\n",
      "Epoch [128/300] Batch [0/7] D Loss: 0.8287 G Loss: 3.5206\n",
      "Epoch [128/300] Batch [5/7] D Loss: 0.5751 G Loss: 3.2691\n",
      "Epoch 128 completed\n",
      "Epoch [129/300] Batch [0/7] D Loss: 0.6824 G Loss: 3.4085\n",
      "Epoch [129/300] Batch [5/7] D Loss: 0.5171 G Loss: 3.6329\n",
      "Epoch 129 completed\n",
      "Epoch [130/300] Batch [0/7] D Loss: 0.9872 G Loss: 3.5896\n",
      "Epoch [130/300] Batch [5/7] D Loss: 0.7092 G Loss: 3.0531\n",
      "Epoch 130 completed\n",
      "Epoch [131/300] Batch [0/7] D Loss: 0.6929 G Loss: 3.0597\n",
      "Epoch [131/300] Batch [5/7] D Loss: 0.7052 G Loss: 2.8812\n",
      "Epoch 131 completed\n",
      "Epoch [132/300] Batch [0/7] D Loss: 0.6531 G Loss: 3.0541\n",
      "Epoch [132/300] Batch [5/7] D Loss: 0.6875 G Loss: 3.4058\n",
      "Epoch 132 completed\n",
      "Epoch [133/300] Batch [0/7] D Loss: 0.7371 G Loss: 3.0654\n",
      "Epoch [133/300] Batch [5/7] D Loss: 0.6426 G Loss: 2.8884\n",
      "Epoch 133 completed\n",
      "Epoch [134/300] Batch [0/7] D Loss: 0.8609 G Loss: 3.7892\n",
      "Epoch [134/300] Batch [5/7] D Loss: 0.7708 G Loss: 3.1451\n",
      "Epoch 134 completed\n",
      "Epoch [135/300] Batch [0/7] D Loss: 0.7024 G Loss: 3.2966\n",
      "Epoch [135/300] Batch [5/7] D Loss: 0.5905 G Loss: 4.0078\n",
      "Epoch 135 completed\n",
      "Epoch [136/300] Batch [0/7] D Loss: 0.9503 G Loss: 3.8051\n",
      "Epoch [136/300] Batch [5/7] D Loss: 0.6174 G Loss: 2.8398\n",
      "Epoch 136 completed\n",
      "Epoch [137/300] Batch [0/7] D Loss: 0.8126 G Loss: 3.3481\n",
      "Epoch [137/300] Batch [5/7] D Loss: 0.8762 G Loss: 2.5248\n",
      "Epoch 137 completed\n",
      "Epoch [138/300] Batch [0/7] D Loss: 0.5626 G Loss: 2.9734\n",
      "Epoch [138/300] Batch [5/7] D Loss: 0.5173 G Loss: 3.0921\n",
      "Epoch 138 completed\n",
      "Epoch [139/300] Batch [0/7] D Loss: 0.5212 G Loss: 2.9795\n",
      "Epoch [139/300] Batch [5/7] D Loss: 0.6541 G Loss: 3.1457\n",
      "Epoch 139 completed\n",
      "Epoch [140/300] Batch [0/7] D Loss: 0.6100 G Loss: 3.4760\n",
      "Epoch [140/300] Batch [5/7] D Loss: 0.7662 G Loss: 2.5504\n",
      "Epoch 140 completed\n",
      "Epoch [141/300] Batch [0/7] D Loss: 0.7516 G Loss: 2.6658\n",
      "Epoch [141/300] Batch [5/7] D Loss: 0.6971 G Loss: 2.8162\n",
      "Epoch 141 completed\n",
      "Epoch [142/300] Batch [0/7] D Loss: 0.5469 G Loss: 3.1242\n",
      "Epoch [142/300] Batch [5/7] D Loss: 0.5863 G Loss: 3.2428\n",
      "Epoch 142 completed\n",
      "Epoch [143/300] Batch [0/7] D Loss: 0.4960 G Loss: 3.3723\n",
      "Epoch [143/300] Batch [5/7] D Loss: 0.6954 G Loss: 3.5993\n",
      "Epoch 143 completed\n",
      "Epoch [144/300] Batch [0/7] D Loss: 0.8331 G Loss: 3.0811\n",
      "Epoch [144/300] Batch [5/7] D Loss: 0.7789 G Loss: 2.8356\n",
      "Epoch 144 completed\n",
      "Epoch [145/300] Batch [0/7] D Loss: 0.5741 G Loss: 3.5197\n",
      "Epoch [145/300] Batch [5/7] D Loss: 0.8657 G Loss: 2.0916\n",
      "Epoch 145 completed\n",
      "Epoch [146/300] Batch [0/7] D Loss: 0.6246 G Loss: 3.8203\n",
      "Epoch [146/300] Batch [5/7] D Loss: 0.5486 G Loss: 3.2807\n",
      "Epoch 146 completed\n",
      "Epoch [147/300] Batch [0/7] D Loss: 0.4921 G Loss: 3.2067\n",
      "Epoch [147/300] Batch [5/7] D Loss: 0.5379 G Loss: 2.9315\n",
      "Epoch 147 completed\n",
      "Epoch [148/300] Batch [0/7] D Loss: 0.4947 G Loss: 3.4733\n",
      "Epoch [148/300] Batch [5/7] D Loss: 0.7849 G Loss: 2.4007\n",
      "Epoch 148 completed\n",
      "Epoch [149/300] Batch [0/7] D Loss: 0.6382 G Loss: 3.7304\n",
      "Epoch [149/300] Batch [5/7] D Loss: 0.5539 G Loss: 4.0086\n",
      "Epoch 149 completed\n",
      "Epoch [150/300] Batch [0/7] D Loss: 0.5324 G Loss: 3.4186\n",
      "Epoch [150/300] Batch [5/7] D Loss: 0.5166 G Loss: 3.0976\n",
      "Epoch 150 completed\n",
      "Epoch [151/300] Batch [0/7] D Loss: 0.6187 G Loss: 3.5324\n",
      "Epoch [151/300] Batch [5/7] D Loss: 0.7787 G Loss: 3.5530\n",
      "Epoch 151 completed\n",
      "Epoch [152/300] Batch [0/7] D Loss: 0.6872 G Loss: 3.6052\n",
      "Epoch [152/300] Batch [5/7] D Loss: 0.8463 G Loss: 3.4510\n",
      "Epoch 152 completed\n",
      "Epoch [153/300] Batch [0/7] D Loss: 0.8393 G Loss: 2.6739\n",
      "Epoch [153/300] Batch [5/7] D Loss: 0.5906 G Loss: 2.9978\n",
      "Epoch 153 completed\n",
      "Epoch [154/300] Batch [0/7] D Loss: 0.6726 G Loss: 2.7476\n",
      "Epoch [154/300] Batch [5/7] D Loss: 0.5277 G Loss: 3.3381\n",
      "Epoch 154 completed\n",
      "Epoch [155/300] Batch [0/7] D Loss: 0.8903 G Loss: 3.7070\n",
      "Epoch [155/300] Batch [5/7] D Loss: 0.6408 G Loss: 3.3233\n",
      "Epoch 155 completed\n",
      "Epoch [156/300] Batch [0/7] D Loss: 0.6022 G Loss: 3.0645\n",
      "Epoch [156/300] Batch [5/7] D Loss: 0.4231 G Loss: 3.9833\n",
      "Epoch 156 completed\n",
      "Epoch [157/300] Batch [0/7] D Loss: 0.5732 G Loss: 3.3186\n",
      "Epoch [157/300] Batch [5/7] D Loss: 0.5354 G Loss: 3.5314\n",
      "Epoch 157 completed\n",
      "Epoch [158/300] Batch [0/7] D Loss: 0.6099 G Loss: 3.2166\n",
      "Epoch [158/300] Batch [5/7] D Loss: 0.5702 G Loss: 2.9702\n",
      "Epoch 158 completed\n",
      "Epoch [159/300] Batch [0/7] D Loss: 0.5103 G Loss: 3.2479\n",
      "Epoch [159/300] Batch [5/7] D Loss: 0.6455 G Loss: 3.1191\n",
      "Epoch 159 completed\n",
      "Epoch [160/300] Batch [0/7] D Loss: 0.4576 G Loss: 3.5394\n",
      "Epoch [160/300] Batch [5/7] D Loss: 0.6975 G Loss: 4.2468\n",
      "Epoch 160 completed\n",
      "Epoch [161/300] Batch [0/7] D Loss: 0.8031 G Loss: 2.6898\n",
      "Epoch [161/300] Batch [5/7] D Loss: 0.8399 G Loss: 3.7966\n",
      "Epoch 161 completed\n",
      "Epoch [162/300] Batch [0/7] D Loss: 0.5204 G Loss: 3.6491\n",
      "Epoch [162/300] Batch [5/7] D Loss: 0.5155 G Loss: 3.7006\n",
      "Epoch 162 completed\n",
      "Epoch [163/300] Batch [0/7] D Loss: 0.6991 G Loss: 3.3358\n",
      "Epoch [163/300] Batch [5/7] D Loss: 0.6847 G Loss: 2.8104\n",
      "Epoch 163 completed\n",
      "Epoch [164/300] Batch [0/7] D Loss: 0.5370 G Loss: 3.4908\n",
      "Epoch [164/300] Batch [5/7] D Loss: 0.7048 G Loss: 3.7526\n",
      "Epoch 164 completed\n",
      "Epoch [165/300] Batch [0/7] D Loss: 2.0107 G Loss: 2.5857\n",
      "Epoch [165/300] Batch [5/7] D Loss: 0.7249 G Loss: 2.9566\n",
      "Epoch 165 completed\n",
      "Epoch [166/300] Batch [0/7] D Loss: 1.3997 G Loss: 1.7804\n",
      "Epoch [166/300] Batch [5/7] D Loss: 0.6236 G Loss: 2.2406\n",
      "Epoch 166 completed\n",
      "Epoch [167/300] Batch [0/7] D Loss: 0.5226 G Loss: 3.4242\n",
      "Epoch [167/300] Batch [5/7] D Loss: 0.5899 G Loss: 3.1024\n",
      "Epoch 167 completed\n",
      "Epoch [168/300] Batch [0/7] D Loss: 0.9563 G Loss: 3.4034\n",
      "Epoch [168/300] Batch [5/7] D Loss: 0.7015 G Loss: 3.1132\n",
      "Epoch 168 completed\n",
      "Epoch [169/300] Batch [0/7] D Loss: 1.3392 G Loss: 5.1294\n",
      "Epoch [169/300] Batch [5/7] D Loss: 0.5992 G Loss: 3.9342\n",
      "Epoch 169 completed\n",
      "Epoch [170/300] Batch [0/7] D Loss: 0.6381 G Loss: 3.4042\n",
      "Epoch [170/300] Batch [5/7] D Loss: 0.8582 G Loss: 3.8801\n",
      "Epoch 170 completed\n",
      "Epoch [171/300] Batch [0/7] D Loss: 1.0422 G Loss: 4.7901\n",
      "Epoch [171/300] Batch [5/7] D Loss: 0.5726 G Loss: 3.2711\n",
      "Epoch 171 completed\n",
      "Epoch [172/300] Batch [0/7] D Loss: 0.4624 G Loss: 3.4981\n",
      "Epoch [172/300] Batch [5/7] D Loss: 0.5984 G Loss: 3.3989\n",
      "Epoch 172 completed\n",
      "Epoch [173/300] Batch [0/7] D Loss: 0.7498 G Loss: 3.7036\n",
      "Epoch [173/300] Batch [5/7] D Loss: 0.5932 G Loss: 2.9981\n",
      "Epoch 173 completed\n",
      "Epoch [174/300] Batch [0/7] D Loss: 1.1091 G Loss: 3.9633\n",
      "Epoch [174/300] Batch [5/7] D Loss: 0.6943 G Loss: 2.6858\n",
      "Epoch 174 completed\n",
      "Epoch [175/300] Batch [0/7] D Loss: 0.6371 G Loss: 2.7620\n",
      "Epoch [175/300] Batch [5/7] D Loss: 0.5274 G Loss: 3.4493\n",
      "Epoch 175 completed\n",
      "Epoch [176/300] Batch [0/7] D Loss: 0.5131 G Loss: 3.3231\n",
      "Epoch [176/300] Batch [5/7] D Loss: 0.5830 G Loss: 2.8349\n",
      "Epoch 176 completed\n",
      "Epoch [177/300] Batch [0/7] D Loss: 1.0411 G Loss: 4.1198\n",
      "Epoch [177/300] Batch [5/7] D Loss: 0.5754 G Loss: 3.7925\n",
      "Epoch 177 completed\n",
      "Epoch [178/300] Batch [0/7] D Loss: 1.1566 G Loss: 4.2342\n",
      "Epoch [178/300] Batch [5/7] D Loss: 0.5943 G Loss: 3.3746\n",
      "Epoch 178 completed\n",
      "Epoch [179/300] Batch [0/7] D Loss: 0.6654 G Loss: 4.1665\n",
      "Epoch [179/300] Batch [5/7] D Loss: 0.9068 G Loss: 3.6349\n",
      "Epoch 179 completed\n",
      "Epoch [180/300] Batch [0/7] D Loss: 0.7711 G Loss: 2.9188\n",
      "Epoch [180/300] Batch [5/7] D Loss: 0.5791 G Loss: 3.4472\n",
      "Epoch 180 completed\n",
      "Epoch [181/300] Batch [0/7] D Loss: 1.0948 G Loss: 3.0235\n",
      "Epoch [181/300] Batch [5/7] D Loss: 0.6184 G Loss: 3.0516\n",
      "Epoch 181 completed\n",
      "Epoch [182/300] Batch [0/7] D Loss: 0.4577 G Loss: 3.3245\n",
      "Epoch [182/300] Batch [5/7] D Loss: 0.6554 G Loss: 2.8655\n",
      "Epoch 182 completed\n",
      "Epoch [183/300] Batch [0/7] D Loss: 0.6811 G Loss: 3.6856\n",
      "Epoch [183/300] Batch [5/7] D Loss: 0.6312 G Loss: 3.3244\n",
      "Epoch 183 completed\n",
      "Epoch [184/300] Batch [0/7] D Loss: 0.5740 G Loss: 2.9527\n",
      "Epoch [184/300] Batch [5/7] D Loss: 0.6338 G Loss: 2.6134\n",
      "Epoch 184 completed\n",
      "Epoch [185/300] Batch [0/7] D Loss: 0.4573 G Loss: 3.5007\n",
      "Epoch [185/300] Batch [5/7] D Loss: 0.3973 G Loss: 3.7918\n",
      "Epoch 185 completed\n",
      "Epoch [186/300] Batch [0/7] D Loss: 0.6755 G Loss: 2.8568\n",
      "Epoch [186/300] Batch [5/7] D Loss: 0.7318 G Loss: 2.7283\n",
      "Epoch 186 completed\n",
      "Epoch [187/300] Batch [0/7] D Loss: 0.8718 G Loss: 3.7143\n",
      "Epoch [187/300] Batch [5/7] D Loss: 0.5734 G Loss: 2.9224\n",
      "Epoch 187 completed\n",
      "Epoch [188/300] Batch [0/7] D Loss: 0.7625 G Loss: 2.7326\n",
      "Epoch [188/300] Batch [5/7] D Loss: 0.8252 G Loss: 1.9499\n",
      "Epoch 188 completed\n",
      "Epoch [189/300] Batch [0/7] D Loss: 0.9519 G Loss: 5.6057\n",
      "Epoch [189/300] Batch [5/7] D Loss: 0.7072 G Loss: 3.5275\n",
      "Epoch 189 completed\n",
      "Epoch [190/300] Batch [0/7] D Loss: 0.6715 G Loss: 3.3688\n",
      "Epoch [190/300] Batch [5/7] D Loss: 0.9852 G Loss: 2.4728\n",
      "Epoch 190 completed\n",
      "Epoch [191/300] Batch [0/7] D Loss: 0.5448 G Loss: 3.5896\n",
      "Epoch [191/300] Batch [5/7] D Loss: 0.7334 G Loss: 2.2195\n",
      "Epoch 191 completed\n",
      "Epoch [192/300] Batch [0/7] D Loss: 0.7418 G Loss: 4.1795\n",
      "Epoch [192/300] Batch [5/7] D Loss: 0.9268 G Loss: 2.0609\n",
      "Epoch 192 completed\n",
      "Epoch [193/300] Batch [0/7] D Loss: 0.7016 G Loss: 3.6124\n",
      "Epoch [193/300] Batch [5/7] D Loss: 0.4428 G Loss: 4.3551\n",
      "Epoch 193 completed\n",
      "Epoch [194/300] Batch [0/7] D Loss: 0.8024 G Loss: 4.7936\n",
      "Epoch [194/300] Batch [5/7] D Loss: 0.6164 G Loss: 3.4774\n",
      "Epoch 194 completed\n",
      "Epoch [195/300] Batch [0/7] D Loss: 0.7701 G Loss: 2.7602\n",
      "Epoch [195/300] Batch [5/7] D Loss: 0.5526 G Loss: 3.2633\n",
      "Epoch 195 completed\n",
      "Epoch [196/300] Batch [0/7] D Loss: 0.4246 G Loss: 3.6676\n",
      "Epoch [196/300] Batch [5/7] D Loss: 0.5266 G Loss: 3.2955\n",
      "Epoch 196 completed\n",
      "Epoch [197/300] Batch [0/7] D Loss: 0.7850 G Loss: 5.1437\n",
      "Epoch [197/300] Batch [5/7] D Loss: 0.4936 G Loss: 3.1452\n",
      "Epoch 197 completed\n",
      "Epoch [198/300] Batch [0/7] D Loss: 0.5187 G Loss: 3.7781\n",
      "Epoch [198/300] Batch [5/7] D Loss: 0.5066 G Loss: 3.9414\n",
      "Epoch 198 completed\n",
      "Epoch [199/300] Batch [0/7] D Loss: 0.5597 G Loss: 3.9492\n",
      "Epoch [199/300] Batch [5/7] D Loss: 0.5681 G Loss: 3.3215\n",
      "Epoch 199 completed\n",
      "Epoch [200/300] Batch [0/7] D Loss: 0.5127 G Loss: 3.5840\n",
      "Epoch [200/300] Batch [5/7] D Loss: 0.5158 G Loss: 3.2058\n",
      "Epoch 200 completed\n",
      "Epoch [201/300] Batch [0/7] D Loss: 0.5113 G Loss: 3.1848\n",
      "Epoch [201/300] Batch [5/7] D Loss: 0.4960 G Loss: 3.8615\n",
      "Epoch 201 completed\n",
      "Epoch [202/300] Batch [0/7] D Loss: 0.4988 G Loss: 2.8225\n",
      "Epoch [202/300] Batch [5/7] D Loss: 0.4889 G Loss: 3.2212\n",
      "Epoch 202 completed\n",
      "Epoch [203/300] Batch [0/7] D Loss: 0.4736 G Loss: 3.6264\n",
      "Epoch [203/300] Batch [5/7] D Loss: 0.5082 G Loss: 3.1659\n",
      "Epoch 203 completed\n",
      "Epoch [204/300] Batch [0/7] D Loss: 0.6908 G Loss: 3.9509\n",
      "Epoch [204/300] Batch [5/7] D Loss: 0.5242 G Loss: 3.4144\n",
      "Epoch 204 completed\n",
      "Epoch [205/300] Batch [0/7] D Loss: 1.7180 G Loss: 4.5546\n",
      "Epoch [205/300] Batch [5/7] D Loss: 0.4481 G Loss: 2.9492\n",
      "Epoch 205 completed\n",
      "Epoch [206/300] Batch [0/7] D Loss: 0.4840 G Loss: 4.2038\n",
      "Epoch [206/300] Batch [5/7] D Loss: 0.6792 G Loss: 3.7695\n",
      "Epoch 206 completed\n",
      "Epoch [207/300] Batch [0/7] D Loss: 0.5723 G Loss: 3.0686\n",
      "Epoch [207/300] Batch [5/7] D Loss: 0.5618 G Loss: 3.1831\n",
      "Epoch 207 completed\n",
      "Epoch [208/300] Batch [0/7] D Loss: 0.7497 G Loss: 3.6501\n",
      "Epoch [208/300] Batch [5/7] D Loss: 0.4231 G Loss: 3.6768\n",
      "Epoch 208 completed\n",
      "Epoch [209/300] Batch [0/7] D Loss: 0.3903 G Loss: 3.9541\n",
      "Epoch [209/300] Batch [5/7] D Loss: 0.5933 G Loss: 2.7500\n",
      "Epoch 209 completed\n",
      "Epoch [210/300] Batch [0/7] D Loss: 0.7956 G Loss: 3.1211\n",
      "Epoch [210/300] Batch [5/7] D Loss: 0.8421 G Loss: 3.7301\n",
      "Epoch 210 completed\n",
      "Epoch [211/300] Batch [0/7] D Loss: 0.6012 G Loss: 3.0288\n",
      "Epoch [211/300] Batch [5/7] D Loss: 1.1834 G Loss: 4.8516\n",
      "Epoch 211 completed\n",
      "Epoch [212/300] Batch [0/7] D Loss: 0.4515 G Loss: 2.9909\n",
      "Epoch [212/300] Batch [5/7] D Loss: 0.6369 G Loss: 2.5036\n",
      "Epoch 212 completed\n",
      "Epoch [213/300] Batch [0/7] D Loss: 0.8347 G Loss: 4.8008\n",
      "Epoch [213/300] Batch [5/7] D Loss: 0.3807 G Loss: 5.8001\n",
      "Epoch 213 completed\n",
      "Epoch [214/300] Batch [0/7] D Loss: 0.5318 G Loss: 2.6786\n",
      "Epoch [214/300] Batch [5/7] D Loss: 0.5039 G Loss: 3.2505\n",
      "Epoch 214 completed\n",
      "Epoch [215/300] Batch [0/7] D Loss: 1.0341 G Loss: 4.5463\n",
      "Epoch [215/300] Batch [5/7] D Loss: 0.6568 G Loss: 3.1255\n",
      "Epoch 215 completed\n",
      "Epoch [216/300] Batch [0/7] D Loss: 1.3534 G Loss: 5.3241\n",
      "Epoch [216/300] Batch [5/7] D Loss: 0.6046 G Loss: 3.7191\n",
      "Epoch 216 completed\n",
      "Epoch [217/300] Batch [0/7] D Loss: 0.5026 G Loss: 3.4151\n",
      "Epoch [217/300] Batch [5/7] D Loss: 0.4975 G Loss: 3.5658\n",
      "Epoch 217 completed\n",
      "Epoch [218/300] Batch [0/7] D Loss: 0.4216 G Loss: 3.7260\n",
      "Epoch [218/300] Batch [5/7] D Loss: 0.6941 G Loss: 2.4610\n",
      "Epoch 218 completed\n",
      "Epoch [219/300] Batch [0/7] D Loss: 0.7627 G Loss: 4.9565\n",
      "Epoch [219/300] Batch [5/7] D Loss: 0.5260 G Loss: 3.1965\n",
      "Epoch 219 completed\n",
      "Epoch [220/300] Batch [0/7] D Loss: 0.5178 G Loss: 3.3331\n",
      "Epoch [220/300] Batch [5/7] D Loss: 0.5060 G Loss: 3.1390\n",
      "Epoch 220 completed\n",
      "Epoch [221/300] Batch [0/7] D Loss: 0.4933 G Loss: 3.4711\n",
      "Epoch [221/300] Batch [5/7] D Loss: 0.5997 G Loss: 2.5135\n",
      "Epoch 221 completed\n",
      "Epoch [222/300] Batch [0/7] D Loss: 0.6976 G Loss: 3.6052\n",
      "Epoch [222/300] Batch [5/7] D Loss: 0.4539 G Loss: 3.6601\n",
      "Epoch 222 completed\n",
      "Epoch [223/300] Batch [0/7] D Loss: 0.8270 G Loss: 3.5412\n",
      "Epoch [223/300] Batch [5/7] D Loss: 0.5218 G Loss: 3.6406\n",
      "Epoch 223 completed\n",
      "Epoch [224/300] Batch [0/7] D Loss: 0.7298 G Loss: 4.1243\n",
      "Epoch [224/300] Batch [5/7] D Loss: 0.5236 G Loss: 3.1363\n",
      "Epoch 224 completed\n",
      "Epoch [225/300] Batch [0/7] D Loss: 0.4264 G Loss: 3.8004\n",
      "Epoch [225/300] Batch [5/7] D Loss: 0.4898 G Loss: 3.3541\n",
      "Epoch 225 completed\n",
      "Epoch [226/300] Batch [0/7] D Loss: 0.4804 G Loss: 3.2660\n",
      "Epoch [226/300] Batch [5/7] D Loss: 0.7084 G Loss: 2.4981\n",
      "Epoch 226 completed\n",
      "Epoch [227/300] Batch [0/7] D Loss: 0.7779 G Loss: 3.6866\n",
      "Epoch [227/300] Batch [5/7] D Loss: 0.4156 G Loss: 3.5915\n",
      "Epoch 227 completed\n",
      "Epoch [228/300] Batch [0/7] D Loss: 0.7066 G Loss: 3.5215\n",
      "Epoch [228/300] Batch [5/7] D Loss: 0.4865 G Loss: 3.1558\n",
      "Epoch 228 completed\n",
      "Epoch [229/300] Batch [0/7] D Loss: 1.0725 G Loss: 3.9171\n",
      "Epoch [229/300] Batch [5/7] D Loss: 0.7158 G Loss: 3.3484\n",
      "Epoch 229 completed\n",
      "Epoch [230/300] Batch [0/7] D Loss: 0.7281 G Loss: 4.0816\n",
      "Epoch [230/300] Batch [5/7] D Loss: 0.5156 G Loss: 3.2587\n",
      "Epoch 230 completed\n",
      "Epoch [231/300] Batch [0/7] D Loss: 0.7156 G Loss: 4.7262\n",
      "Epoch [231/300] Batch [5/7] D Loss: 0.8662 G Loss: 3.2594\n",
      "Epoch 231 completed\n",
      "Epoch [232/300] Batch [0/7] D Loss: 0.6769 G Loss: 2.8142\n",
      "Epoch [232/300] Batch [5/7] D Loss: 0.6064 G Loss: 3.2309\n",
      "Epoch 232 completed\n",
      "Epoch [233/300] Batch [0/7] D Loss: 0.7883 G Loss: 4.2759\n",
      "Epoch [233/300] Batch [5/7] D Loss: 0.7473 G Loss: 1.9899\n",
      "Epoch 233 completed\n",
      "Epoch [234/300] Batch [0/7] D Loss: 0.5474 G Loss: 4.9187\n",
      "Epoch [234/300] Batch [5/7] D Loss: 0.5031 G Loss: 2.8798\n",
      "Epoch 234 completed\n",
      "Epoch [235/300] Batch [0/7] D Loss: 0.7260 G Loss: 4.1988\n",
      "Epoch [235/300] Batch [5/7] D Loss: 0.5488 G Loss: 3.5132\n",
      "Epoch 235 completed\n",
      "Epoch [236/300] Batch [0/7] D Loss: 0.7675 G Loss: 4.4511\n",
      "Epoch [236/300] Batch [5/7] D Loss: 0.5644 G Loss: 3.5342\n",
      "Epoch 236 completed\n",
      "Epoch [237/300] Batch [0/7] D Loss: 0.5983 G Loss: 3.2342\n",
      "Epoch [237/300] Batch [5/7] D Loss: 0.8136 G Loss: 3.1975\n",
      "Epoch 237 completed\n",
      "Epoch [238/300] Batch [0/7] D Loss: 0.5388 G Loss: 3.1381\n",
      "Epoch [238/300] Batch [5/7] D Loss: 0.4242 G Loss: 3.4611\n",
      "Epoch 238 completed\n",
      "Epoch [239/300] Batch [0/7] D Loss: 0.6423 G Loss: 3.5779\n",
      "Epoch [239/300] Batch [5/7] D Loss: 0.4941 G Loss: 3.5620\n",
      "Epoch 239 completed\n",
      "Epoch [240/300] Batch [0/7] D Loss: 0.5173 G Loss: 3.7369\n",
      "Epoch [240/300] Batch [5/7] D Loss: 0.5282 G Loss: 3.6080\n",
      "Epoch 240 completed\n",
      "Epoch [241/300] Batch [0/7] D Loss: 0.4560 G Loss: 3.3078\n",
      "Epoch [241/300] Batch [5/7] D Loss: 0.4229 G Loss: 3.7300\n",
      "Epoch 241 completed\n",
      "Epoch [242/300] Batch [0/7] D Loss: 0.5488 G Loss: 2.7494\n",
      "Epoch [242/300] Batch [5/7] D Loss: 0.4210 G Loss: 3.7827\n",
      "Epoch 242 completed\n",
      "Epoch [243/300] Batch [0/7] D Loss: 0.4950 G Loss: 2.8986\n",
      "Epoch [243/300] Batch [5/7] D Loss: 0.5749 G Loss: 2.8283\n",
      "Epoch 243 completed\n",
      "Epoch [244/300] Batch [0/7] D Loss: 0.5181 G Loss: 3.2434\n",
      "Epoch [244/300] Batch [5/7] D Loss: 0.5583 G Loss: 3.5752\n",
      "Epoch 244 completed\n",
      "Epoch [245/300] Batch [0/7] D Loss: 0.4736 G Loss: 3.1311\n",
      "Epoch [245/300] Batch [5/7] D Loss: 0.5995 G Loss: 3.0837\n",
      "Epoch 245 completed\n",
      "Epoch [246/300] Batch [0/7] D Loss: 0.4782 G Loss: 3.9798\n",
      "Epoch [246/300] Batch [5/7] D Loss: 0.7886 G Loss: 2.3369\n",
      "Epoch 246 completed\n",
      "Epoch [247/300] Batch [0/7] D Loss: 0.8319 G Loss: 5.8786\n",
      "Epoch [247/300] Batch [5/7] D Loss: 0.5189 G Loss: 4.2441\n",
      "Epoch 247 completed\n",
      "Epoch [248/300] Batch [0/7] D Loss: 0.6460 G Loss: 4.0310\n",
      "Epoch [248/300] Batch [5/7] D Loss: 0.4535 G Loss: 4.0355\n",
      "Epoch 248 completed\n",
      "Epoch [249/300] Batch [0/7] D Loss: 1.1334 G Loss: 3.9363\n",
      "Epoch [249/300] Batch [5/7] D Loss: 0.4148 G Loss: 3.2776\n",
      "Epoch 249 completed\n",
      "Epoch [250/300] Batch [0/7] D Loss: 0.8042 G Loss: 4.0012\n",
      "Epoch [250/300] Batch [5/7] D Loss: 0.5579 G Loss: 3.4608\n",
      "Epoch 250 completed\n",
      "Epoch [251/300] Batch [0/7] D Loss: 1.0210 G Loss: 5.1620\n",
      "Epoch [251/300] Batch [5/7] D Loss: 0.5656 G Loss: 3.7521\n",
      "Epoch 251 completed\n",
      "Epoch [252/300] Batch [0/7] D Loss: 0.4180 G Loss: 3.6326\n",
      "Epoch [252/300] Batch [5/7] D Loss: 0.4484 G Loss: 4.5153\n",
      "Epoch 252 completed\n",
      "Epoch [253/300] Batch [0/7] D Loss: 0.6415 G Loss: 3.1617\n",
      "Epoch [253/300] Batch [5/7] D Loss: 0.4303 G Loss: 3.5247\n",
      "Epoch 253 completed\n",
      "Epoch [254/300] Batch [0/7] D Loss: 1.2194 G Loss: 6.2131\n",
      "Epoch [254/300] Batch [5/7] D Loss: 0.4996 G Loss: 3.3046\n",
      "Epoch 254 completed\n",
      "Epoch [255/300] Batch [0/7] D Loss: 0.4631 G Loss: 4.0532\n",
      "Epoch [255/300] Batch [5/7] D Loss: 0.4908 G Loss: 3.5984\n",
      "Epoch 255 completed\n",
      "Epoch [256/300] Batch [0/7] D Loss: 0.5867 G Loss: 3.6798\n",
      "Epoch [256/300] Batch [5/7] D Loss: 0.5277 G Loss: 3.7125\n",
      "Epoch 256 completed\n",
      "Epoch [257/300] Batch [0/7] D Loss: 0.5288 G Loss: 3.3600\n",
      "Epoch [257/300] Batch [5/7] D Loss: 0.5634 G Loss: 3.9798\n",
      "Epoch 257 completed\n",
      "Epoch [258/300] Batch [0/7] D Loss: 0.9449 G Loss: 4.1366\n",
      "Epoch [258/300] Batch [5/7] D Loss: 0.4737 G Loss: 3.4681\n",
      "Epoch 258 completed\n",
      "Epoch [259/300] Batch [0/7] D Loss: 0.5987 G Loss: 3.8120\n",
      "Epoch [259/300] Batch [5/7] D Loss: 0.4693 G Loss: 3.7058\n",
      "Epoch 259 completed\n",
      "Epoch [260/300] Batch [0/7] D Loss: 1.0268 G Loss: 5.6454\n",
      "Epoch [260/300] Batch [5/7] D Loss: 0.4161 G Loss: 3.7854\n",
      "Epoch 260 completed\n",
      "Epoch [261/300] Batch [0/7] D Loss: 0.7599 G Loss: 3.4256\n",
      "Epoch [261/300] Batch [5/7] D Loss: 0.4570 G Loss: 3.4805\n",
      "Epoch 261 completed\n",
      "Epoch [262/300] Batch [0/7] D Loss: 0.5769 G Loss: 3.4879\n",
      "Epoch [262/300] Batch [5/7] D Loss: 0.4635 G Loss: 4.6750\n",
      "Epoch 262 completed\n",
      "Epoch [263/300] Batch [0/7] D Loss: 0.4298 G Loss: 3.1846\n",
      "Epoch [263/300] Batch [5/7] D Loss: 0.4280 G Loss: 3.3877\n",
      "Epoch 263 completed\n",
      "Epoch [264/300] Batch [0/7] D Loss: 0.4354 G Loss: 3.4681\n",
      "Epoch [264/300] Batch [5/7] D Loss: 0.5782 G Loss: 3.3966\n",
      "Epoch 264 completed\n",
      "Epoch [265/300] Batch [0/7] D Loss: 0.5170 G Loss: 3.8793\n",
      "Epoch [265/300] Batch [5/7] D Loss: 0.5051 G Loss: 3.2665\n",
      "Epoch 265 completed\n",
      "Epoch [266/300] Batch [0/7] D Loss: 0.5146 G Loss: 3.4425\n",
      "Epoch [266/300] Batch [5/7] D Loss: 0.4486 G Loss: 3.3302\n",
      "Epoch 266 completed\n",
      "Epoch [267/300] Batch [0/7] D Loss: 0.5241 G Loss: 2.9275\n",
      "Epoch [267/300] Batch [5/7] D Loss: 0.6837 G Loss: 3.0220\n",
      "Epoch 267 completed\n",
      "Epoch [268/300] Batch [0/7] D Loss: 0.4570 G Loss: 3.6900\n",
      "Epoch [268/300] Batch [5/7] D Loss: 0.5470 G Loss: 3.2927\n",
      "Epoch 268 completed\n",
      "Epoch [269/300] Batch [0/7] D Loss: 0.4972 G Loss: 3.7793\n",
      "Epoch [269/300] Batch [5/7] D Loss: 0.4432 G Loss: 3.9655\n",
      "Epoch 269 completed\n",
      "Epoch [270/300] Batch [0/7] D Loss: 1.0034 G Loss: 4.8617\n",
      "Epoch [270/300] Batch [5/7] D Loss: 0.5375 G Loss: 3.7518\n",
      "Epoch 270 completed\n",
      "Epoch [271/300] Batch [0/7] D Loss: 0.6382 G Loss: 3.7571\n",
      "Epoch [271/300] Batch [5/7] D Loss: 0.5041 G Loss: 3.3912\n",
      "Epoch 271 completed\n",
      "Epoch [272/300] Batch [0/7] D Loss: 0.4462 G Loss: 3.6245\n",
      "Epoch [272/300] Batch [5/7] D Loss: 0.4848 G Loss: 3.1582\n",
      "Epoch 272 completed\n",
      "Epoch [273/300] Batch [0/7] D Loss: 0.8410 G Loss: 4.6711\n",
      "Epoch [273/300] Batch [5/7] D Loss: 0.4748 G Loss: 3.9181\n",
      "Epoch 273 completed\n",
      "Epoch [274/300] Batch [0/7] D Loss: 0.9646 G Loss: 4.4723\n",
      "Epoch [274/300] Batch [5/7] D Loss: 0.4795 G Loss: 3.7035\n",
      "Epoch 274 completed\n",
      "Epoch [275/300] Batch [0/7] D Loss: 0.9611 G Loss: 4.0035\n",
      "Epoch [275/300] Batch [5/7] D Loss: 0.5776 G Loss: 3.5550\n",
      "Epoch 275 completed\n",
      "Epoch [276/300] Batch [0/7] D Loss: 0.4639 G Loss: 3.3063\n",
      "Epoch [276/300] Batch [5/7] D Loss: 0.7850 G Loss: 2.4660\n",
      "Epoch 276 completed\n",
      "Epoch [277/300] Batch [0/7] D Loss: 0.4046 G Loss: 3.9898\n",
      "Epoch [277/300] Batch [5/7] D Loss: 0.5187 G Loss: 3.3781\n",
      "Epoch 277 completed\n",
      "Epoch [278/300] Batch [0/7] D Loss: 0.9922 G Loss: 4.1835\n",
      "Epoch [278/300] Batch [5/7] D Loss: 0.4990 G Loss: 3.1124\n",
      "Epoch 278 completed\n",
      "Epoch [279/300] Batch [0/7] D Loss: 0.7385 G Loss: 3.7988\n",
      "Epoch [279/300] Batch [5/7] D Loss: 0.4512 G Loss: 3.2665\n",
      "Epoch 279 completed\n",
      "Epoch [280/300] Batch [0/7] D Loss: 0.5098 G Loss: 3.3399\n",
      "Epoch [280/300] Batch [5/7] D Loss: 0.4966 G Loss: 3.2950\n",
      "Epoch 280 completed\n",
      "Epoch [281/300] Batch [0/7] D Loss: 0.4525 G Loss: 3.1812\n",
      "Epoch [281/300] Batch [5/7] D Loss: 0.4244 G Loss: 3.5318\n",
      "Epoch 281 completed\n",
      "Epoch [282/300] Batch [0/7] D Loss: 0.6661 G Loss: 2.9615\n",
      "Epoch [282/300] Batch [5/7] D Loss: 0.4795 G Loss: 3.2121\n",
      "Epoch 282 completed\n",
      "Epoch [283/300] Batch [0/7] D Loss: 0.8964 G Loss: 3.6265\n",
      "Epoch [283/300] Batch [5/7] D Loss: 0.4608 G Loss: 3.8308\n",
      "Epoch 283 completed\n",
      "Epoch [284/300] Batch [0/7] D Loss: 0.5025 G Loss: 3.5508\n",
      "Epoch [284/300] Batch [5/7] D Loss: 0.9333 G Loss: 2.7335\n",
      "Epoch 284 completed\n",
      "Epoch [285/300] Batch [0/7] D Loss: 0.9486 G Loss: 4.8681\n",
      "Epoch [285/300] Batch [5/7] D Loss: 0.5482 G Loss: 3.3792\n",
      "Epoch 285 completed\n",
      "Epoch [286/300] Batch [0/7] D Loss: 1.7309 G Loss: 6.5699\n",
      "Epoch [286/300] Batch [5/7] D Loss: 0.9988 G Loss: 1.7970\n",
      "Epoch 286 completed\n",
      "Epoch [287/300] Batch [0/7] D Loss: 0.4918 G Loss: 4.6231\n",
      "Epoch [287/300] Batch [5/7] D Loss: 0.5680 G Loss: 3.5886\n",
      "Epoch 287 completed\n",
      "Epoch [288/300] Batch [0/7] D Loss: 0.5482 G Loss: 2.9234\n",
      "Epoch [288/300] Batch [5/7] D Loss: 0.5488 G Loss: 3.6779\n",
      "Epoch 288 completed\n",
      "Epoch [289/300] Batch [0/7] D Loss: 0.5923 G Loss: 3.8963\n",
      "Epoch [289/300] Batch [5/7] D Loss: 0.4878 G Loss: 2.7060\n",
      "Epoch 289 completed\n",
      "Epoch [290/300] Batch [0/7] D Loss: 0.4865 G Loss: 3.3136\n",
      "Epoch [290/300] Batch [5/7] D Loss: 0.3882 G Loss: 3.8340\n",
      "Epoch 290 completed\n",
      "Epoch [291/300] Batch [0/7] D Loss: 0.4559 G Loss: 3.2266\n",
      "Epoch [291/300] Batch [5/7] D Loss: 0.4741 G Loss: 3.2349\n",
      "Epoch 291 completed\n",
      "Epoch [292/300] Batch [0/7] D Loss: 1.0371 G Loss: 4.4448\n",
      "Epoch [292/300] Batch [5/7] D Loss: 0.4531 G Loss: 2.8590\n",
      "Epoch 292 completed\n",
      "Epoch [293/300] Batch [0/7] D Loss: 0.9025 G Loss: 5.8201\n",
      "Epoch [293/300] Batch [5/7] D Loss: 0.4496 G Loss: 3.5923\n",
      "Epoch 293 completed\n",
      "Epoch [294/300] Batch [0/7] D Loss: 0.4867 G Loss: 3.3572\n",
      "Epoch [294/300] Batch [5/7] D Loss: 0.5530 G Loss: 3.2719\n",
      "Epoch 294 completed\n",
      "Epoch [295/300] Batch [0/7] D Loss: 0.5170 G Loss: 3.3739\n",
      "Epoch [295/300] Batch [5/7] D Loss: 0.5647 G Loss: 3.5745\n",
      "Epoch 295 completed\n",
      "Epoch [296/300] Batch [0/7] D Loss: 0.4302 G Loss: 4.1256\n",
      "Epoch [296/300] Batch [5/7] D Loss: 0.4782 G Loss: 3.4238\n",
      "Epoch 296 completed\n",
      "Epoch [297/300] Batch [0/7] D Loss: 0.6634 G Loss: 3.4461\n",
      "Epoch [297/300] Batch [5/7] D Loss: 0.4302 G Loss: 3.3545\n",
      "Epoch 297 completed\n",
      "Epoch [298/300] Batch [0/7] D Loss: 1.2447 G Loss: 5.6781\n",
      "Epoch [298/300] Batch [5/7] D Loss: 0.5327 G Loss: 3.3509\n",
      "Epoch 298 completed\n",
      "Epoch [299/300] Batch [0/7] D Loss: 1.1038 G Loss: 3.3868\n",
      "Epoch [299/300] Batch [5/7] D Loss: 0.6401 G Loss: 2.5416\n",
      "Epoch 299 completed\n",
      "Epoch [300/300] Batch [0/7] D Loss: 0.4237 G Loss: 4.0352\n",
      "Epoch [300/300] Batch [5/7] D Loss: 0.4912 G Loss: 3.2068\n",
      "Epoch 300 completed\n",
      "Training finished\n",
      "Generating Loose images\n",
      "Loose: 0/1000 uploaded\n",
      "Loose: 100/1000 uploaded\n",
      "Loose: 200/1000 uploaded\n",
      "Loose: 300/1000 uploaded\n",
      "Loose: 400/1000 uploaded\n",
      "Loose: 500/1000 uploaded\n",
      "Loose: 600/1000 uploaded\n",
      "Loose: 700/1000 uploaded\n",
      "Loose: 800/1000 uploaded\n",
      "Loose: 900/1000 uploaded\n",
      "Loose generation completed\n",
      "Generating Tight images\n",
      "Tight: 0/1000 uploaded\n",
      "Tight: 100/1000 uploaded\n",
      "Tight: 200/1000 uploaded\n",
      "Tight: 300/1000 uploaded\n",
      "Tight: 400/1000 uploaded\n",
      "Tight: 500/1000 uploaded\n",
      "Tight: 600/1000 uploaded\n",
      "Tight: 700/1000 uploaded\n",
      "Tight: 800/1000 uploaded\n",
      "Tight: 900/1000 uploaded\n",
      "Tight generation completed\n",
      "All DCGAN images generated and uploaded\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# IMPORT REQUIRED LIBRARIES\n",
    "# =========================\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# =========================\n",
    "# AWS S3 CONFIGURATION\n",
    "# =========================\n",
    "\n",
    "BUCKET_NAME = \"ai-bmi-predictor\"\n",
    "\n",
    "INPUT_PREFIX = \"tight and loose classifier/orginal dataset/Dataset/Training Data/\"\n",
    "OUTPUT_PREFIX = \"tight and loose classifier/synthetic data/Training Data/\"\n",
    "\n",
    "print(\"Connecting to S3...\")\n",
    "s3 = boto3.client(\"s3\")\n",
    "print(\"Connected to S3\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# IMAGE SETTINGS\n",
    "# =========================\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "CHANNELS = 3\n",
    "LATENT_DIM = 100\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 300\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# IMAGE TRANSFORMS\n",
    "# =========================\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CUSTOM S3 DATASET\n",
    "# =========================\n",
    "\n",
    "class S3ImageDataset(Dataset):\n",
    "    def __init__(self, prefix, transform):\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "\n",
    "        print(\"Loading images from S3...\")\n",
    "\n",
    "        for label, folder in enumerate([\"Loose/\", \"Tight/\"]):\n",
    "            response = s3.list_objects_v2(\n",
    "                Bucket=BUCKET_NAME,\n",
    "                Prefix=prefix + folder\n",
    "            )\n",
    "            for obj in response.get(\"Contents\", []):\n",
    "                self.images.append(obj[\"Key\"])\n",
    "                self.labels.append(label)\n",
    "\n",
    "        print(f\"Total images loaded: {len(self.images)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        obj = s3.get_object(Bucket=BUCKET_NAME, Key=self.images[idx])\n",
    "        img = Image.open(obj[\"Body\"]).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "\n",
    "\n",
    "dataset = S3ImageDataset(INPUT_PREFIX, transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(\"Dataset ready\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CONDITIONAL DCGAN GENERATOR\n",
    "# =========================\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(NUM_CLASSES, LATENT_DIM)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(LATENT_DIM * 2, 512, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, CHANNELS, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        label_embedding = self.label_emb(labels)\n",
    "        x = torch.cat((noise, label_embedding), dim=1)\n",
    "        x = x.unsqueeze(2).unsqueeze(3)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CONDITIONAL DCGAN DISCRIMINATOR\n",
    "# =========================\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(NUM_CLASSES, IMAGE_SIZE * IMAGE_SIZE)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(CHANNELS + 1, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        label_map = self.label_emb(labels).view(-1, 1, IMAGE_SIZE, IMAGE_SIZE)\n",
    "        x = torch.cat((img, label_map), dim=1)\n",
    "        return self.net(x).view(-1, 1)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# INITIALIZE MODELS\n",
    "# =========================\n",
    "\n",
    "generator = Generator().to(DEVICE)\n",
    "discriminator = Discriminator().to(DEVICE)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "\n",
    "print(\"Models initialized\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "\n",
    "print(\"Starting DCGAN training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch_idx, (imgs, labels) in enumerate(dataloader):\n",
    "\n",
    "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "        batch_size = imgs.size(0)\n",
    "\n",
    "        real = torch.full((batch_size, 1), 0.9, device=DEVICE)\n",
    "        fake = torch.zeros(batch_size, 1, device=DEVICE)\n",
    "\n",
    "        # Train Discriminator\n",
    "        noise = torch.randn(batch_size, LATENT_DIM, device=DEVICE)\n",
    "        gen_imgs = generator(noise, labels)\n",
    "\n",
    "        d_real = criterion(discriminator(imgs, labels), real)\n",
    "        d_fake = criterion(discriminator(gen_imgs.detach(), labels), fake)\n",
    "        d_loss = d_real + d_fake\n",
    "\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Train Generator\n",
    "        g_loss = criterion(discriminator(gen_imgs, labels), real)\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        if batch_idx % 5 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "                f\"Batch [{batch_idx}/{len(dataloader)}] \"\n",
    "                f\"D Loss: {d_loss.item():.4f} \"\n",
    "                f\"G Loss: {g_loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed\")\n",
    "\n",
    "print(\"Training finished\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# GENERATE & UPLOAD IMAGES\n",
    "# =========================\n",
    "\n",
    "generator.eval()\n",
    "\n",
    "def generate_and_upload(label, name):\n",
    "    print(f\"Generating {name} images\")\n",
    "\n",
    "    for i in range(1000):\n",
    "        noise = torch.randn(1, LATENT_DIM, device=DEVICE)\n",
    "        lbl = torch.tensor([label], device=DEVICE)\n",
    "\n",
    "        img = generator(noise, lbl)\n",
    "        img = (img + 1) / 2\n",
    "\n",
    "        filename = f\"{name}_{i}.png\"\n",
    "        path = f\"/tmp/{filename}\"\n",
    "\n",
    "        save_image(img, path)\n",
    "\n",
    "        s3.upload_file(\n",
    "            path,\n",
    "            BUCKET_NAME,\n",
    "            f\"{OUTPUT_PREFIX}{name}/{filename}\"\n",
    "        )\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"{name}: {i}/1000 uploaded\")\n",
    "\n",
    "    print(f\"{name} generation completed\")\n",
    "\n",
    "\n",
    "generate_and_upload(0, \"Loose\")\n",
    "generate_and_upload(1, \"Tight\")\n",
    "\n",
    "print(\"All DCGAN images generated and uploaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c05432-584f-4dff-8e00-eb942f59353a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
