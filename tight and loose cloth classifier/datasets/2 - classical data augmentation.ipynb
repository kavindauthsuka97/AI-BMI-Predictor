{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "312f5cf1-382c-4164-8648-b11421a0f8a4",
   "metadata": {},
   "source": [
    "Cell 1 â€” Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa928041-c6af-41eb-be14-e5d99fac89c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting albumentations\n",
      "  Downloading albumentations-2.0.8-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: opencv-python in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (1.40.72)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.24.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from albumentations) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from albumentations) (1.15.2)\n",
      "Requirement already satisfied: PyYAML in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from albumentations) (6.0.3)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from albumentations) (2.12.3)\n",
      "Collecting albucore==0.0.24 (from albumentations)\n",
      "  Downloading albucore-0.0.24-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting opencv-python-headless>=4.9.0.80 (from albumentations)\n",
      "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting stringzilla>=3.10.4 (from albucore==0.0.24->albumentations)\n",
      "  Downloading stringzilla-4.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (121 kB)\n",
      "Collecting simsimd>=5.9.2 (from albucore==0.0.24->albumentations)\n",
      "  Downloading simsimd-6.5.12-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (70 kB)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.72 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3) (1.40.72)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from boto3) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from botocore<1.41.0,>=1.40.72->boto3) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from botocore<1.41.0,>=1.40.72->boto3) (1.26.20)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.72->boto3) (1.17.0)\n",
      "Collecting numpy>=1.24.4 (from albumentations)\n",
      "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pydantic>=2.9.2->albumentations) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pydantic>=2.9.2->albumentations) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from pydantic>=2.9.2->albumentations) (0.4.2)\n",
      "Downloading albumentations-2.0.8-py3-none-any.whl (369 kB)\n",
      "Downloading albucore-0.0.24-py3-none-any.whl (15 kB)\n",
      "Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m204.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m292.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading simsimd-6.5.12-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (582 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m582.3/582.3 kB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading stringzilla-4.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: simsimd, stringzilla, numpy, opencv-python-headless, albucore, albumentations\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 1.26.4\n",
      "\u001b[2K    Uninstalling numpy-1.26.4:â•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/6\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled numpy-1.26.4â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/6\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6/6\u001b[0m [albumentations]m [albumentations]eadless]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.16.2 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed albucore-0.0.24 albumentations-2.0.8 numpy-2.2.6 opencv-python-headless-4.12.0.88 simsimd-6.5.12 stringzilla-4.5.1\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install albumentations opencv-python boto3 tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab3fd5-d570-44c3-9e00-f5d8a1a4379b",
   "metadata": {},
   "source": [
    "Cell 2 â€” Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc57bad5-694e-420c-9541-529fd9ad6719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS utilities\n",
    "import os\n",
    "\n",
    "# Random selection\n",
    "import random\n",
    "\n",
    "# OpenCV for image processing\n",
    "import cv2\n",
    "\n",
    "# Numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Albumentations for data augmentation\n",
    "import albumentations as A\n",
    "\n",
    "# AWS SDK for Python\n",
    "import boto3\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fe04b4-a868-4bb7-80ef-42016ef66773",
   "metadata": {},
   "source": [
    "STEP 3 â€” Initialize S3 client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae909e7a-bced-497c-9fec-08ebefd6e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create S3 client (AWS credentials must already be configured)\n",
    "s3 = boto3.client(\"s3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e44ff3-d55b-4551-8006-0bb226af80c5",
   "metadata": {},
   "source": [
    "Cell 4 â€” Define S3 path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1a84016-ef6d-4a56-a99e-d34af1d5cb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket name\n",
    "S3_BUCKET = \"ai-bmi-predictor-v2\"\n",
    "\n",
    "# Source images path (original dataset)\n",
    "S3_SOURCE_BASE = (\n",
    "    \"tight and loose classifier/\"\n",
    "    \"orginal dataset/\"\n",
    "    \"Dataset/\"\n",
    "    \"Training Data\"\n",
    ")\n",
    "\n",
    "# Target path for synthetic images\n",
    "S3_TARGET_BASE = (\n",
    "    \"tight and loose classifier/\"\n",
    "    \"synthetic data/\"\n",
    "    \"Training Data\"\n",
    ")\n",
    "\n",
    "# Number of synthetic images per class\n",
    "TARGET_IMAGES_PER_CLASS = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7a4909-8f4d-4f28-83ac-c3b1338cb80a",
   "metadata": {},
   "source": [
    "Cell 5 â€” Define augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1613f24a-2499-425e-995b-0c6fa11b75f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6041/1217378263.py:20: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(\n"
     ]
    }
   ],
   "source": [
    "# Classical augmentation pipeline\n",
    "augmentation_pipeline = A.Compose([\n",
    "\n",
    "    # Slight brightness and contrast changes\n",
    "    A.RandomBrightnessContrast(\n",
    "        brightness_limit=0.1,\n",
    "        contrast_limit=0.1,\n",
    "        p=0.8\n",
    "    ),\n",
    "\n",
    "    # Very small color changes\n",
    "    A.HueSaturationValue(\n",
    "        hue_shift_limit=5,\n",
    "        sat_shift_limit=10,\n",
    "        val_shift_limit=5,\n",
    "        p=0.6\n",
    "    ),\n",
    "\n",
    "    # Tiny random noise\n",
    "    A.GaussNoise(\n",
    "        var_limit=(5.0, 15.0),\n",
    "        p=0.5\n",
    "    ),\n",
    "\n",
    "    # Slight blur\n",
    "    A.GaussianBlur(\n",
    "        blur_limit=(3, 5),\n",
    "        p=0.4\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552551c1-f284-444e-8bb8-38d667c15d50",
   "metadata": {},
   "source": [
    "Cell 6 â€” List images from an S3 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a867462-7ba2-4008-a7c7-b66471261406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to upload image to S3\n",
    "def upload_to_s3(image, bucket, s3_key):\n",
    "    \n",
    "    # Encode image as JPEG\n",
    "    success, buffer = cv2.imencode(\".jpg\", image)\n",
    "\n",
    "    # Upload encoded image to S3\n",
    "    s3.put_object(\n",
    "        Bucket=bucket,\n",
    "        Key=s3_key,\n",
    "        Body=buffer.tobytes(),\n",
    "        ContentType=\"image/jpeg\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e64294b-9b67-45a2-93c0-d28100456510",
   "metadata": {},
   "source": [
    "Cell 7 â€” Generate & upload synthetic images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "470e4b43-444f-4a0b-aa84-bb26c4c504e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List image files from S3 prefix\n",
    "def list_s3_images(bucket, prefix):\n",
    "\n",
    "    image_keys = []\n",
    "\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            if obj[\"Key\"].lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                image_keys.append(obj[\"Key\"])\n",
    "\n",
    "    print(f\"ğŸ“‚ Found {len(image_keys)} images in {prefix}\")\n",
    "    return image_keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cb7193-3431-4a85-b5b1-bea4737f31c2",
   "metadata": {},
   "source": [
    "STEP 7 â€” Load image from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b049c6ff-aa92-4e71-bae5-6741421f3fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image from S3 into OpenCV\n",
    "def load_image_from_s3(bucket, key):\n",
    "\n",
    "    # Download object\n",
    "    response = s3.get_object(Bucket=bucket, Key=key)\n",
    "\n",
    "    # Read bytes\n",
    "    image_bytes = response[\"Body\"].read()\n",
    "\n",
    "    # Convert bytes to numpy array\n",
    "    image_array = np.frombuffer(image_bytes, np.uint8)\n",
    "\n",
    "    # Decode image\n",
    "    image = cv2.imdecode(image_array, cv2.IMREAD_COLOR)\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3788b06-0930-4c51-bc06-58c7649efcf3",
   "metadata": {},
   "source": [
    "STEP 8 â€” Upload image to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5e844a0-5fc0-4b0a-ba01-8f54d394bb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload OpenCV image to S3\n",
    "def upload_to_s3(image, bucket, key):\n",
    "\n",
    "    # Encode image as JPEG\n",
    "    success, buffer = cv2.imencode(\".jpg\", image)\n",
    "\n",
    "    # Upload to S3\n",
    "    s3.put_object(\n",
    "        Bucket=bucket,\n",
    "        Key=key,\n",
    "        Body=buffer.tobytes(),\n",
    "        ContentType=\"image/jpeg\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6288b8ef-44e3-4271-84e0-15f78292d0c6",
   "metadata": {},
   "source": [
    "STEP 9 â€” Generate & upload augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5653b71c-9ec5-4234-85ab-9f18edd7e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and upload synthetic images for a class\n",
    "def generate_and_upload(class_name):\n",
    "\n",
    "    print(f\"\\nğŸš€ Starting augmentation for class: {class_name}\")\n",
    "\n",
    "    # Build source prefix\n",
    "    source_prefix = f\"{S3_SOURCE_BASE}/{class_name}/\"\n",
    "\n",
    "    # Get list of original images\n",
    "    source_keys = list_s3_images(S3_BUCKET, source_prefix)\n",
    "\n",
    "    if len(source_keys) == 0:\n",
    "        raise RuntimeError(f\"No images found for class {class_name}\")\n",
    "\n",
    "    generated_count = 0\n",
    "\n",
    "    # Progress bar\n",
    "    with tqdm(total=TARGET_IMAGES_PER_CLASS, desc=f\"Generating {class_name}\") as pbar:\n",
    "\n",
    "        while generated_count < TARGET_IMAGES_PER_CLASS:\n",
    "\n",
    "            # Pick random source image\n",
    "            source_key = random.choice(source_keys)\n",
    "\n",
    "            # Load image\n",
    "            image = load_image_from_s3(S3_BUCKET, source_key)\n",
    "\n",
    "            if image is None:\n",
    "                print(f\"âš ï¸ Failed to load {source_key}\")\n",
    "                continue\n",
    "\n",
    "            # Apply augmentation\n",
    "            augmented = augmentation_pipeline(image=image)\n",
    "            augmented_image = augmented[\"image\"]\n",
    "\n",
    "            # Build target S3 key\n",
    "            target_key = (\n",
    "                f\"{S3_TARGET_BASE}/{class_name}/\"\n",
    "                f\"{class_name}_{generated_count}.jpg\"\n",
    "            )\n",
    "\n",
    "            # Print progress every 50 images\n",
    "            if generated_count % 50 == 0:\n",
    "                print(f\"â¬†ï¸ Uploading {generated_count}/{TARGET_IMAGES_PER_CLASS}\")\n",
    "\n",
    "            # Upload augmented image\n",
    "            upload_to_s3(augmented_image, S3_BUCKET, target_key)\n",
    "\n",
    "            generated_count += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    print(f\"âœ… Completed augmentation for class: {class_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02b1bbd2-3cd2-4c0d-8cfb-91dfe26a86c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Starting augmentation for class: Tight\n",
      "ğŸ“‚ Found 46 images in tight and loose classifier/orginal dataset/Dataset/Training Data/Tight/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Tight:   0%|          | 1/1000 [00:00<01:54,  8.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 0/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Tight:   5%|â–Œ         | 52/1000 [00:05<01:40,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 50/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Tight:  10%|â–ˆ         | 102/1000 [00:09<01:14, 12.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 100/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Tight:  15%|â–ˆâ–Œ        | 152/1000 [00:13<01:06, 12.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 150/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Tight:  20%|â–ˆâ–ˆ        | 202/1000 [00:17<01:05, 12.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 200/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Tight:  25%|â–ˆâ–ˆâ–Œ       | 252/1000 [00:20<00:57, 12.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 250/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Tight:  30%|â–ˆâ–ˆâ–ˆ       | 302/1000 [00:24<00:50, 13.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 300/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Tight:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 352/1000 [00:28<00:48, 13.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 350/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Tight:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 402/1000 [00:31<00:45, 13.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 400/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Tight:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 452/1000 [00:34<00:39, 13.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 450/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Tight:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 502/1000 [00:38<00:35, 13.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 500/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Tight:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 552/1000 [00:42<00:32, 13.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 550/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Tight:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 602/1000 [00:45<00:29, 13.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 600/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Tight:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 652/1000 [00:48<00:26, 12.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 650/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Tight:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 702/1000 [00:52<00:20, 14.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 700/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Tight:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 752/1000 [00:55<00:17, 14.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 750/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Tight:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 802/1000 [00:59<00:15, 12.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 800/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Tight:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 852/1000 [01:02<00:11, 12.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 850/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Tight:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 902/1000 [01:06<00:06, 14.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 900/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Tight:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 952/1000 [01:09<00:03, 13.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 950/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Tight: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [01:13<00:00, 13.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed augmentation for class: Tight\n",
      "\n",
      "ğŸš€ Starting augmentation for class: Loose\n",
      "ğŸ“‚ Found 48 images in tight and loose classifier/orginal dataset/Dataset/Training Data/Loose/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Loose:   0%|          | 1/1000 [00:00<01:42,  9.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 0/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Loose:   5%|â–Œ         | 51/1000 [00:04<01:34, 10.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 50/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Loose:  10%|â–ˆ         | 101/1000 [00:09<01:09, 12.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 100/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Loose:  15%|â–ˆâ–Œ        | 153/1000 [00:13<01:08, 12.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 150/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Loose:  20%|â–ˆâ–ˆ        | 203/1000 [00:16<00:58, 13.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 200/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Loose:  25%|â–ˆâ–ˆâ–Œ       | 253/1000 [00:20<00:56, 13.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 250/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Loose:  30%|â–ˆâ–ˆâ–ˆ       | 301/1000 [00:23<00:54, 12.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 300/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Loose:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 353/1000 [00:27<00:46, 14.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 350/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Loose:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 403/1000 [00:31<00:42, 14.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 400/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Loose:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 453/1000 [00:34<00:39, 13.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 450/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Loose:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 503/1000 [00:38<00:36, 13.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 500/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Loose:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 553/1000 [00:41<00:31, 13.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 550/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Loose:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 603/1000 [00:45<00:28, 13.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 600/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Loose:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 653/1000 [00:48<00:25, 13.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 650/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Loose:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 703/1000 [00:52<00:22, 13.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 700/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Loose:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 753/1000 [00:55<00:16, 14.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 750/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Loose:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 803/1000 [00:59<00:15, 12.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 800/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Loose:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 853/1000 [01:03<00:10, 13.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 850/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Loose:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 903/1000 [01:06<00:07, 13.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 900/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Loose:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 953/1000 [01:10<00:03, 13.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â¬†ï¸ Uploading 950/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Loose: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [01:13<00:00, 13.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Completed augmentation for class: Loose\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data for Tight\n",
    "generate_and_upload(\"Tight\")\n",
    "\n",
    "# Generate synthetic data for Loose\n",
    "generate_and_upload(\"Loose\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfbe1fa-0ba9-43d9-aed2-4e4ceb18fde7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
