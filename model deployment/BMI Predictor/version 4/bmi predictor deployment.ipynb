{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a78b1259-df21-4eb6-98b3-9edea83fdf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote bmi_inference_code/inference.py and requirements.txt\n",
      "✅ Downloaded: s3://ai-bmi-predictor/trained-models/efficientnet-models/eff-ann-v8-training-2025-12-19-03-37-10-946/output/model.tar.gz\n",
      "✅ Found H5: bmi_model_work/extracted/eff_ann_version8.h5\n",
      "✅ Loaded Keras model\n",
      "INFO:tensorflow:Assets written to: bmi_model_work/model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: bmi_model_work/model/1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported SavedModel to: bmi_model_work/model/1\n",
      "✅ Repacked SavedModel tar: bmi_model_work/savedmodel.tar.gz\n",
      "✅ Uploaded SavedModel artifact to: s3://ai-bmi-predictor/trained-models/efficientnet-models/eff-ann-v8-training-2025-12-19-03-37-10-946/output/savedmodel/model.tar.gz\n",
      "--------!\n",
      "✅ Deployed endpoint: BMI-predcitor-v3\n",
      "   • Artifact: s3://ai-bmi-predictor/trained-models/efficientnet-models/eff-ann-v8-training-2025-12-19-03-37-10-946/output/savedmodel/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import boto3\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import tensorflow as tf\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "H5_TAR_S3_URI = \"s3://ai-bmi-predictor/trained-models/efficientnet-models/eff-ann-v8-training-2025-12-19-03-37-10-946/output/model.tar.gz\"\n",
    "ENDPOINT_NAME = \"BMI-predcitor-v3\"\n",
    "INSTANCE_TYPE = \"ml.g4dn.xlarge\"\n",
    "FRAMEWORK_VERSION = \"2.11.0\"\n",
    "\n",
    "SAVEDMODEL_TAR_S3_URI = \"s3://ai-bmi-predictor/trained-models/efficientnet-models/eff-ann-v8-training-2025-12-19-03-37-10-946/output/savedmodel/model.tar.gz\"\n",
    "\n",
    "# Existing feature extraction endpoint\n",
    "FEATURE_ENDPOINT_NAME = \"feature-extraction-efficientnetb7\"\n",
    "\n",
    "# Scalers\n",
    "SCALER_TARGETS_S3_URI = \"s3://ai-bmi-predictor/scalers/scaler_targets.pkl\"\n",
    "SCALER_ROBUST_FEATURES_S3_URI = \"s3://ai-bmi-predictor/scalers/scaler_robust_features.pkl\"\n",
    "SCALER_HEIGHT_S3_URI = \"s3://ai-bmi-predictor/scalers/scaler_standard_features.pkl\"\n",
    "\n",
    "# =========================\n",
    "# Role inference\n",
    "# =========================\n",
    "try:\n",
    "    ROLE  # type: ignore\n",
    "except NameError:\n",
    "    try:\n",
    "        from sagemaker import get_execution_role\n",
    "        ROLE = get_execution_role()\n",
    "    except Exception:\n",
    "        ROLE = None\n",
    "\n",
    "if not ROLE:\n",
    "    raise ValueError(\"ROLE is None. Set ROLE to your SageMaker execution role ARN if running outside SageMaker.\")\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def parse_s3_uri(uri: str):\n",
    "    p = urlparse(uri)\n",
    "    if p.scheme != \"s3\" or not p.netloc or not p.path:\n",
    "        raise ValueError(f\"Invalid S3 URI: {uri}\")\n",
    "    return p.netloc, p.path.lstrip(\"/\")\n",
    "\n",
    "def safe_extract(tar: tarfile.TarFile, path: str):\n",
    "    abs_path = os.path.abspath(path)\n",
    "    for member in tar.getmembers():\n",
    "        member_path = os.path.abspath(os.path.join(path, member.name))\n",
    "        if not member_path.startswith(abs_path + os.sep) and member_path != abs_path:\n",
    "            raise Exception(f\"Blocked path traversal attempt: {member.name}\")\n",
    "    tar.extractall(path)\n",
    "\n",
    "def upload_file_to_s3(local_path: str, s3_uri: str):\n",
    "    bucket, key = parse_s3_uri(s3_uri)\n",
    "    boto3.client(\"s3\").upload_file(local_path, bucket, key)\n",
    "    return f\"s3://{bucket}/{key}\"\n",
    "\n",
    "# =========================\n",
    "# 0) Write inference code BEFORE deploy\n",
    "# =========================\n",
    "source_dir = \"bmi_inference_code\"\n",
    "os.makedirs(source_dir, exist_ok=True)\n",
    "\n",
    "# requirements (installed in container)\n",
    "with open(os.path.join(source_dir, \"requirements.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"numpy\\nscikit-learn\\nrequests\\n\")\n",
    "\n",
    "# inference.py (preprocess inside endpoint)\n",
    "inference_py = r'''\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import boto3\n",
    "import requests\n",
    "\n",
    "_s3 = boto3.client(\"s3\")\n",
    "_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "FEATURE_ENDPOINT_NAME = os.environ.get(\"FEATURE_ENDPOINT_NAME\", \"feature-extraction-efficientnetb7\")\n",
    "SCALER_TARGETS_S3_URI = os.environ[\"SCALER_TARGETS_S3_URI\"]\n",
    "SCALER_ROBUST_FEATURES_S3_URI = os.environ[\"SCALER_ROBUST_FEATURES_S3_URI\"]\n",
    "SCALER_HEIGHT_S3_URI = os.environ[\"SCALER_HEIGHT_S3_URI\"]\n",
    "\n",
    "TARGET_COLS = [\n",
    "    \"ankle\", \"arm-length\", \"bicep\", \"calf\", \"chest\", \"forearm\", \"hip\",\n",
    "    \"leg-length\", \"shoulder-breadth\", \"shoulder-to-crotch\", \"thigh\",\n",
    "    \"waist\", \"wrist\", \"weight_kg\"\n",
    "]\n",
    "\n",
    "def _parse_s3_uri(uri: str):\n",
    "    if not uri.startswith(\"s3://\"):\n",
    "        raise ValueError(f\"Invalid S3 URI: {uri}\")\n",
    "    x = uri[5:]\n",
    "    bucket, key = x.split(\"/\", 1)\n",
    "    return bucket, key\n",
    "\n",
    "def _load_pickle_from_s3(s3_uri: str):\n",
    "    b, k = _parse_s3_uri(s3_uri)\n",
    "    obj = _s3.get_object(Bucket=b, Key=k)\n",
    "    return pickle.loads(obj[\"Body\"].read())\n",
    "\n",
    "# Load scalers once (cold start)\n",
    "TARGET_SCALER = _load_pickle_from_s3(SCALER_TARGETS_S3_URI)\n",
    "ROBUST_FEATURES_SCALER = _load_pickle_from_s3(SCALER_ROBUST_FEATURES_S3_URI)\n",
    "HEIGHT_SCALER = _load_pickle_from_s3(SCALER_HEIGHT_S3_URI)\n",
    "\n",
    "def _gender_to_code(g):\n",
    "    # IMPORTANT: adjust if your training mapping differs\n",
    "    s = str(g).strip().lower()\n",
    "    if s in [\"female\", \"f\", \"0\"]:\n",
    "        return 0\n",
    "    if s in [\"male\", \"m\", \"1\"]:\n",
    "        return 1\n",
    "    raise ValueError(f\"Unsupported gender: {g}\")\n",
    "\n",
    "def _get_image_bytes(payload: dict, key_b64: str, key_s3: str):\n",
    "    # Accept either base64 image or s3 uri\n",
    "    if payload.get(key_b64):\n",
    "        return base64.b64decode(payload[key_b64])\n",
    "    if payload.get(key_s3):\n",
    "        b, k = _parse_s3_uri(payload[key_s3])\n",
    "        obj = _s3.get_object(Bucket=b, Key=k)\n",
    "        return obj[\"Body\"].read()\n",
    "    raise ValueError(f\"Provide '{key_b64}' or '{key_s3}'\")\n",
    "\n",
    "def _invoke_feature_endpoint(image_bytes: bytes):\n",
    "    # Send image as base64 JSON\n",
    "    b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "    req = json.dumps({\"instances\": [{\"b64\": b64}]})\n",
    "\n",
    "    resp = _runtime.invoke_endpoint(\n",
    "        EndpointName=FEATURE_ENDPOINT_NAME,\n",
    "        ContentType=\"application/json\",\n",
    "        Accept=\"application/json\",\n",
    "        Body=req.encode(\"utf-8\"),\n",
    "    )\n",
    "    body = resp[\"Body\"].read().decode(\"utf-8\")\n",
    "    data = json.loads(body)\n",
    "\n",
    "    # Common response shapes\n",
    "    if \"predictions\" in data:\n",
    "        return np.asarray(data[\"predictions\"][0], dtype=np.float32).reshape(-1)\n",
    "    if \"features\" in data:\n",
    "        return np.asarray(data[\"features\"], dtype=np.float32).reshape(-1)\n",
    "    if isinstance(data, list):\n",
    "        return np.asarray(data, dtype=np.float32).reshape(-1)\n",
    "\n",
    "    raise ValueError(f\"Unrecognized feature endpoint response: {data}\")\n",
    "\n",
    "def handler(data, context):\n",
    "    # 1) Read JSON\n",
    "    raw = data.read()\n",
    "    payload = json.loads(raw.decode(\"utf-8\")) if raw else {}\n",
    "\n",
    "    height_cm = float(payload[\"height_cm\"])\n",
    "    gender_code = _gender_to_code(payload[\"gender\"])\n",
    "\n",
    "    front_bytes = _get_image_bytes(payload, \"front_image_b64\", \"front_image_s3\")\n",
    "    side_bytes  = _get_image_bytes(payload, \"side_image_b64\",  \"side_image_s3\")\n",
    "\n",
    "    # 2) Feature extraction (front + side)\n",
    "    front_feat = _invoke_feature_endpoint(front_bytes)\n",
    "    side_feat  = _invoke_feature_endpoint(side_bytes)\n",
    "\n",
    "    # 3) Final feature order: front_feats, side_feats, gender, height\n",
    "    cnn = np.concatenate([front_feat, side_feat], axis=0).astype(np.float32).reshape(1, -1)\n",
    "\n",
    "    cnn_scaled = ROBUST_FEATURES_SCALER.transform(cnn).astype(np.float32)\n",
    "    height_scaled = HEIGHT_SCALER.transform(np.array([[height_cm]], dtype=np.float32)).astype(np.float32)\n",
    "\n",
    "    final_x = np.concatenate(\n",
    "        [cnn_scaled, np.array([[gender_code]], dtype=np.float32), height_scaled],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # 4) Call the local TF Serving model\n",
    "    tfs_req = json.dumps({\"instances\": final_x.tolist()})\n",
    "    tfs_resp = requests.post(context.rest_uri, data=tfs_req)\n",
    "\n",
    "    if tfs_resp.status_code != 200:\n",
    "        raise ValueError(tfs_resp.content.decode(\"utf-8\"))\n",
    "\n",
    "    pred = tfs_resp.json()\n",
    "    y_scaled = np.asarray(pred[\"predictions\"], dtype=np.float32)\n",
    "\n",
    "    # 5) Inverse-scale targets\n",
    "    y = TARGET_SCALER.inverse_transform(y_scaled)[0].tolist()\n",
    "    out = {TARGET_COLS[i]: float(y[i]) for i in range(len(TARGET_COLS))}\n",
    "\n",
    "    return json.dumps(out).encode(\"utf-8\"), \"application/json\"\n",
    "'''\n",
    "with open(os.path.join(source_dir, \"inference.py\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(inference_py)\n",
    "\n",
    "print(\"✅ Wrote bmi_inference_code/inference.py and requirements.txt\")\n",
    "\n",
    "# =========================\n",
    "# 1) Download & extract H5 from tar.gz\n",
    "# =========================\n",
    "workdir = \"bmi_model_work\"\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "local_in_tar = os.path.join(workdir, \"model.tar.gz\")\n",
    "extract_dir = os.path.join(workdir, \"extracted\")\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "in_bucket, in_key = parse_s3_uri(H5_TAR_S3_URI)\n",
    "boto3.client(\"s3\").download_file(in_bucket, in_key, local_in_tar)\n",
    "print(f\"✅ Downloaded: {H5_TAR_S3_URI}\")\n",
    "\n",
    "with tarfile.open(local_in_tar, \"r:gz\") as tar:\n",
    "    safe_extract(tar, extract_dir)\n",
    "\n",
    "# Find .h5\n",
    "h5_path = None\n",
    "for root, _, files in os.walk(extract_dir):\n",
    "    for fn in files:\n",
    "        if fn.lower().endswith((\".h5\", \".hdf5\")):\n",
    "            h5_path = os.path.join(root, fn)\n",
    "            break\n",
    "    if h5_path:\n",
    "        break\n",
    "\n",
    "if not h5_path:\n",
    "    raise ValueError(\"❌ No .h5 found inside the downloaded archive.\")\n",
    "\n",
    "print(f\"✅ Found H5: {h5_path}\")\n",
    "\n",
    "# =========================\n",
    "# 2) Convert H5 -> SavedModel (model/1/)\n",
    "# =========================\n",
    "model = tf.keras.models.load_model(h5_path, compile=False)\n",
    "print(\"✅ Loaded Keras model\")\n",
    "\n",
    "serving_root = os.path.join(workdir, \"model\")\n",
    "version_dir = os.path.join(serving_root, \"1\")\n",
    "os.makedirs(version_dir, exist_ok=True)\n",
    "\n",
    "tf.saved_model.save(model, version_dir)\n",
    "print(f\"✅ Exported SavedModel to: {version_dir}\")\n",
    "\n",
    "# =========================\n",
    "# 3) Pack SavedModel tar.gz (contains model/1/...)\n",
    "# =========================\n",
    "local_out_tar = os.path.join(workdir, \"savedmodel.tar.gz\")\n",
    "with tarfile.open(local_out_tar, \"w:gz\") as tar:\n",
    "    tar.add(serving_root, arcname=\"model\")\n",
    "\n",
    "print(f\"✅ Repacked SavedModel tar: {local_out_tar}\")\n",
    "\n",
    "# =========================\n",
    "# 4) Upload SavedModel tarball\n",
    "# =========================\n",
    "uploaded_s3 = upload_file_to_s3(local_out_tar, SAVEDMODEL_TAR_S3_URI)\n",
    "print(f\"✅ Uploaded SavedModel artifact to: {uploaded_s3}\")\n",
    "\n",
    "# =========================\n",
    "# 5) Deploy endpoint (preprocess happens inside inference.py)\n",
    "# =========================\n",
    "session = sagemaker.Session()\n",
    "\n",
    "tf_model = TensorFlowModel(\n",
    "    model_data=uploaded_s3,\n",
    "    role=ROLE,\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    sagemaker_session=session,\n",
    "    entry_point=\"inference.py\",\n",
    "    source_dir=source_dir,\n",
    "    env={\n",
    "        \"FEATURE_ENDPOINT_NAME\": FEATURE_ENDPOINT_NAME,\n",
    "        \"SCALER_TARGETS_S3_URI\": SCALER_TARGETS_S3_URI,\n",
    "        \"SCALER_ROBUST_FEATURES_S3_URI\": SCALER_ROBUST_FEATURES_S3_URI,\n",
    "        \"SCALER_HEIGHT_S3_URI\": SCALER_HEIGHT_S3_URI,\n",
    "    },\n",
    ")\n",
    "\n",
    "predictor = tf_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=INSTANCE_TYPE,\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Deployed endpoint: {ENDPOINT_NAME}\")\n",
    "print(f\"   • Artifact: {uploaded_s3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88844b1e-b28a-4dc6-99f5-8ea2ed1a3fff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
