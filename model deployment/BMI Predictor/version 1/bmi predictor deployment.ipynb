{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "066f5995-7813-48d7-ad7d-5c125914de65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 11:43:05.635608: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-18 11:43:05.651091: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-12-18 11:43:05.675391: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-12-18 11:43:05.675423: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-18 11:43:05.690474: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-18 11:43:06.488618: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "✅ Downloaded: s3://ai-bmi-predictor/trained-models/efficientnet-models/eff-ann-v8-training-2025-12-18-11-02-51-500/output/model.tar.gz\n",
      "✅ Found H5: bmi_model_work/extracted/eff_ann_version8.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 11:43:11.360772: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-18 11:43:20.798560: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-18 11:43:20.801747: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-18 11:43:20.805232: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-18 11:43:20.807863: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-18 11:43:20.810395: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-18 11:43:21.253321: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-18 11:43:21.254439: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-18 11:43:21.255440: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-18 11:43:21.257222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13760 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Keras model\n",
      "INFO:tensorflow:Assets written to: bmi_model_work/model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: bmi_model_work/model/1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported SavedModel to: bmi_model_work/model/1\n",
      "✅ Repacked SavedModel tar: bmi_model_work/savedmodel.tar.gz\n",
      "✅ Uploaded SavedModel artifact to: s3://ai-bmi-predictor/trained-models/efficientnet-models/eff-ann-v8-training-2025-12-18-11-02-51-500/output/savedmodel/model.tar.gz\n",
      "---------!\n",
      "✅ Model successfully deployed to endpoint: BMI-predcitor-v8\n",
      "   • Region: eu-north-1\n",
      "   • Artifact: s3://ai-bmi-predictor/trained-models/efficientnet-models/eff-ann-v8-training-2025-12-18-11-02-51-500/output/savedmodel/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import boto3\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import tensorflow as tf\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "\n",
    "# =========================\n",
    "# Config (YOUR BMI MODEL)\n",
    "# =========================\n",
    "H5_TAR_S3_URI = \"s3://ai-bmi-predictor/trained-models/efficientnet-models/eff-ann-v8-training-2025-12-18-11-02-51-500/output/model.tar.gz\"\n",
    "ENDPOINT_NAME = \"BMI-predcitor-v8\"  # keeping your requested name\n",
    "INSTANCE_TYPE = \"ml.g4dn.xlarge\"       # change to ml.g4dn.xlarge if you really need GPU\n",
    "FRAMEWORK_VERSION = \"2.11.0\"\n",
    "\n",
    "# Where to upload the converted SavedModel tar.gz\n",
    "SAVEDMODEL_TAR_S3_URI = \"s3://ai-bmi-predictor/trained-models/efficientnet-models/eff-ann-v8-training-2025-12-18-11-02-51-500/output/savedmodel/model.tar.gz\"\n",
    "\n",
    "# =========================\n",
    "# Role inference (same pattern you had)\n",
    "# =========================\n",
    "try:\n",
    "    ROLE  # type: ignore\n",
    "except NameError:\n",
    "    try:\n",
    "        from sagemaker import get_execution_role\n",
    "        ROLE = get_execution_role()\n",
    "    except Exception:\n",
    "        ROLE = None\n",
    "\n",
    "if not ROLE:\n",
    "    raise ValueError(\"ROLE is None. Set ROLE to your SageMaker execution role ARN if running outside SageMaker.\")\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def parse_s3_uri(uri: str):\n",
    "    p = urlparse(uri)\n",
    "    if p.scheme != \"s3\" or not p.netloc or not p.path:\n",
    "        raise ValueError(f\"Invalid S3 URI: {uri}\")\n",
    "    return p.netloc, p.path.lstrip(\"/\")\n",
    "\n",
    "def safe_extract(tar: tarfile.TarFile, path: str):\n",
    "    abs_path = os.path.abspath(path)\n",
    "    for member in tar.getmembers():\n",
    "        member_path = os.path.abspath(os.path.join(path, member.name))\n",
    "        if not member_path.startswith(abs_path + os.sep) and member_path != abs_path:\n",
    "            raise Exception(f\"Blocked path traversal attempt: {member.name}\")\n",
    "    tar.extractall(path)\n",
    "\n",
    "def upload_file_to_s3(local_path: str, s3_uri: str):\n",
    "    bucket, key = parse_s3_uri(s3_uri)\n",
    "    boto3.client(\"s3\").upload_file(local_path, bucket, key)\n",
    "    return f\"s3://{bucket}/{key}\"\n",
    "\n",
    "# =========================\n",
    "# 1) Download & extract .h5 from your tar.gz\n",
    "# =========================\n",
    "workdir = \"bmi_model_work\"\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "local_in_tar = os.path.join(workdir, \"model.tar.gz\")\n",
    "extract_dir = os.path.join(workdir, \"extracted\")\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "in_bucket, in_key = parse_s3_uri(H5_TAR_S3_URI)\n",
    "boto3.client(\"s3\").download_file(in_bucket, in_key, local_in_tar)\n",
    "print(f\"✅ Downloaded: {H5_TAR_S3_URI}\")\n",
    "\n",
    "with tarfile.open(local_in_tar, \"r:gz\") as tar:\n",
    "    safe_extract(tar, extract_dir)\n",
    "\n",
    "# Find the .h5 file\n",
    "h5_path = None\n",
    "for root, _, files in os.walk(extract_dir):\n",
    "    for f in files:\n",
    "        if f.lower().endswith((\".h5\", \".hdf5\")):\n",
    "            h5_path = os.path.join(root, f)\n",
    "            break\n",
    "    if h5_path:\n",
    "        break\n",
    "\n",
    "if not h5_path:\n",
    "    raise ValueError(\"❌ No .h5 found inside the downloaded archive.\")\n",
    "\n",
    "print(f\"✅ Found H5: {h5_path}\")\n",
    "\n",
    "# =========================\n",
    "# 2) Convert H5 -> SavedModel (TF Serving layout: model/1/)\n",
    "# =========================\n",
    "model = tf.keras.models.load_model(h5_path, compile=False)\n",
    "print(\"✅ Loaded Keras model\")\n",
    "\n",
    "serving_root = os.path.join(workdir, \"model\")   # will become the tar root\n",
    "version_dir = os.path.join(serving_root, \"1\")   # TF Serving convention\n",
    "os.makedirs(version_dir, exist_ok=True)\n",
    "\n",
    "tf.saved_model.save(model, version_dir)\n",
    "print(f\"✅ Exported SavedModel to: {version_dir}\")\n",
    "\n",
    "# =========================\n",
    "# 3) Re-pack as model.tar.gz (must contain 'model/1/...' inside)\n",
    "# =========================\n",
    "local_out_tar = os.path.join(workdir, \"savedmodel.tar.gz\")\n",
    "with tarfile.open(local_out_tar, \"w:gz\") as tar:\n",
    "    tar.add(serving_root, arcname=\"model\")\n",
    "print(f\"✅ Repacked SavedModel tar: {local_out_tar}\")\n",
    "\n",
    "# =========================\n",
    "# 4) Upload converted artifact to S3 (this is what SageMaker will deploy)\n",
    "# =========================\n",
    "uploaded_s3 = upload_file_to_s3(local_out_tar, SAVEDMODEL_TAR_S3_URI)\n",
    "print(f\"✅ Uploaded SavedModel artifact to: {uploaded_s3}\")\n",
    "\n",
    "# =========================\n",
    "# 5) Deploy as SageMaker endpoint\n",
    "# =========================\n",
    "session = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "tf_model = TensorFlowModel(\n",
    "    model_data=uploaded_s3,\n",
    "    role=ROLE,\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "\n",
    "predictor = tf_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=INSTANCE_TYPE,\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Model successfully deployed to endpoint: {ENDPOINT_NAME}\")\n",
    "print(f\"   • Region: {region}\")\n",
    "print(f\"   • Artifact: {uploaded_s3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83316645-dbe0-4cf2-8d56-963b01021049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
