{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5cda281-5cb3-4b4f-9d09-e07e091222ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote deploy_src/code/inference.py\n",
      "✅ Downloaded: s3://ai-bmi-predictor-v2/image-segmentation/sapiens/sapiens_2b_goliath_best_goliath_mIoU_8179_epoch_181_torchscript.pt2\n",
      "✅ Created: model.tar.gz\n",
      "✅ Uploaded packaged model to: s3://ai-bmi-predictor-v2/image-segmentation/sapiens/sagemaker/model.tar.gz\n",
      "--------!\n",
      "[STEP 6] Enabling autoscaling (Option A: min>=1, max=N)...\n",
      "✅ Deployed endpoint: sapiens-segmentation-endpoint (eu-west-2)\n",
      "✅ Autoscaling enabled for variant: AllTraffic\n",
      "   Min: 1  Max: 4\n",
      "   Target Invocations/Instance: 20.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import boto3\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import image_uris\n",
    "\n",
    "# ========= CONFIG =========\n",
    "MODEL_PT2_S3 = \"s3://ai-bmi-predictor-v2/image-segmentation/sapiens/sapiens_2b_goliath_best_goliath_mIoU_8179_epoch_181_torchscript.pt2\"\n",
    "PACKAGED_MODEL_S3 = \"s3://ai-bmi-predictor-v2/image-segmentation/sapiens/sagemaker/model.tar.gz\"\n",
    "\n",
    "ENDPOINT_NAME = \"sapiens-segmentation-endpoint\"\n",
    "INSTANCE_TYPE = \"ml.g4dn.4xlarge\"      # GPU recommended\n",
    "PYTORCH_VERSION = \"2.6\"\n",
    "PY_VERSION = \"py312\"\n",
    "\n",
    "# ---- Option A autoscaling settings (min >= 1) ----\n",
    "AUTOSCALING_MIN_CAPACITY = 1\n",
    "AUTOSCALING_MAX_CAPACITY = 4\n",
    "TARGET_INVOCATIONS_PER_INSTANCE = 20.0   # GPU models often need a lower target; tune later\n",
    "SCALE_OUT_COOLDOWN_SECONDS = 60\n",
    "SCALE_IN_COOLDOWN_SECONDS = 300\n",
    "\n",
    "# ========= ROLE =========\n",
    "try:\n",
    "    from sagemaker import get_execution_role\n",
    "    ROLE = get_execution_role()\n",
    "except Exception:\n",
    "    ROLE = None\n",
    "if not ROLE:\n",
    "    raise ValueError(\"ROLE is None. Set ROLE to your SageMaker execution role ARN if running outside SageMaker.\")\n",
    "\n",
    "# ========= HELPERS =========\n",
    "def parse_s3_uri(uri: str):\n",
    "    p = urlparse(uri)\n",
    "    if p.scheme != \"s3\" or not p.netloc or not p.path:\n",
    "        raise ValueError(f\"Invalid S3 URI: {uri}\")\n",
    "    return p.netloc, p.path.lstrip(\"/\")\n",
    "\n",
    "def s3_download(s3_uri: str, local_path: str):\n",
    "    b, k = parse_s3_uri(s3_uri)\n",
    "    boto3.client(\"s3\").download_file(b, k, local_path)\n",
    "\n",
    "def s3_upload(local_path: str, s3_uri: str):\n",
    "    b, k = parse_s3_uri(s3_uri)\n",
    "    boto3.client(\"s3\").upload_file(local_path, b, k)\n",
    "\n",
    "# ========= 1) Create inference.py =========\n",
    "os.makedirs(\"deploy_src/code\", exist_ok=True)\n",
    "\n",
    "inference_py = r'''\n",
    "import base64\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "_preprocess = transforms.Compose([\n",
    "    transforms.Resize((1024, 768)),  # (H, W)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                         std=(0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "def _load_model(model_path: str):\n",
    "    # Your local code: torch.jit.load first\n",
    "    try:\n",
    "        return torch.jit.load(model_path)\n",
    "    except Exception as e1:\n",
    "        # Fallback: PT2 archive\n",
    "        try:\n",
    "            ep = torch.export.load(model_path)\n",
    "            return ep.module()\n",
    "        except Exception as e2:\n",
    "            raise RuntimeError(\n",
    "                f\"Failed to load model as TorchScript and PT2.\\nTorchScript error: {e1}\\nPT2 error: {e2}\"\n",
    "            )\n",
    "\n",
    "def model_fn(model_dir: str):\n",
    "    candidates = []\n",
    "    for f in os.listdir(model_dir):\n",
    "        if f.endswith((\".pt2\", \".pt\", \".pth\")):\n",
    "            candidates.append(os.path.join(model_dir, f))\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"No .pt2/.pt/.pth found in {model_dir}\")\n",
    "\n",
    "    model_path = sorted(candidates)[0]\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = _load_model(model_path).eval().to(device)\n",
    "    return {\"model\": model, \"device\": device}\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    if request_content_type == \"application/json\":\n",
    "        payload = json.loads(request_body)\n",
    "        if \"image_b64\" not in payload:\n",
    "            raise ValueError(\"JSON must include 'image_b64'.\")\n",
    "        img_bytes = base64.b64decode(payload[\"image_b64\"])\n",
    "        return Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
    "\n",
    "    if request_content_type.startswith(\"image/\") or request_content_type == \"application/x-image\":\n",
    "        return Image.open(io.BytesIO(request_body)).convert(\"RGB\")\n",
    "\n",
    "    raise ValueError(f\"Unsupported content type: {request_content_type}\")\n",
    "\n",
    "def predict_fn(img: Image.Image, model_bundle):\n",
    "    model = model_bundle[\"model\"]\n",
    "    device = model_bundle[\"device\"]\n",
    "\n",
    "    orig_w, orig_h = img.size  # (W, H)\n",
    "    x = _preprocess(img).unsqueeze(0).to(device)  # (1, C, 1024, 768)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out = model(x)\n",
    "\n",
    "    # Match your code: logits_small = output[0]\n",
    "    if isinstance(out, (tuple, list)):\n",
    "        logits_small = out[0]\n",
    "    else:\n",
    "        logits_small = out\n",
    "\n",
    "    if logits_small.ndim == 4:\n",
    "        logits_small = logits_small[0]  # (C, h, w)\n",
    "\n",
    "    logits_small = logits_small.to(\"cpu\")\n",
    "\n",
    "    logits = F.interpolate(\n",
    "        logits_small.unsqueeze(0),\n",
    "        size=(orig_h, orig_w),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    ).squeeze(0)\n",
    "\n",
    "    seg = logits.argmax(dim=0).numpy().astype(np.uint8)\n",
    "    body_mask_bw = (seg != 0).astype(np.uint8) * 255\n",
    "\n",
    "    mask_img = Image.fromarray(body_mask_bw, mode=\"L\")\n",
    "    buf = io.BytesIO()\n",
    "    mask_img.save(buf, format=\"PNG\")\n",
    "\n",
    "    return {\n",
    "        \"mask_png_b64\": base64.b64encode(buf.getvalue()).decode(\"utf-8\"),\n",
    "        \"mask_shape_hw\": [int(orig_h), int(orig_w)],\n",
    "    }\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    if accept == \"application/json\":\n",
    "        return json.dumps(prediction), accept\n",
    "    raise ValueError(f\"Unsupported accept type: {accept}\")\n",
    "'''\n",
    "\n",
    "with open(\"deploy_src/code/inference.py\", \"w\") as f:\n",
    "    f.write(inference_py)\n",
    "\n",
    "print(\"✅ Wrote deploy_src/code/inference.py\")\n",
    "\n",
    "# ========= 2) Download model =========\n",
    "local_model = \"deploy_src/model.pt2\"\n",
    "s3_download(MODEL_PT2_S3, local_model)\n",
    "print(\"✅ Downloaded:\", MODEL_PT2_S3)\n",
    "\n",
    "# ========= 3) Create model.tar.gz =========\n",
    "tar_path = \"model.tar.gz\"\n",
    "with tarfile.open(tar_path, \"w:gz\") as tar:\n",
    "    tar.add(local_model, arcname=\"model.pt2\")\n",
    "    tar.add(\"deploy_src/code\", arcname=\"code\")\n",
    "print(\"✅ Created:\", tar_path)\n",
    "\n",
    "# ========= 4) Upload =========\n",
    "s3_upload(tar_path, PACKAGED_MODEL_S3)\n",
    "print(\"✅ Uploaded packaged model to:\", PACKAGED_MODEL_S3)\n",
    "\n",
    "# ========= 5) Deploy =========\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "image_uri = image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version=PYTORCH_VERSION,\n",
    "    py_version=PY_VERSION,\n",
    "    image_scope=\"inference\",\n",
    "    instance_type=INSTANCE_TYPE,\n",
    ")\n",
    "\n",
    "model = PyTorchModel(\n",
    "    model_data=PACKAGED_MODEL_S3,\n",
    "    role=ROLE,\n",
    "    entry_point=\"inference.py\",\n",
    "    source_dir=\"deploy_src/code\",\n",
    "    framework_version=PYTORCH_VERSION,\n",
    "    py_version=PY_VERSION,\n",
    "    image_uri=image_uri,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=INSTANCE_TYPE,\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    ")\n",
    "\n",
    "# ========= 6) Enable autoscaling (Option A) =========\n",
    "print(\"\\n[STEP 6] Enabling autoscaling (Option A: min>=1, max=N)...\")\n",
    "\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "aas = boto3.client(\"application-autoscaling\")\n",
    "\n",
    "ep_desc = sm.describe_endpoint(EndpointName=ENDPOINT_NAME)\n",
    "epc_desc = sm.describe_endpoint_config(EndpointConfigName=ep_desc[\"EndpointConfigName\"])\n",
    "variant_name = epc_desc[\"ProductionVariants\"][0][\"VariantName\"]\n",
    "\n",
    "resource_id = f\"endpoint/{ENDPOINT_NAME}/variant/{variant_name}\"\n",
    "\n",
    "aas.register_scalable_target(\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    MinCapacity=AUTOSCALING_MIN_CAPACITY,\n",
    "    MaxCapacity=AUTOSCALING_MAX_CAPACITY,\n",
    ")\n",
    "\n",
    "aas.put_scaling_policy(\n",
    "    PolicyName=f\"{ENDPOINT_NAME}-invocations-tt\",\n",
    "    ServiceNamespace=\"sagemaker\",\n",
    "    ResourceId=resource_id,\n",
    "    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n",
    "    PolicyType=\"TargetTrackingScaling\",\n",
    "    TargetTrackingScalingPolicyConfiguration={\n",
    "        \"PredefinedMetricSpecification\": {\n",
    "            \"PredefinedMetricType\": \"SageMakerVariantInvocationsPerInstance\"\n",
    "        },\n",
    "        \"TargetValue\": TARGET_INVOCATIONS_PER_INSTANCE,\n",
    "        \"ScaleOutCooldown\": SCALE_OUT_COOLDOWN_SECONDS,\n",
    "        \"ScaleInCooldown\": SCALE_IN_COOLDOWN_SECONDS,\n",
    "        \"DisableScaleIn\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"✅ Deployed endpoint: {ENDPOINT_NAME} ({region})\")\n",
    "print(f\"✅ Autoscaling enabled for variant: {variant_name}\")\n",
    "print(f\"   Min: {AUTOSCALING_MIN_CAPACITY}  Max: {AUTOSCALING_MAX_CAPACITY}\")\n",
    "print(f\"   Target Invocations/Instance: {TARGET_INVOCATIONS_PER_INSTANCE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74391be3-5dd7-44c0-a0c4-beb41e59dc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
