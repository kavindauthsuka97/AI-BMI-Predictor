{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d327c46b-233c-437b-b13d-41ab36af9bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_S3_URI: s3://ai-bmi-predictor/feature-extraction-data/feature-extraction-efficientnetb7-6/model.tar.gz\n",
      "✅ Using SageMaker Role: arn:aws:iam::252375266853:role/service-role/AmazonSageMaker-ExecutionRole-20250911T180987\n",
      "\n",
      "[STEP 1] Loading EfficientNetB7 model...\n",
      "✅ Feature extractor output shape: (None, 2560)\n",
      "\n",
      "[STEP 1.5] Creating bytes->image->features SavedModel wrapper...\n",
      "\n",
      "[STEP 2] Saving model in TensorFlow SavedModel format...\n",
      "INFO:tensorflow:Assets written to: efficientnet_work/model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: efficientnet_work/model/1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved to: efficientnet_work/model/1\n",
      "\n",
      "[STEP 3] Creating model.tar.gz...\n",
      "✅ Tarball created: efficientnet_work/model.tar.gz\n",
      "\n",
      "[STEP 4] Uploading to S3: s3://ai-bmi-predictor/feature-extraction-data/feature-extraction-efficientnetb7-6/model.tar.gz\n",
      "✅ Model uploaded to: s3://ai-bmi-predictor/feature-extraction-data/feature-extraction-efficientnetb7-6/model.tar.gz\n",
      "\n",
      "[STEP 5] Deploying model to endpoint: feature-extraction-efficientnetb7-6\n",
      "--------!\n",
      "✅✅✅ SUCCESS! ✅✅✅\n",
      "Endpoint Name: feature-extraction-efficientnetb7-6\n",
      "Region: eu-north-1\n",
      "Model S3 URI: s3://ai-bmi-predictor/feature-extraction-data/feature-extraction-efficientnetb7-6/model.tar.gz\n",
      "\n",
      "Call format: {'instances':[{'image_bytes': {'b64':'<base64>'}}]}\n"
     ]
    }
   ],
   "source": [
    "import os  # file ops\n",
    "import tarfile  # tar.gz create\n",
    "import boto3  # AWS SDK\n",
    "import sagemaker  # SageMaker SDK\n",
    "from sagemaker.tensorflow import TensorFlowModel  # TF model wrapper\n",
    "\n",
    "import tensorflow as tf  # TensorFlow\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB7, preprocess_input\n",
    "from tensorflow.keras.models import Model  # Model class\n",
    "\n",
    "# =========================\n",
    "# Configuration\n",
    "# =========================\n",
    "S3_BUCKET = \"ai-bmi-predictor\"\n",
    "S3_PREFIX = \"feature-extraction-data\"\n",
    "ENDPOINT_NAME = \"feature-extraction-efficientnetb7-6\"\n",
    "INSTANCE_TYPE = \"ml.g4dn.xlarge\"\n",
    "FRAMEWORK_VERSION = \"2.11.0\"\n",
    "\n",
    "# versioned model path (recommended)\n",
    "MODEL_S3_URI = f\"s3://{S3_BUCKET}/{S3_PREFIX}/{ENDPOINT_NAME}/model.tar.gz\"\n",
    "print(f\"MODEL_S3_URI: {MODEL_S3_URI}\")\n",
    "\n",
    "# =========================\n",
    "# Get SageMaker execution role\n",
    "# =========================\n",
    "try:\n",
    "    ROLE  # type: ignore\n",
    "except NameError:\n",
    "    try:\n",
    "        from sagemaker import get_execution_role\n",
    "        ROLE = get_execution_role()\n",
    "    except Exception:\n",
    "        ROLE = None\n",
    "\n",
    "if not ROLE:\n",
    "    raise ValueError(\"ROLE is None. Set ROLE to your SageMaker execution role ARN.\")\n",
    "\n",
    "print(f\"✅ Using SageMaker Role: {ROLE}\")\n",
    "\n",
    "# =========================\n",
    "# Step 1: Build EfficientNetB7 feature extractor\n",
    "# =========================\n",
    "print(\"\\n[STEP 1] Loading EfficientNetB7 model...\")\n",
    "\n",
    "base_model = EfficientNetB7(weights=\"imagenet\")\n",
    "feature_extractor = Model(inputs=base_model.inputs, outputs=base_model.layers[-2].output)\n",
    "\n",
    "print(f\"✅ Feature extractor output shape: {feature_extractor.output_shape}\")\n",
    "\n",
    "# =========================\n",
    "# Step 1.5: Wrap as SavedModel that accepts BYTES input named 'image_bytes'\n",
    "# TF Serving will decode {\"b64\":\"...\"} into raw bytes automatically.\n",
    "# =========================\n",
    "print(\"\\n[STEP 1.5] Creating bytes->image->features SavedModel wrapper...\")\n",
    "\n",
    "class FeatureServingModule(tf.Module):\n",
    "    def __init__(self, extractor):\n",
    "        super().__init__()\n",
    "        self.extractor = extractor\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string, name=\"image_bytes\")])\n",
    "    def serving_default(self, image_bytes):\n",
    "        # image_bytes: batch of raw image bytes (png/jpg), already decoded by TF Serving\n",
    "\n",
    "        def _decode_resize(x):\n",
    "            img = tf.io.decode_image(x, channels=3, expand_animations=False)\n",
    "            img = tf.image.resize(img, (600, 600))\n",
    "            img = tf.cast(img, tf.float32)\n",
    "            return img\n",
    "\n",
    "        images = tf.map_fn(\n",
    "            _decode_resize,\n",
    "            image_bytes,\n",
    "            fn_output_signature=tf.TensorSpec(shape=(600, 600, 3), dtype=tf.float32),\n",
    "        )\n",
    "\n",
    "        images = preprocess_input(images)\n",
    "        feats = self.extractor(images, training=False)  # (batch, 2560)\n",
    "\n",
    "        return {\"features\": feats}\n",
    "\n",
    "serving_module = FeatureServingModule(feature_extractor)\n",
    "\n",
    "# =========================\n",
    "# Step 2: Save as TensorFlow SavedModel format (model/1/)\n",
    "# =========================\n",
    "print(\"\\n[STEP 2] Saving model in TensorFlow SavedModel format...\")\n",
    "\n",
    "workdir = \"efficientnet_work\"\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "serving_root = os.path.join(workdir, \"model\")\n",
    "version_dir = os.path.join(serving_root, \"1\")\n",
    "os.makedirs(version_dir, exist_ok=True)\n",
    "\n",
    "tf.saved_model.save(\n",
    "    serving_module,\n",
    "    version_dir,\n",
    "    signatures={\"serving_default\": serving_module.serving_default},\n",
    ")\n",
    "\n",
    "print(f\"✅ Model saved to: {version_dir}\")\n",
    "\n",
    "# =========================\n",
    "# Step 3: Create tarball (model.tar.gz)\n",
    "# =========================\n",
    "print(\"\\n[STEP 3] Creating model.tar.gz...\")\n",
    "\n",
    "tarball_path = os.path.join(workdir, \"model.tar.gz\")\n",
    "with tarfile.open(tarball_path, \"w:gz\") as tar:\n",
    "    tar.add(serving_root, arcname=\"model\")\n",
    "\n",
    "print(f\"✅ Tarball created: {tarball_path}\")\n",
    "\n",
    "# =========================\n",
    "# Step 4: Upload tarball to S3\n",
    "# =========================\n",
    "print(f\"\\n[STEP 4] Uploading to S3: {MODEL_S3_URI}\")\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "key = f\"{S3_PREFIX}/{ENDPOINT_NAME}/model.tar.gz\"\n",
    "s3_client.upload_file(tarball_path, S3_BUCKET, key)\n",
    "\n",
    "print(f\"✅ Model uploaded to: {MODEL_S3_URI}\")\n",
    "\n",
    "# =========================\n",
    "# Step 5: Deploy to SageMaker endpoint (PURE TF SERVING)\n",
    "# =========================\n",
    "print(f\"\\n[STEP 5] Deploying model to endpoint: {ENDPOINT_NAME}\")\n",
    "\n",
    "session = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "tf_model = TensorFlowModel(\n",
    "    model_data=MODEL_S3_URI,\n",
    "    role=ROLE,\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "\n",
    "predictor = tf_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=INSTANCE_TYPE,\n",
    "    endpoint_name=ENDPOINT_NAME,\n",
    ")\n",
    "\n",
    "print(\"\\n✅✅✅ SUCCESS! ✅✅✅\")\n",
    "print(f\"Endpoint Name: {ENDPOINT_NAME}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Model S3 URI: {MODEL_S3_URI}\")\n",
    "print(\"\\nCall format: {'instances':[{'image_bytes': {'b64':'<base64>'}}]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b446f2ed-75aa-475a-9e06-355661fff8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
