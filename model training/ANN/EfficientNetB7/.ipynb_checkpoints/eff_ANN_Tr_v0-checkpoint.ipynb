{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0a089bb-ebd7-4d32-b8ce-89424733811e",
   "metadata": {},
   "source": [
    "1. load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ca32a18-ee45-42b9-848f-47c53613db38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>photo_id</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>hip</th>\n",
       "      <th>leg-length</th>\n",
       "      <th>shoulder-breadth</th>\n",
       "      <th>shoulder-to-crotch</th>\n",
       "      <th>thigh</th>\n",
       "      <th>waist</th>\n",
       "      <th>wrist</th>\n",
       "      <th>gender</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>weight_kg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6ab1d061f51c6079633aeceed2faeb0b</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.108145</td>\n",
       "      <td>-0.138813</td>\n",
       "      <td>0.633156</td>\n",
       "      <td>0.346266</td>\n",
       "      <td>-0.046055</td>\n",
       "      <td>0.016021</td>\n",
       "      <td>-0.058632</td>\n",
       "      <td>0.097968</td>\n",
       "      <td>...</td>\n",
       "      <td>105.333900</td>\n",
       "      <td>76.817467</td>\n",
       "      <td>35.362858</td>\n",
       "      <td>65.993683</td>\n",
       "      <td>54.459591</td>\n",
       "      <td>88.813789</td>\n",
       "      <td>16.764332</td>\n",
       "      <td>female</td>\n",
       "      <td>170.50</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e94e2e05fb8b099955bbc4fa5ce81e22</td>\n",
       "      <td>0.020843</td>\n",
       "      <td>0.026005</td>\n",
       "      <td>-0.093442</td>\n",
       "      <td>0.736929</td>\n",
       "      <td>0.240569</td>\n",
       "      <td>0.089982</td>\n",
       "      <td>-0.112391</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>-0.076110</td>\n",
       "      <td>...</td>\n",
       "      <td>101.478989</td>\n",
       "      <td>85.154358</td>\n",
       "      <td>37.256760</td>\n",
       "      <td>65.861588</td>\n",
       "      <td>52.773052</td>\n",
       "      <td>89.176338</td>\n",
       "      <td>15.690955</td>\n",
       "      <td>male</td>\n",
       "      <td>178.30</td>\n",
       "      <td>71.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ba6951a4f37fc9302243370e927a02e2</td>\n",
       "      <td>0.014542</td>\n",
       "      <td>-0.071332</td>\n",
       "      <td>-0.154407</td>\n",
       "      <td>0.577781</td>\n",
       "      <td>0.196485</td>\n",
       "      <td>-0.125341</td>\n",
       "      <td>-0.056713</td>\n",
       "      <td>-0.027295</td>\n",
       "      <td>0.094879</td>\n",
       "      <td>...</td>\n",
       "      <td>97.488243</td>\n",
       "      <td>81.410393</td>\n",
       "      <td>37.503147</td>\n",
       "      <td>66.042679</td>\n",
       "      <td>57.059261</td>\n",
       "      <td>82.201988</td>\n",
       "      <td>16.686253</td>\n",
       "      <td>male</td>\n",
       "      <td>176.25</td>\n",
       "      <td>76.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>947d16539d4702427aa74f737329ffb9</td>\n",
       "      <td>0.041775</td>\n",
       "      <td>0.075746</td>\n",
       "      <td>-0.128497</td>\n",
       "      <td>0.485010</td>\n",
       "      <td>0.120409</td>\n",
       "      <td>0.011227</td>\n",
       "      <td>0.017852</td>\n",
       "      <td>-0.089796</td>\n",
       "      <td>-0.011273</td>\n",
       "      <td>...</td>\n",
       "      <td>120.586845</td>\n",
       "      <td>69.361534</td>\n",
       "      <td>34.084633</td>\n",
       "      <td>60.413330</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>102.323845</td>\n",
       "      <td>17.693762</td>\n",
       "      <td>female</td>\n",
       "      <td>152.10</td>\n",
       "      <td>88.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9326695bf62926ec22690f576a633bba</td>\n",
       "      <td>0.004397</td>\n",
       "      <td>0.058590</td>\n",
       "      <td>-0.154224</td>\n",
       "      <td>0.528140</td>\n",
       "      <td>0.290956</td>\n",
       "      <td>-0.108486</td>\n",
       "      <td>-0.021441</td>\n",
       "      <td>-0.099909</td>\n",
       "      <td>0.080770</td>\n",
       "      <td>...</td>\n",
       "      <td>110.543564</td>\n",
       "      <td>77.160583</td>\n",
       "      <td>38.086231</td>\n",
       "      <td>68.400543</td>\n",
       "      <td>57.172279</td>\n",
       "      <td>107.378578</td>\n",
       "      <td>16.594791</td>\n",
       "      <td>male</td>\n",
       "      <td>171.50</td>\n",
       "      <td>88.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           photo_id        f1        f2        f3        f4  \\\n",
       "0  6ab1d061f51c6079633aeceed2faeb0b  0.000068  0.108145 -0.138813  0.633156   \n",
       "1  e94e2e05fb8b099955bbc4fa5ce81e22  0.020843  0.026005 -0.093442  0.736929   \n",
       "2  ba6951a4f37fc9302243370e927a02e2  0.014542 -0.071332 -0.154407  0.577781   \n",
       "3  947d16539d4702427aa74f737329ffb9  0.041775  0.075746 -0.128497  0.485010   \n",
       "4  9326695bf62926ec22690f576a633bba  0.004397  0.058590 -0.154224  0.528140   \n",
       "\n",
       "         f5        f6        f7        f8        f9  ...         hip  \\\n",
       "0  0.346266 -0.046055  0.016021 -0.058632  0.097968  ...  105.333900   \n",
       "1  0.240569  0.089982 -0.112391  0.000435 -0.076110  ...  101.478989   \n",
       "2  0.196485 -0.125341 -0.056713 -0.027295  0.094879  ...   97.488243   \n",
       "3  0.120409  0.011227  0.017852 -0.089796 -0.011273  ...  120.586845   \n",
       "4  0.290956 -0.108486 -0.021441 -0.099909  0.080770  ...  110.543564   \n",
       "\n",
       "   leg-length  shoulder-breadth  shoulder-to-crotch      thigh       waist  \\\n",
       "0   76.817467         35.362858           65.993683  54.459591   88.813789   \n",
       "1   85.154358         37.256760           65.861588  52.773052   89.176338   \n",
       "2   81.410393         37.503147           66.042679  57.059261   82.201988   \n",
       "3   69.361534         34.084633           60.413330  65.000000  102.323845   \n",
       "4   77.160583         38.086231           68.400543  57.172279  107.378578   \n",
       "\n",
       "       wrist  gender  height_cm  weight_kg  \n",
       "0  16.764332  female     170.50       72.0  \n",
       "1  15.690955    male     178.30       71.8  \n",
       "2  16.686253    male     176.25       76.5  \n",
       "3  17.693762  female     152.10       88.9  \n",
       "4  16.594791    male     171.50       88.4  \n",
       "\n",
       "[5 rows x 5138 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "bucket = \"ai-bmi-predictor\"\n",
    "key = \"data/eff_training.csv\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "\n",
    "data = pd.read_csv(obj[\"Body\"])\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04ca77b-fb2a-4f3c-9897-29c1e84fcde0",
   "metadata": {},
   "source": [
    "2. data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee9d28a-110f-4e9d-9936-f18919462e0e",
   "metadata": {},
   "source": [
    "2.1. categorical encoding for 'gender' feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89a49b37-abd8-4b37-8802-2c181918c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                 # import pandas for data handling\n",
    "\n",
    "data['gender'] = data['gender'].astype('category')  # convert 'gender' values to categorical type\n",
    "data['gender'] = data['gender'].cat.codes           # replace 'gender' with its numeric category codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73940a3b-8471-40f4-93c3-d720861de107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    1\n",
       "3    0\n",
       "4    1\n",
       "Name: gender, dtype: int8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['gender'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9787987-59e1-42a5-be40-d041b7ec47b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['height_cm'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054494a2-7444-472e-837b-ab49865ccca7",
   "metadata": {},
   "source": [
    "2.2. define weight frequencies for class imbalance issue for weight_kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31e3f129-bcd9-4eb8-b9c6-045dc0d70955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total samples in dataset: 6134\n",
      "\n",
      "Class frequencies:\n",
      "Class 1 (weight_kg < 60): 1049\n",
      "Class 2 (weight_kg > 100): 514\n",
      "Class 3 (60 <= weight_kg <= 100): 4571\n",
      "\n",
      "Number of classes: 3\n",
      "\n",
      "Class weights (inverse frequency):\n",
      "Weight for Class 1 (weight_kg < 60): 1.9491579281855735\n",
      "Weight for Class 2 (weight_kg > 100): 3.9779507133592737\n",
      "Weight for Class 3 (60 <= weight_kg <= 100): 0.4473127689054182\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd                     # import pandas for data handling\n",
    "import numpy as np                      # import numpy to help with safe division\n",
    "\n",
    "# Assume 'data' is your DataFrame and already loaded\n",
    "#print(\"Preview of data:\\n\", data.head())  # print first few rows to check data\n",
    "print(\"\\nTotal samples in dataset:\", len(data))  # print total number of rows\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Create boolean masks for the three weight_kg classes\n",
    "# -----------------------------\n",
    "class_1_mask = data['weight_kg'] < 60                      # True where weight_kg is less than 60\n",
    "class_2_mask = data['weight_kg'] > 100                     # True where weight_kg is greater than 100\n",
    "class_3_mask = (data['weight_kg'] >= 60) & (data['weight_kg'] <= 100)  # True where weight is between 60 and 100\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Calculate class frequencies (counts)\n",
    "# -----------------------------\n",
    "freq_class_1 = class_1_mask.sum()          # number of samples with weight_kg < 60\n",
    "freq_class_2 = class_2_mask.sum()          # number of samples with weight_kg > 100\n",
    "freq_class_3 = class_3_mask.sum()          # number of samples with 60 <= weight_kg <= 100\n",
    "\n",
    "print(\"\\nClass frequencies:\")              # header for clarity\n",
    "print(\"Class 1 (weight_kg < 60):\", freq_class_1)   # print frequency of class 1\n",
    "print(\"Class 2 (weight_kg > 100):\", freq_class_2)  # print frequency of class 2\n",
    "print(\"Class 3 (60 <= weight_kg <= 100):\", freq_class_3)  # print frequency of class 3\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Number of classes according to the strategy\n",
    "# -----------------------------\n",
    "num_classes = 3                             # we defined three classes by the rules above\n",
    "print(\"\\nNumber of classes:\", num_classes)  # print number of classes\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Compute inverse-frequency weights for each class\n",
    "#    Formula: w = total_samples / (num_classes * class_frequency)\n",
    "# -----------------------------\n",
    "total_samples = len(data)                   # total number of rows in the dataset\n",
    "\n",
    "def safe_weight(class_freq):                # helper function to avoid division by zero\n",
    "    if class_freq == 0:                     # check if a class has zero samples\n",
    "        return np.nan                       # return NaN if no samples exist for that class\n",
    "    return total_samples / (num_classes * class_freq)  # apply weighting formula\n",
    "\n",
    "weight_class_1 = safe_weight(freq_class_1)  # compute weight for class 1\n",
    "weight_class_2 = safe_weight(freq_class_2)  # compute weight for class 2\n",
    "weight_class_3 = safe_weight(freq_class_3)  # compute weight for class 3\n",
    "\n",
    "print(\"\\nClass weights (inverse frequency):\")          # header for class weights\n",
    "print(\"Weight for Class 1 (weight_kg < 60):\", weight_class_1)   # print weight of class 1\n",
    "print(\"Weight for Class 2 (weight_kg > 100):\", weight_class_2)  # print weight of class 2\n",
    "print(\"Weight for Class 3 (60 <= weight_kg <= 100):\", weight_class_3)  # print weight of class 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6008f8b-e977-4bef-ae79-33879ec53c33",
   "metadata": {},
   "source": [
    "2.3. define weight frequencies for class imbalance issue for gender feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f089c02-aff3-4c91-b679-be4b012a9e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of gender column:\n",
      " 0    0\n",
      "1    1\n",
      "2    1\n",
      "3    0\n",
      "4    1\n",
      "Name: gender, dtype: int8\n",
      "\n",
      "Class frequencies for gender:\n",
      "Class 1: 3650\n",
      "Class 0: 2484\n",
      "\n",
      "Number of gender classes: 2\n",
      "\n",
      "Class weights (inverse frequency) for gender:\n",
      "Weight for class 1: 0.8402739726027397\n",
      "Weight for class 0: 1.2347020933977455\n"
     ]
    }
   ],
   "source": [
    "import numpy as np                                      # import numpy for numeric utilities (like NaN)\n",
    "\n",
    "print(\"Preview of gender column:\\n\", data['gender'].head())  # show first few gender values to inspect\n",
    "\n",
    "# -----------------------------------\n",
    "# 1. Calculate class frequencies for gender\n",
    "# -----------------------------------\n",
    "gender_counts = data['gender'].value_counts()           # count how many samples belong to each gender class\n",
    "\n",
    "print(\"\\nClass frequencies for gender:\")                # header for class frequency output\n",
    "for gender_class, freq in gender_counts.items():        # loop over each gender class and its frequency\n",
    "    print(f\"Class {gender_class}: {freq}\")              # print the class label and its frequency\n",
    "\n",
    "# -----------------------------------\n",
    "# 2. Number of gender classes\n",
    "# -----------------------------------\n",
    "num_gender_classes = len(gender_counts)                 # compute how many distinct gender classes we have\n",
    "print(\"\\nNumber of gender classes:\", num_gender_classes)  # print number of gender classes\n",
    "\n",
    "# -----------------------------------\n",
    "# 3. Compute inverse-frequency weights for each gender class\n",
    "#    Formula: w = total_samples / (num_classes * class_frequency)\n",
    "# -----------------------------------\n",
    "total_samples = len(data)                               # total number of samples in the dataset\n",
    "\n",
    "def safe_weight(class_freq):                            # define helper function to compute class weight safely\n",
    "    if class_freq == 0:                                 # check for zero frequency to avoid division by zero\n",
    "        return np.nan                                   # return NaN if a class somehow has zero samples\n",
    "    return total_samples / (num_gender_classes * class_freq)  # apply the inverse-frequency weight formula\n",
    "\n",
    "gender_weights = {}                                     # create an empty dictionary to store weights per class\n",
    "for gender_class, freq in gender_counts.items():        # loop through each gender class and its frequency\n",
    "    gender_weights[gender_class] = safe_weight(freq)    # compute and store the weight for this gender class\n",
    "\n",
    "print(\"\\nClass weights (inverse frequency) for gender:\")  # header for weight output\n",
    "for gender_class, weight in gender_weights.items():     # loop over each class and its weight\n",
    "    print(f\"Weight for class {gender_class}: {weight}\") # print the computed weight for this gender class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b981d197-ad20-473e-a854-ffca369ef51e",
   "metadata": {},
   "source": [
    "2.4. weight frequencies for weight classes and gender classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bc2d86a-9f9f-4ab2-8749-8dc2bd7644a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight-class weights: {'weight_<60': 1.9491579281855735, 'weight_>100': 3.9779507133592737, 'weight_60_100': 0.4473127689054182}\n",
      "Gender-class weights: {1: 0.8402739726027397, 0: 1.2347020933977455}\n",
      "\n",
      "Combined weights for each (weight_class, gender_class):\n",
      "weight_<60 & gender 1: 1.6378266755466175\n",
      "weight_<60 & gender 0: 2.4066293742935403\n",
      "weight_>100 & gender 1: 3.3425684487322993\n",
      "weight_>100 & gender 0: 4.9115840732177505\n",
      "weight_60_100 & gender 1: 0.37586527732408703\n",
      "weight_60_100 & gender 0: 0.5522980121710618\n"
     ]
    }
   ],
   "source": [
    "import numpy as np   # import numpy for numeric operations\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Store the already-computed weights for weight classes\n",
    "#    (use the variables you created when handling weight_kg)\n",
    "# -------------------------------------------------\n",
    "weight_class_weights = {                          # dictionary to hold weight-class weights\n",
    "    'weight_<60':  weight_class_1,                # weight for class: weight_kg < 60\n",
    "    'weight_>100': weight_class_2,                # weight for class: weight_kg > 100\n",
    "    'weight_60_100': weight_class_3               # weight for class: 60 <= weight_kg <= 100\n",
    "}\n",
    "\n",
    "print(\"Weight-class weights:\", weight_class_weights)  # print weight-class weights to check\n",
    "\n",
    "# gender_weights dict is assumed from previous step, e.g. {0: w0, 1: w1}\n",
    "print(\"Gender-class weights:\", gender_weights)        # print gender-class weights to check\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Multiply each gender class with each weight class\n",
    "#    wi = w_weight * w_gender\n",
    "# -------------------------------------------------\n",
    "combined_weights = {}                                # dictionary to store combined class weights\n",
    "\n",
    "print(\"\\nCombined weights for each (weight_class, gender_class):\")  # header\n",
    "for w_label, w_w in weight_class_weights.items():    # loop over weight classes\n",
    "    for g_label, w_g in gender_weights.items():      # loop over gender classes\n",
    "        wi = w_w * w_g                               # multiply weight and gender class weights\n",
    "        combined_weights[(w_label, g_label)] = wi    # store in dictionary\n",
    "        print(f\"{w_label} & gender {g_label}: {wi}\") # print each combination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9487c312-8683-4912-9b37-7e99383b1ed4",
   "metadata": {},
   "source": [
    "2.5. create a dictionary for weights and row index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "806866b5-519d-48b6-be00-77c2fd191d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns before adding index column:\n",
      " Index(['photo_id', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9',\n",
      "       ...\n",
      "       'hip', 'leg-length', 'shoulder-breadth', 'shoulder-to-crotch', 'thigh',\n",
      "       'waist', 'wrist', 'gender', 'height_cm', 'weight_kg'],\n",
      "      dtype='object', length=5138)\n"
     ]
    }
   ],
   "source": [
    "# Check current columns in the DataFrame\n",
    "print(\"Columns before adding index column:\\n\", data.columns)\n",
    "\n",
    "# Add a new column named 'index' with values from 0 to number_of_rows-1\n",
    "data['index'] = range(len(data))\n",
    "\n",
    "# Move 'index' to the front (optional, just for nicer viewing)\n",
    "cols = ['index'] + [c for c in data.columns if c != 'index']  # build new column order\n",
    "data = data[cols]                                            # reorder columns\n",
    "\n",
    "# Show first few rows to verify the new indexing column\n",
    "#print(\"\\nDataFrame after adding 'index' column:\\n\", data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41807157-b19f-4328-9e9a-faf126794665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight-class weights: {'weight_<60': 1.9491579281855735, 'weight_>100': 3.9779507133592737, 'weight_60_100': 0.4473127689054182}\n",
      "Gender-class weights: {1: 0.8402739726027397, 0: 1.2347020933977455}\n",
      "\n",
      "Building final_weights dictionary...\n",
      "Number of entries in final_weights: 6134\n",
      "First 5 items in final_weights: [(0, 0.5522980121710618), (1, 0.37586527732408703), (2, 0.37586527732408703), (3, 0.5522980121710618), (4, 0.37586527732408703)]\n",
      "\n",
      "Checking entry with index 0...\n",
      "Row 0 -> gender: 0\n",
      "Row 0 -> weight_kg: 72.0\n",
      "Row 0 -> weight class: weight_60_100\n",
      "w_weight for row 0: 0.4473127689054182\n",
      "w_gender for row 0: 1.2347020933977455\n",
      "Combined weight (calculated): 0.5522980121710618\n",
      "Combined weight from final_weights[0]: 0.5522980121710618\n",
      "\n",
      "Saving final_weights dictionary as pickle file...\n",
      "Dictionary saved to 'final_weights.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np               # import numpy for numeric operations\n",
    "import pickle                    # import pickle to save Python objects\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 0. We assume these already exist:\n",
    "#    - weight_class_1, weight_class_2, weight_class_3\n",
    "#    - gender_weights   (dict: {gender_class: weight})\n",
    "# -------------------------------------------------\n",
    "\n",
    "# create a dictionary of weight-class weights (same as before)\n",
    "weight_class_weights = {         # dictionary mapping weight class labels to their weights\n",
    "    'weight_<60':  weight_class_1,      # weight for class: weight_kg < 60\n",
    "    'weight_>100': weight_class_2,      # weight for class: weight_kg > 100\n",
    "    'weight_60_100': weight_class_3     # weight for class: 60 <= weight_kg <= 100\n",
    "}\n",
    "\n",
    "print(\"Weight-class weights:\", weight_class_weights)  # print weight-class weights\n",
    "print(\"Gender-class weights:\", gender_weights)        # print gender-class weights\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Helper function to get the weight class label for a given weight_kg\n",
    "# -------------------------------------------------\n",
    "def get_weight_class(w):         # define a function that receives a single weight value\n",
    "    if w < 60:                   # check if weight is less than 60\n",
    "        return 'weight_<60'      # return label for class 1\n",
    "    elif w > 100:                # check if weight is greater than 100\n",
    "        return 'weight_>100'     # return label for class 2\n",
    "    else:                        # otherwise weight is between 60 and 100 (inclusive)\n",
    "        return 'weight_60_100'   # return label for class 3\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Build dictionary: keys = index values, values = combined weights\n",
    "# -------------------------------------------------\n",
    "final_weights = {}               # create empty dictionary to store final weights\n",
    "\n",
    "print(\"\\nBuilding final_weights dictionary...\")  # message to track progress\n",
    "\n",
    "for _, row in data.iterrows():   # loop over each row of the DataFrame\n",
    "    idx_val = row['index']       # get the value from the 'index' column for this row\n",
    "    gender_val = row['gender']   # get the gender class value for this row\n",
    "    weight_val = row['weight_kg']# get the weight_kg value for this row\n",
    "\n",
    "    w_class = get_weight_class(weight_val)        # determine weight class label from weight_kg\n",
    "    w_weight = weight_class_weights[w_class]      # look up the weight-class weight\n",
    "    w_gender = gender_weights[gender_val]         # look up the gender-class weight\n",
    "\n",
    "    combined_w = w_weight * w_gender             # multiply to get combined weight w_i\n",
    "    final_weights[idx_val] = combined_w          # store combined weight in dictionary with key=index\n",
    "\n",
    "print(\"Number of entries in final_weights:\", len(final_weights))  # print number of entries\n",
    "print(\"First 5 items in final_weights:\", list(final_weights.items())[:5])  # show first few items\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. Check index 0: gender, weight_kg, and combined weight\n",
    "# -------------------------------------------------\n",
    "print(\"\\nChecking entry with index 0...\")        # message to show what we're doing\n",
    "\n",
    "row0 = data.loc[data['index'] == 0].iloc[0]      # select the row where 'index' column equals 0\n",
    "\n",
    "gender0 = row0['gender']                         # get gender value for index 0\n",
    "weight0 = row0['weight_kg']                      # get weight_kg value for index 0\n",
    "w_class0 = get_weight_class(weight0)             # get weight class label for index 0\n",
    "\n",
    "w_weight0 = weight_class_weights[w_class0]       # get weight-class weight for index 0\n",
    "w_gender0 = gender_weights[gender0]              # get gender-class weight for index 0\n",
    "combined0_calc = w_weight0 * w_gender0           # calculate combined weight for index 0\n",
    "\n",
    "print(\"Row 0 -> gender:\", gender0)               # print gender class for index 0\n",
    "print(\"Row 0 -> weight_kg:\", weight0)            # print weight_kg for index 0\n",
    "print(\"Row 0 -> weight class:\", w_class0)        # print weight class label for index 0\n",
    "print(\"w_weight for row 0:\", w_weight0)          # print weight-class weight for index 0\n",
    "print(\"w_gender for row 0:\", w_gender0)          # print gender-class weight for index 0\n",
    "print(\"Combined weight (calculated):\", combined0_calc)        # print calculated combined weight\n",
    "print(\"Combined weight from final_weights[0]:\", final_weights[0])  # print value from dictionary\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Save final_weights dictionary as a pickle file\n",
    "# -------------------------------------------------\n",
    "print(\"\\nSaving final_weights dictionary as pickle file...\")   # message to track saving step\n",
    "\n",
    "with open('final_weights.pkl', 'wb') as f:       # open a file named 'final_weights.pkl' in binary write mode\n",
    "    pickle.dump(final_weights, f)                # write dictionary to the file using pickle\n",
    "\n",
    "print(\"Dictionary saved to 'final_weights.pkl'.\")# confirmation message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f74299-3350-46fa-a49f-988b733ca1b8",
   "metadata": {},
   "source": [
    "2.6. apply min-max scaling with range -1 to 1 for body measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eedc423f-20a2-4ff2-a8b9-17d1c011c80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler   # import the scaler for min-max normalization\n",
    "\n",
    "# list of columns to scale between -1 and 1\n",
    "cols_to_scale = [\n",
    "    'ankle', 'arm-length', 'bicep', 'calf', 'chest', 'forearm', 'hip',\n",
    "    'leg-length', 'shoulder-breadth', 'shoulder-to-crotch', 'thigh',\n",
    "    'waist', 'wrist', 'height_cm', 'weight_kg'\n",
    "]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))    # create a scaler that maps values to range [-1, 1]\n",
    "\n",
    "data[cols_to_scale] = scaler.fit_transform(     # fit the scaler and transform the selected columns\n",
    "    data[cols_to_scale]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4707456-2e11-4ead-9ab7-534b91ca8d19",
   "metadata": {},
   "source": [
    "3. model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e07427-117c-46ac-9bdc-f049f9a1024b",
   "metadata": {},
   "source": [
    "3.1. split the data for independent and dependent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58ffaa0e-f6df-4778-ae89-fb050332a6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected target columns: ['ankle', 'arm-length', 'bicep', 'calf', 'chest', 'forearm', 'hip', 'leg-length', 'shoulder-breadth', 'shoulder-to-crotch', 'thigh', 'waist', 'wrist', 'weight_kg']\n",
      "Shape of Y (samples, targets): (6134, 14)\n"
     ]
    }
   ],
   "source": [
    "# List of columns to be used as dependent (target) features\n",
    "target_cols = [\n",
    "    'ankle', 'arm-length', 'bicep', 'calf', 'chest', 'forearm', 'hip',\n",
    "    'leg-length', 'shoulder-breadth', 'shoulder-to-crotch', 'thigh',\n",
    "    'waist', 'wrist', 'weight_kg'\n",
    "]\n",
    "\n",
    "# Select these columns from the DataFrame as the multi-target Y\n",
    "Y = data[target_cols]                  # Y will hold all dependent variables for multi-target regression\n",
    "\n",
    "print(\"Selected target columns:\", target_cols)  # print which columns are used as targets\n",
    "print(\"Shape of Y (samples, targets):\", Y.shape)  # print shape to confirm dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69820f72-d599-4ec3-acdf-ab2cc8e9d79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to drop for X:\n",
      " ['photo_id', 'subject_id', 'index', 'ankle', 'arm-length', 'bicep', 'calf', 'chest', 'forearm', 'hip', 'leg-length', 'shoulder-breadth', 'shoulder-to-crotch', 'thigh', 'waist', 'wrist', 'weight_kg']\n",
      "\n",
      "Shape of X (samples, independent features): (6134, 5122)\n"
     ]
    }
   ],
   "source": [
    "# Columns to drop for building independent features (X)\n",
    "drop_cols = ['photo_id', 'subject_id','index'] + target_cols   # combine ID columns with target columns\n",
    "\n",
    "print(\"Columns to drop for X:\\n\", drop_cols)           # show which columns will be removed\n",
    "\n",
    "# Create X by dropping ID columns and all target columns\n",
    "X = data.drop(columns=drop_cols)                       # drop the unwanted columns to get independent features\n",
    "\n",
    "print(\"\\nShape of X (samples, independent features):\", X.shape)  # print shape of X\n",
    "#print(\"\\nColumns in X:\\n\", X.columns.tolist())         # list all feature names in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8cb5737-d256-4ae3-bd10-f1567065b74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 18:12:45.947921: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-05 18:12:45.961078: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-12-05 18:12:45.980541: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-12-05 18:12:45.980567: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-05 18:12:45.992813: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-05 18:12:46.902356: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X (features): (6134, 5122)\n",
      "Shape of Y (targets): (6134, 14)\n",
      "\n",
      "Loading final_weights.pkl ...\n",
      "Number of entries in final_weights_dict: 6134\n",
      "First 5 entries in final_weights_dict: [(0, 0.5522980121710618), (1, 0.37586527732408703), (2, 0.37586527732408703), (3, 0.5522980121710618), (4, 0.37586527732408703)]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Imports\n",
    "# -----------------------------\n",
    "import numpy as np                          # numerical operations\n",
    "import pickle                               # to load the final_weights.pkl file\n",
    "import matplotlib.pyplot as plt             # for plotting loss curves\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # to create train/validation sets\n",
    "\n",
    "import tensorflow as tf                     # main deep learning library\n",
    "from tensorflow.keras.models import Sequential          # model container\n",
    "from tensorflow.keras.layers import Dense, Dropout, InputLayer, LeakyReLU, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "# -----------------------------\n",
    "# Reproducibility (optional)\n",
    "# -----------------------------\n",
    "np.random.seed(42)                          # fix numpy random seed\n",
    "tf.random.set_seed(42)                      # fix tensorflow random seed\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Assume you already have:\n",
    "#    - data DataFrame\n",
    "#    - X (independent features)\n",
    "#    - Y (multi-output targets)\n",
    "# If not, you can recreate X, Y here.\n",
    "# -----------------------------\n",
    "\n",
    "# Example (uncomment if you want everything in one place):\n",
    "# target_cols = [\n",
    "#     'ankle', 'arm-length', 'bicep', 'calf', 'chest', 'forearm', 'hip',\n",
    "#     'leg-length', 'shoulder-breadth', 'shoulder-to-crotch', 'thigh',\n",
    "#     'waist', 'wrist', 'weight_kg'\n",
    "# ]\n",
    "# Y = data[target_cols]                                            # select target columns\n",
    "# drop_cols = ['photo_id', 'subject_id'] + target_cols             # columns not used as features\n",
    "# X = data.drop(columns=drop_cols + ['index'])                     # drop also 'index' from features\n",
    "\n",
    "print(\"Shape of X (features):\", X.shape)           # show shape of feature matrix\n",
    "print(\"Shape of Y (targets):\", Y.shape)           # show shape of target matrix\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load final_weights.pkl (sample weights per index)\n",
    "# -----------------------------\n",
    "print(\"\\nLoading final_weights.pkl ...\")          # status message\n",
    "\n",
    "with open('final_weights.pkl', 'rb') as f:        # open pickle file in read-binary mode\n",
    "    final_weights_dict = pickle.load(f)           # load dictionary {index: weight}\n",
    "\n",
    "print(\"Number of entries in final_weights_dict:\", len(final_weights_dict))  # size of dictionary\n",
    "print(\"First 5 entries in final_weights_dict:\", list(final_weights_dict.items())[:5])  # preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a93e352-a406-4218-8106-1c4481f30ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building sample_weight array ...\n",
      "Sample weights shape: (6134,)\n",
      "First 10 sample weights: [0.552298   0.37586528 0.37586528 0.552298   0.37586528 0.552298\n",
      " 0.37586528 0.37586528 0.552298   0.37586528]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 3. Build sample_weight array based on DataFrame 'index' column\n",
    "# -----------------------------\n",
    "print(\"\\nBuilding sample_weight array ...\")       # status message\n",
    "\n",
    "# map each row's 'index' value to its weight in the dictionary\n",
    "sample_weights = data['index'].map(final_weights_dict).values.astype('float32')\n",
    "\n",
    "print(\"Sample weights shape:\", sample_weights.shape)   # show shape of weight array\n",
    "print(\"First 10 sample weights:\", sample_weights[:10]) # preview some weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1db360ce-79cd-4103-928a-8cbf03ecac2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting into train and validation sets ...\n",
      "X_train shape: (4907, 5122)\n",
      "Y_train shape: (4907, 14)\n",
      "X_val shape: (1227, 5122)\n",
      "Y_val shape: (1227, 14)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 4. Train/validation split (X, Y, and weights)\n",
    "# -----------------------------\n",
    "print(\"\\nSplitting into train and validation sets ...\")  # status message\n",
    "\n",
    "X_train, X_val, Y_train, Y_val, w_train, w_val = train_test_split(\n",
    "    X, Y, sample_weights,           # split features, targets, and weights together\n",
    "    test_size=0.2,                  # 20% validation\n",
    "    random_state=42,                # reproducible split\n",
    "    shuffle=True                    # shuffle data before splitting\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)          # show training feature shape\n",
    "print(\"Y_train shape:\", Y_train.shape)          # show training target shape\n",
    "print(\"X_val shape:\", X_val.shape)              # show validation feature shape\n",
    "print(\"Y_val shape:\", Y_val.shape)              # show validation target shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cd70174-01b8-4787-851a-054984bd9218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total hyper-parameter combinations: 1\n",
      "\n",
      "======================================\n",
      "Training combination 1/1\n",
      "Hidden layers: 3\n",
      "Neurons per layer: 512\n",
      "Activation: gelu\n",
      "Learning rate: 0.001\n",
      "Optimizer: adam\n",
      "======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 18:12:47.720181: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-05 18:12:47.765551: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-05 18:12:47.766683: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-05 18:12:47.770517: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-05 18:12:47.771577: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-05 18:12:47.772537: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-05 18:12:47.914258: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-05 18:12:47.915389: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-05 18:12:47.916406: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-05 18:12:47.917338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 112 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n",
      "2025-12-05 18:12:47.933867: I external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:1566] failed to allocate 112.81MiB (118292480 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2025-12-05 18:13:07.581101: W external/local_tsl/tsl/framework/bfc_allocator.cc:487] Allocator (GPU_0_bfc) ran out of memory trying to allocate 191.75MiB (rounded to 201069312)requested by op _EagerConst\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2025-12-05 18:13:07.581130: I external/local_tsl/tsl/framework/bfc_allocator.cc:1044] BFCAllocator dump for GPU_0_bfc\n",
      "2025-12-05 18:13:07.581139: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (256): \tTotal Chunks: 42, Chunks in use: 42. 10.5KiB allocated for chunks. 10.5KiB in use in bin. 3.9KiB client-requested in use in bin.\n",
      "2025-12-05 18:13:07.581146: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-05 18:13:07.581151: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2025-12-05 18:13:07.581157: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (2048): \tTotal Chunks: 18, Chunks in use: 18. 38.5KiB allocated for chunks. 38.5KiB in use in bin. 36.0KiB client-requested in use in bin.\n",
      "2025-12-05 18:13:07.581162: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (4096): \tTotal Chunks: 3, Chunks in use: 3. 12.0KiB allocated for chunks. 12.0KiB in use in bin. 12.0KiB client-requested in use in bin.\n",
      "2025-12-05 18:13:07.581167: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (8192): \tTotal Chunks: 1, Chunks in use: 0. 11.5KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-05 18:13:07.581173: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (16384): \tTotal Chunks: 14, Chunks in use: 14. 248.5KiB allocated for chunks. 248.5KiB in use in bin. 239.2KiB client-requested in use in bin.\n",
      "2025-12-05 18:13:07.581177: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (32768): \tTotal Chunks: 1, Chunks in use: 0. 48.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-05 18:13:07.581182: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (65536): \tTotal Chunks: 1, Chunks in use: 0. 100.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-05 18:13:07.581187: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (131072): \tTotal Chunks: 4, Chunks in use: 3. 544.0KiB allocated for chunks. 384.0KiB in use in bin. 384.0KiB client-requested in use in bin.\n",
      "2025-12-05 18:13:07.581192: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-05 18:13:07.581197: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (524288): \tTotal Chunks: 1, Chunks in use: 0. 512.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-05 18:13:07.581202: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (1048576): \tTotal Chunks: 3, Chunks in use: 2. 3.00MiB allocated for chunks. 2.00MiB in use in bin. 2.00MiB client-requested in use in bin.\n",
      "2025-12-05 18:13:07.581207: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (2097152): \tTotal Chunks: 8, Chunks in use: 6. 17.52MiB allocated for chunks. 13.52MiB in use in bin. 12.00MiB client-requested in use in bin.\n",
      "2025-12-05 18:13:07.581212: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-05 18:13:07.581217: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (8388608): \tTotal Chunks: 1, Chunks in use: 1. 10.00MiB allocated for chunks. 10.00MiB in use in bin. 10.00MiB client-requested in use in bin.\n",
      "2025-12-05 18:13:07.581221: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-05 18:13:07.581226: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-05 18:13:07.581232: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (67108864): \tTotal Chunks: 1, Chunks in use: 0. 69.52MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-05 18:13:07.581237: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-05 18:13:07.581241: I external/local_tsl/tsl/framework/bfc_allocator.cc:1051] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-12-05 18:13:07.581246: I external/local_tsl/tsl/framework/bfc_allocator.cc:1067] Bin for 191.75MiB was 128.00MiB, Chunk State: \n",
      "2025-12-05 18:13:07.581250: I external/local_tsl/tsl/framework/bfc_allocator.cc:1080] Next region of size 106463232\n",
      "2025-12-05 18:13:07.581256: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4000000 of size 256 next 1\n",
      "2025-12-05 18:13:07.581260: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4000100 of size 1280 next 2\n",
      "2025-12-05 18:13:07.581264: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4000600 of size 256 next 3\n",
      "2025-12-05 18:13:07.581267: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4000700 of size 256 next 4\n",
      "2025-12-05 18:13:07.581271: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4000800 of size 256 next 6\n",
      "2025-12-05 18:13:07.581274: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4000900 of size 2048 next 7\n",
      "2025-12-05 18:13:07.581279: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4001100 of size 256 next 5\n",
      "2025-12-05 18:13:07.581282: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4001200 of size 256 next 8\n",
      "2025-12-05 18:13:07.581286: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4001300 of size 256 next 12\n",
      "2025-12-05 18:13:07.581289: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4001400 of size 256 next 11\n",
      "2025-12-05 18:13:07.581293: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4001500 of size 256 next 13\n",
      "2025-12-05 18:13:07.581296: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4001600 of size 256 next 14\n",
      "2025-12-05 18:13:07.581299: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4001700 of size 256 next 16\n",
      "2025-12-05 18:13:07.581303: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4001800 of size 256 next 20\n",
      "2025-12-05 18:13:07.581306: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4001900 of size 256 next 15\n",
      "2025-12-05 18:13:07.581309: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4001a00 of size 256 next 21\n",
      "2025-12-05 18:13:07.581313: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4001b00 of size 256 next 26\n",
      "2025-12-05 18:13:07.581316: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4001c00 of size 256 next 24\n",
      "2025-12-05 18:13:07.581320: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4001d00 of size 256 next 25\n",
      "2025-12-05 18:13:07.581323: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4001e00 of size 256 next 30\n",
      "2025-12-05 18:13:07.581326: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4001f00 of size 2048 next 28\n",
      "2025-12-05 18:13:07.581330: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4002700 of size 2048 next 29\n",
      "2025-12-05 18:13:07.581333: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4002f00 of size 256 next 33\n",
      "2025-12-05 18:13:07.581336: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4003000 of size 256 next 34\n",
      "2025-12-05 18:13:07.581340: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4003100 of size 4096 next 37\n",
      "2025-12-05 18:13:07.581344: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4004100 of size 2048 next 40\n",
      "2025-12-05 18:13:07.581347: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4004900 of size 2048 next 35\n",
      "2025-12-05 18:13:07.581351: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4005100 of size 2048 next 17\n",
      "2025-12-05 18:13:07.581354: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4005900 of size 16384 next 18\n",
      "2025-12-05 18:13:07.581358: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4009900 of size 16384 next 19\n",
      "2025-12-05 18:13:07.581361: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c400d900 of size 256 next 36\n",
      "2025-12-05 18:13:07.581365: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c400da00 of size 256 next 42\n",
      "2025-12-05 18:13:07.581368: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c400db00 of size 2048 next 45\n",
      "2025-12-05 18:13:07.581371: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c400e300 of size 256 next 44\n",
      "2025-12-05 18:13:07.581375: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c400e400 of size 256 next 43\n",
      "2025-12-05 18:13:07.581378: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c400e500 of size 256 next 48\n",
      "2025-12-05 18:13:07.581382: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c400e600 of size 256 next 49\n",
      "2025-12-05 18:13:07.581385: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c400e700 of size 29184 next 23\n",
      "2025-12-05 18:13:07.581389: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4015900 of size 16384 next 22\n",
      "2025-12-05 18:13:07.581392: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4019900 of size 16384 next 27\n",
      "2025-12-05 18:13:07.581396: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c401d900 of size 256 next 52\n",
      "2025-12-05 18:13:07.581399: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c401da00 of size 2048 next 56\n",
      "2025-12-05 18:13:07.581402: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c401e200 of size 2048 next 58\n",
      "2025-12-05 18:13:07.581406: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c401ea00 of size 4096 next 61\n",
      "2025-12-05 18:13:07.581409: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c401fa00 of size 2048 next 62\n",
      "2025-12-05 18:13:07.581413: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4020200 of size 2048 next 65\n",
      "2025-12-05 18:13:07.581416: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4020a00 of size 3840 next 50\n",
      "2025-12-05 18:13:07.581420: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4021900 of size 16384 next 51\n",
      "2025-12-05 18:13:07.581423: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4025900 of size 16384 next 53\n",
      "2025-12-05 18:13:07.581426: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4029900 of size 2048 next 69\n",
      "2025-12-05 18:13:07.581430: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c402a100 of size 256 next 68\n",
      "2025-12-05 18:13:07.581433: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c402a200 of size 256 next 67\n",
      "2025-12-05 18:13:07.581436: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c402a300 of size 256 next 71\n",
      "2025-12-05 18:13:07.581440: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c402a400 of size 256 next 72\n",
      "2025-12-05 18:13:07.581443: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c402a500 of size 256 next 76\n",
      "2025-12-05 18:13:07.581451: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c402a600 of size 2048 next 78\n",
      "2025-12-05 18:13:07.581455: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c402ae00 of size 2048 next 80\n",
      "2025-12-05 18:13:07.581458: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c402b600 of size 4096 next 83\n",
      "2025-12-05 18:13:07.581461: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c402c600 of size 2048 next 84\n",
      "2025-12-05 18:13:07.581465: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c402ce00 of size 2816 next 55\n",
      "2025-12-05 18:13:07.581468: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c402d900 of size 16384 next 54\n",
      "2025-12-05 18:13:07.581472: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4031900 of size 16384 next 57\n",
      "2025-12-05 18:13:07.581475: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] Free  at 7f88c4035900 of size 163840 next 32\n",
      "2025-12-05 18:13:07.581479: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c405d900 of size 131072 next 31\n",
      "2025-12-05 18:13:07.581482: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] Free  at 7f88c407d900 of size 2097152 next 47\n",
      "2025-12-05 18:13:07.581486: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c427d900 of size 1048576 next 46\n",
      "2025-12-05 18:13:07.581489: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c437d900 of size 2048 next 85\n",
      "2025-12-05 18:13:07.581493: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c437e100 of size 256 next 88\n",
      "2025-12-05 18:13:07.581496: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c437e200 of size 256 next 89\n",
      "2025-12-05 18:13:07.581500: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c437e300 of size 256 next 90\n",
      "2025-12-05 18:13:07.581503: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c437e400 of size 256 next 91\n",
      "2025-12-05 18:13:07.581506: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c437e500 of size 256 next 93\n",
      "2025-12-05 18:13:07.581510: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c437e600 of size 256 next 94\n",
      "2025-12-05 18:13:07.581513: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c437e700 of size 256 next 95\n",
      "2025-12-05 18:13:07.581516: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c437e800 of size 256 next 96\n",
      "2025-12-05 18:13:07.581520: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c437e900 of size 256 next 97\n",
      "2025-12-05 18:13:07.581523: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c437ea00 of size 256 next 98\n",
      "2025-12-05 18:13:07.581526: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] Free  at 7f88c437eb00 of size 11776 next 73\n",
      "2025-12-05 18:13:07.581530: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4381900 of size 16384 next 74\n",
      "2025-12-05 18:13:07.581533: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4385900 of size 16384 next 75\n",
      "2025-12-05 18:13:07.581537: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4389900 of size 16384 next 77\n",
      "2025-12-05 18:13:07.581540: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c438d900 of size 16384 next 79\n",
      "2025-12-05 18:13:07.581543: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] Free  at 7f88c4391900 of size 49152 next 60\n",
      "2025-12-05 18:13:07.581547: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c439d900 of size 131072 next 59\n",
      "2025-12-05 18:13:07.581550: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c43bd900 of size 28672 next 92\n",
      "2025-12-05 18:13:07.581554: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] Free  at 7f88c43c4900 of size 102400 next 82\n",
      "2025-12-05 18:13:07.581557: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c43dd900 of size 131072 next 81\n",
      "2025-12-05 18:13:07.581561: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] Free  at 7f88c43fd900 of size 524288 next 39\n",
      "2025-12-05 18:13:07.581564: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c447d900 of size 2097152 next 38\n",
      "2025-12-05 18:13:07.581568: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c467d900 of size 2097152 next 41\n",
      "2025-12-05 18:13:07.581571: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c487d900 of size 1048576 next 70\n",
      "2025-12-05 18:13:07.581575: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] Free  at 7f88c497d900 of size 1048576 next 64\n",
      "2025-12-05 18:13:07.581578: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4a7d900 of size 2097152 next 63\n",
      "2025-12-05 18:13:07.581582: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c4c7d900 of size 2097152 next 66\n",
      "2025-12-05 18:13:07.581585: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] Free  at 7f88c4e7d900 of size 2097152 next 86\n",
      "2025-12-05 18:13:07.581588: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c507d900 of size 3691008 next 9\n",
      "2025-12-05 18:13:07.581592: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c5402b00 of size 10489856 next 10\n",
      "2025-12-05 18:13:07.581597: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] InUse at 7f88c5e03b00 of size 2097152 next 87\n",
      "2025-12-05 18:13:07.581601: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100] Free  at 7f88c6003b00 of size 72893696 next 18446744073709551615\n",
      "2025-12-05 18:13:07.581604: I external/local_tsl/tsl/framework/bfc_allocator.cc:1105]      Summary of in-use Chunks by size: \n",
      "2025-12-05 18:13:07.581609: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 42 Chunks of size 256 totalling 10.5KiB\n",
      "2025-12-05 18:13:07.581613: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2025-12-05 18:13:07.581618: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 16 Chunks of size 2048 totalling 32.0KiB\n",
      "2025-12-05 18:13:07.581621: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 1 Chunks of size 2816 totalling 2.8KiB\n",
      "2025-12-05 18:13:07.581625: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 1 Chunks of size 3840 totalling 3.8KiB\n",
      "2025-12-05 18:13:07.581629: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 3 Chunks of size 4096 totalling 12.0KiB\n",
      "2025-12-05 18:13:07.581633: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 12 Chunks of size 16384 totalling 192.0KiB\n",
      "2025-12-05 18:13:07.581637: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 1 Chunks of size 28672 totalling 28.0KiB\n",
      "2025-12-05 18:13:07.581641: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 1 Chunks of size 29184 totalling 28.5KiB\n",
      "2025-12-05 18:13:07.581645: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 3 Chunks of size 131072 totalling 384.0KiB\n",
      "2025-12-05 18:13:07.581649: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 2 Chunks of size 1048576 totalling 2.00MiB\n",
      "2025-12-05 18:13:07.581653: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 5 Chunks of size 2097152 totalling 10.00MiB\n",
      "2025-12-05 18:13:07.581657: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 1 Chunks of size 3691008 totalling 3.52MiB\n",
      "2025-12-05 18:13:07.581661: I external/local_tsl/tsl/framework/bfc_allocator.cc:1108] 1 Chunks of size 10489856 totalling 10.00MiB\n",
      "2025-12-05 18:13:07.581665: I external/local_tsl/tsl/framework/bfc_allocator.cc:1112] Sum Total of in-use chunks: 26.20MiB\n",
      "2025-12-05 18:13:07.581669: I external/local_tsl/tsl/framework/bfc_allocator.cc:1114] Total bytes in pool: 106463232 memory_limit_: 118292480 available bytes: 11829248 curr_region_allocation_bytes_: 236584960\n",
      "2025-12-05 18:13:07.581677: I external/local_tsl/tsl/framework/bfc_allocator.cc:1119] Stats: \n",
      "Limit:                       118292480\n",
      "InUse:                        27475200\n",
      "MaxInUse:                     31602688\n",
      "NumAllocs:                         230\n",
      "MaxAllocSize:                 10489856\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-12-05 18:13:07.581684: W external/local_tsl/tsl/framework/bfc_allocator.cc:499] *_*************_****************____________________________________________________________________\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/data/util/structure.py\", line 105, in normalize_element\n",
      "    spec = type_spec_from_value(t, use_fallback=False)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/data/util/structure.py\", line 514, in type_spec_from_value\n",
      "    raise TypeError(\"Could not build a `TypeSpec` for {} with type {}\".format(\n",
      "TypeError: Could not build a `TypeSpec` for             f1        f2        f3        f4        f5        f6        f7  \\\n",
      "3946  0.047495  0.012609 -0.136065  0.683395  0.335336 -0.062406 -0.044196   \n",
      "1345 -0.015872 -0.035510 -0.144925  0.538641  0.198866 -0.150294 -0.032007   \n",
      "1606 -0.002796  0.060905 -0.142280  0.525178  0.282501 -0.154363 -0.025482   \n",
      "2377  0.057395  0.127693 -0.138721  0.595069  0.251055 -0.119108 -0.031434   \n",
      "1055  0.036597  0.031368 -0.153503  0.669124  0.259073 -0.120021 -0.125852   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "3772  0.017424 -0.003016 -0.128894  0.611507  0.282464 -0.057504 -0.036479   \n",
      "5191  0.004287 -0.003860 -0.148466  0.546357  0.317269 -0.115861 -0.020662   \n",
      "5226  0.085760  0.056135 -0.123833  0.540060  0.275112 -0.061143 -0.028153   \n",
      "5390  0.005916  0.021928 -0.156794  0.430881  0.341810 -0.103624 -0.045887   \n",
      "860   0.105165  0.112506 -0.122905  0.687118  0.410256 -0.023220 -0.069637   \n",
      "\n",
      "            f8        f9       f10  ...     s2553     s2554     s2555  \\\n",
      "3946 -0.109313  0.110457  0.240287  ... -0.178132 -0.040372 -0.132801   \n",
      "1345 -0.128297  0.069969  0.168250  ... -0.210008 -0.088108 -0.143143   \n",
      "1606 -0.081871  0.133355  0.285044  ... -0.162072 -0.044252 -0.129530   \n",
      "2377 -0.090095  0.073442  0.274556  ... -0.117398 -0.019121 -0.156043   \n",
      "1055 -0.079916  0.018769  0.267637  ... -0.166978  0.054604 -0.132995   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "3772 -0.114189  0.057736  0.144209  ... -0.169168 -0.031656 -0.125664   \n",
      "5191 -0.094640  0.077533  0.286567  ... -0.187537 -0.120605 -0.122904   \n",
      "5226 -0.001547  0.043055  0.246456  ... -0.171929 -0.034006 -0.127080   \n",
      "5390 -0.063621  0.073945  0.141708  ... -0.173200 -0.059031 -0.139369   \n",
      "860  -0.112306  0.000387  0.280712  ... -0.123037 -0.070794 -0.147644   \n",
      "\n",
      "         s2556     s2557     s2558     s2559     s2560  gender  height_cm  \n",
      "3946 -0.227414 -0.084601 -0.234124 -0.177869  0.459008       0   0.212928  \n",
      "1345 -0.225494 -0.191922 -0.149914 -0.152593  0.163651       1  -0.072243  \n",
      "1606 -0.230123 -0.101245 -0.212125 -0.220942  0.363113       0  -0.015209  \n",
      "2377 -0.223282 -0.061029 -0.243489 -0.163225  0.325478       0  -0.235741  \n",
      "1055 -0.228669 -0.138805 -0.195539 -0.171886  0.254902       1  -0.176806  \n",
      "...        ...       ...       ...       ...       ...     ...        ...  \n",
      "3772 -0.230284 -0.148149 -0.232164 -0.184422  0.370254       0  -0.258555  \n",
      "5191 -0.143789 -0.082698 -0.216099 -0.157898  0.197296       0  -0.323194  \n",
      "5226 -0.230746 -0.077675 -0.213621 -0.185026  0.342262       0   0.235741  \n",
      "5390 -0.233169 -0.077855 -0.209909 -0.213487  0.339418       1  -0.091255  \n",
      "860  -0.234130  0.046089 -0.219538 -0.192062  0.511154       1   0.840304  \n",
      "\n",
      "[4907 rows x 5122 columns] with type DataFrame\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3552, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_54632/2184712540.py\", line 289, in <cell line: 243>\n",
      "    history = model.fit(\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 1742, in fit\n",
      "    data_handler = data_adapter.get_data_handler(\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tf_keras/src/engine/data_adapter.py\", line 1682, in get_data_handler\n",
      "    return DataHandler(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tf_keras/src/engine/data_adapter.py\", line 1287, in __init__\n",
      "    self._adapter = adapter_cls(\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tf_keras/src/engine/data_adapter.py\", line 351, in __init__\n",
      "    dataset = self.slice_inputs(indices_dataset, inputs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tf_keras/src/engine/data_adapter.py\", line 384, in slice_inputs\n",
      "    (indices_dataset, tf.data.Dataset.from_tensors(inputs).repeat())\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 741, in from_tensors\n",
      "    return from_tensors_op._from_tensors(tensors, name)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/data/ops/from_tensors_op.py\", line 23, in _from_tensors\n",
      "    return _TensorDataset(tensors, name)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/data/ops/from_tensors_op.py\", line 31, in __init__\n",
      "    element = structure.normalize_element(element)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/data/util/structure.py\", line 110, in normalize_element\n",
      "    ops.convert_to_tensor(t, name=\"component_%d\" % i))\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py\", line 183, in wrapped\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\", line 713, in convert_to_tensor\n",
      "    return tensor_conversion_registry.convert(\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\", line 234, in convert\n",
      "    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/framework/constant_tensor_conversion.py\", line 29, in _constant_tensor_conversion_function\n",
      "    return constant_op.constant(v, dtype=dtype, name=name)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py\", line 142, in wrapper\n",
      "    return op(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py\", line 276, in constant\n",
      "    return _constant_impl(value, dtype, shape, name, verify_shape=False,\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py\", line 289, in _constant_impl\n",
      "    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py\", line 301, in _constant_eager_impl\n",
      "    t = convert_to_eager_tensor(value, ctx, dtype)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py\", line 108, in convert_to_eager_tensor\n",
      "    return ops.EagerTensor(value, ctx.device_name, dtype)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/framework/errors_impl.py\", line 462, in __init__\n",
      "    def __init__(self, node_def, op, message, *args):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2098, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
      "    module = getmodule(object, filename)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/inspect.py\", line 875, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/inspect.py\", line 844, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/inspect.py\", line 820, in getsourcefile\n",
      "    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/inspect.py\", line 820, in <genexpr>\n",
      "    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m           \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mtype_spec_from_value\u001b[0;34m(element, use_fallback)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m   raise TypeError(\"Could not build a `TypeSpec` for {} with type {}\".format(\n\u001b[0m\u001b[1;32m    515\u001b[0m       \u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not build a `TypeSpec` for             f1        f2        f3        f4        f5        f6        f7  \\\n3946  0.047495  0.012609 -0.136065  0.683395  0.335336 -0.062406 -0.044196   \n1345 -0.015872 -0.035510 -0.144925  0.538641  0.198866 -0.150294 -0.032007   \n1606 -0.002796  0.060905 -0.142280  0.525178  0.282501 -0.154363 -0.025482   \n2377  0.057395  0.127693 -0.138721  0.595069  0.251055 -0.119108 -0.031434   \n1055  0.036597  0.031368 -0.153503  0.669124  0.259073 -0.120021 -0.125852   \n...        ...       ...       ...       ...       ...       ...       ...   \n3772  0.017424 -0.003016 -0.128894  0.611507  0.282464 -0.057504 -0.036479   \n5191  0.004287 -0.003860 -0.148466  0.546357  0.317269 -0.115861 -0.020662   \n5226  0.085760  0.056135 -0.123833  0.540060  0.275112 -0.061143 -0.028153   \n5390  0.005916  0.021928 -0.156794  0.430881  0.341810 -0.103624 -0.045887   \n860   0.105165  0.112506 -0.122905  0.687118  0.410256 -0.023220 -0.069637   \n\n            f8        f9       f10  ...     s2553     s2554     s2555  \\\n3946 -0.109313  0.110457  0.240287  ... -0.178132 -0.040372 -0.132801   \n1345 -0.128297  0.069969  0.168250  ... -0.210008 -0.088108 -0.143143   \n1606 -0.081871  0.133355  0.285044  ... -0.162072 -0.044252 -0.129530   \n2377 -0.090095  0.073442  0.274556  ... -0.117398 -0.019121 -0.156043   \n1055 -0.079916  0.018769  0.267637  ... -0.166978  0.054604 -0.132995   \n...        ...       ...       ...  ...       ...       ...       ...   \n3772 -0.114189  0.057736  0.144209  ... -0.169168 -0.031656 -0.125664   \n5191 -0.094640  0.077533  0.286567  ... -0.187537 -0.120605 -0.122904   \n5226 -0.001547  0.043055  0.246456  ... -0.171929 -0.034006 -0.127080   \n5390 -0.063621  0.073945  0.141708  ... -0.173200 -0.059031 -0.139369   \n860  -0.112306  0.000387  0.280712  ... -0.123037 -0.070794 -0.147644   \n\n         s2556     s2557     s2558     s2559     s2560  gender  height_cm  \n3946 -0.227414 -0.084601 -0.234124 -0.177869  0.459008       0   0.212928  \n1345 -0.225494 -0.191922 -0.149914 -0.152593  0.163651       1  -0.072243  \n1606 -0.230123 -0.101245 -0.212125 -0.220942  0.363113       0  -0.015209  \n2377 -0.223282 -0.061029 -0.243489 -0.163225  0.325478       0  -0.235741  \n1055 -0.228669 -0.138805 -0.195539 -0.171886  0.254902       1  -0.176806  \n...        ...       ...       ...       ...       ...     ...        ...  \n3772 -0.230284 -0.148149 -0.232164 -0.184422  0.370254       0  -0.258555  \n5191 -0.143789 -0.082698 -0.216099 -0.157898  0.197296       0  -0.323194  \n5226 -0.230746 -0.077675 -0.213621 -0.185026  0.342262       0   0.235741  \n5390 -0.233169 -0.077855 -0.209909 -0.213487  0.339418       1  -0.091255  \n860  -0.234130  0.046089 -0.219538 -0.192062  0.511154       1   0.840304  \n\n[4907 rows x 5122 columns] with type DataFrame",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_54632/2184712540.py\u001b[0m in \u001b[0;36m<cell line: 243>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m                     \u001b[0;31m# train the model with sample weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m                     history = model.fit(\n\u001b[0m\u001b[1;32m    290\u001b[0m                         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0;31m# training features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tf_keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1741\u001b[0m             \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1742\u001b[0;31m             data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   1743\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tf_keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tf_keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1286\u001b[0m         \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m         self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1288\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tf_keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tf_keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    383\u001b[0m         dataset = tf.data.Dataset.zip(\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_tensors_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrom_tensors_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m     \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/data/ops/from_tensors_op.py\u001b[0m in \u001b[0;36m_from_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_from_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unused-private-name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_TensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/data/ops/from_tensors_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element, name)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34m\"\"\"See `tf.data.Dataset.from_tensors` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    109\u001b[0m         normalized_components.append(\n\u001b[0;32m--> 110\u001b[0;31m             ops.convert_to_tensor(t, name=\"component_%d\" % i))\n\u001b[0m\u001b[1;32m    111\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    712\u001b[0m   \u001b[0mpreferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreferred_dtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m   return tensor_conversion_registry.convert(\n\u001b[0m\u001b[1;32m    714\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_result_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/framework/constant_tensor_conversion.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    275\u001b[0m   \"\"\"\n\u001b[0;32m--> 276\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    277\u001b[0m                         allow_broadcast=True)\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, op, message, *args)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m     \u001b[0;34m\"\"\"Creates an `InternalError`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2097\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2098\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2099\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2099\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2100\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2101\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0. Imports\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    InputLayer, Dense, Dropout, Activation, LeakyReLU,\n",
    "    MultiHeadAttention\n",
    ")\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Simple custom LayerNorm (GPU-safe, no MKL fused op)\n",
    "# ============================================================\n",
    "class SimpleLayerNorm(tf.keras.layers.Layer):\n",
    "    def __init__(self, epsilon=1e-6, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        dim = int(input_shape[-1])\n",
    "        self.gamma = self.add_weight(\n",
    "            name=\"gamma\",\n",
    "            shape=(dim,),\n",
    "            initializer=\"ones\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.beta = self.add_weight(\n",
    "            name=\"beta\",\n",
    "            shape=(dim,),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs: (batch, features)\n",
    "        mean = tf.reduce_mean(inputs, axis=-1, keepdims=True)\n",
    "        var = tf.reduce_mean(tf.square(inputs - mean), axis=-1, keepdims=True)\n",
    "        normed = (inputs - mean) / tf.sqrt(var + self.epsilon)  # (batch, features)\n",
    "        # gamma and beta broadcast along batch dimension\n",
    "        return self.gamma * normed + self.beta\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"epsilon\": self.epsilon})\n",
    "        return config\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Transformer-style FeatureSelfAttention block\n",
    "# ============================================================\n",
    "class FeatureSelfAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Transformer-style encoder block over the feature dimension.\n",
    "\n",
    "    - Self-attention sublayer:   inputs -> MHA -> residual + LayerNorm\n",
    "    - FFN sublayer:              -> Dense -> Dense -> residual + LayerNorm\n",
    "\n",
    "    Input:  (batch, features)\n",
    "    Output: (batch, features)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads=4, key_dim=16, ff_factor=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.ff_factor = ff_factor\n",
    "        self.embed_dim = num_heads * key_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_dim = int(input_shape[-1])\n",
    "\n",
    "        # ---- Self-attention part ----\n",
    "        # Project scalar feature values to an embedding\n",
    "        self.proj_dense = Dense(self.embed_dim, use_bias=False)\n",
    "\n",
    "        self.mha = MultiHeadAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            key_dim=self.key_dim,\n",
    "        )\n",
    "\n",
    "        # Project attention output back to feature dimension\n",
    "        self.attn_out_dense = Dense(feature_dim, use_bias=False)\n",
    "\n",
    "        # LayerNorm after residual for attention sublayer\n",
    "        self.ln1 = SimpleLayerNorm()\n",
    "\n",
    "        # ---- Feed-forward part (position-wise FFN) ----\n",
    "        ff_dim = self.ff_factor * feature_dim\n",
    "        self.ffn_dense1 = Dense(\n",
    "            ff_dim,\n",
    "            activation=tf.keras.activations.gelu  # or 'relu'\n",
    "        )\n",
    "        self.ffn_dense2 = Dense(feature_dim)\n",
    "\n",
    "        # LayerNorm after residual for FFN sublayer\n",
    "        self.ln2 = SimpleLayerNorm()\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs: (batch, features)\n",
    "\n",
    "        # ====== 1) Self-attention sublayer ======\n",
    "        x = tf.expand_dims(inputs, axis=-1)             # (batch, features, 1)\n",
    "        x = self.proj_dense(x)                          # (batch, features, embed_dim)\n",
    "\n",
    "        attn = self.mha(x, x, x, training=training)     # (batch, features, embed_dim)\n",
    "        attn = self.attn_out_dense(attn)                # (batch, features, feature_dim)\n",
    "\n",
    "        # collapse embedding dimension -> (batch, features)\n",
    "        attn = tf.reduce_mean(attn, axis=-1)\n",
    "\n",
    "        # residual + layer norm\n",
    "        x1 = self.ln1(inputs + attn)                    # (batch, features)\n",
    "\n",
    "        # ====== 2) Feed-forward sublayer ======\n",
    "        f = self.ffn_dense1(x1)                         # (batch, ff_dim)\n",
    "        f = self.ffn_dense2(f)                          # (batch, features)\n",
    "\n",
    "        # residual + layer norm\n",
    "        out = self.ln2(x1 + f)                          # (batch, features)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"key_dim\": self.key_dim,\n",
    "            \"ff_factor\": self.ff_factor,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. build_model with Transformer blocks added\n",
    "# ============================================================\n",
    "def build_model(num_hidden_layers, num_neurons, activation_name,\n",
    "                learning_rate, optimizer_name, input_dim, output_dim,\n",
    "                dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    Build and compile a Keras Sequential model\n",
    "    for multi-output regression.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()                              # create empty sequential model\n",
    "\n",
    "    model.add(InputLayer(input_shape=(input_dim,)))   # add input layer with correct feature size\n",
    "\n",
    "    for layer_idx in range(num_hidden_layers):        # loop over number of hidden layers\n",
    "        model.add(Dense(num_neurons))                 # add dense layer with given number of neurons\n",
    "\n",
    "        # choose activation depending on activation_name\n",
    "        if activation_name.lower() == 'leakyrelu':    # check if activation is LeakyReLU\n",
    "            model.add(LeakyReLU(alpha=0.1))           # add LeakyReLU activation layer\n",
    "        elif activation_name.lower() == 'gelu':       # check if activation is GELU\n",
    "            model.add(Activation(tf.keras.activations.gelu))  # add GELU activation layer\n",
    "        elif activation_name.lower() == 'tanh':       # check if activation is tanh\n",
    "            model.add(Activation('tanh'))             # add tanh activation layer\n",
    "        else:                                         # if activation name is unknown\n",
    "            raise ValueError(\"Unknown activation: {}\".format(activation_name))  # raise error\n",
    "\n",
    "        model.add(Dropout(dropout_rate))              # add dropout layer for regularization\n",
    "\n",
    "        # >>> TRANSFORMER BLOCK over features (attention + FFN) <<<\n",
    "        model.add(\n",
    "            FeatureSelfAttention(\n",
    "                num_heads=4,\n",
    "                key_dim=16,\n",
    "                ff_factor=2,                          # can tune (e.g., 2â€“4)\n",
    "                name=f\"feature_self_attention_{layer_idx+1}\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    model.add(Dense(output_dim, activation='linear')) # add output layer (linear for regression)\n",
    "\n",
    "    # choose optimizer based on name\n",
    "    if optimizer_name.lower() == 'sgd':               # if optimizer is SGD\n",
    "        optimizer = SGD(learning_rate=learning_rate, momentum=0.9)  # create SGD with momentum\n",
    "    elif optimizer_name.lower() == 'adam':            # if optimizer is Adam\n",
    "        optimizer = Adam(learning_rate=learning_rate) # create Adam optimizer\n",
    "    elif optimizer_name.lower() == 'rmsprop':         # if optimizer is RMSprop\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)  # create RMSprop optimizer\n",
    "    else:                                             # if optimizer name is unknown\n",
    "        raise ValueError(\"Unknown optimizer: {}\".format(optimizer_name))  # raise error\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,          # set chosen optimizer\n",
    "        loss='mse',                   # use mean squared error as base loss (will use sample_weight)\n",
    "        metrics=[],                   # no unweighted metrics\n",
    "        weighted_metrics=[            # metrics that MUST use sample_weight\n",
    "            tf.keras.metrics.MeanSquaredError(name='mse')  # weighted MSE metric called \"mse\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model                      # return compiled model\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Hyper-parameter grid\n",
    "# ============================================================\n",
    "num_hidden_layers_list = [3]                    # options for number of hidden layers\n",
    "num_neurons_list = [512]                         # options for neurons per hidden layer\n",
    "activation_list = ['gelu']                      # options for activation functions\n",
    "learning_rates = [1e-3]                         # options for learning rates\n",
    "optimizer_list = ['adam']                       # options for optimizers\n",
    "\n",
    "batch_size = 200                                # batch size for training\n",
    "num_epochs = 300                                # maximum number of epochs\n",
    "\n",
    "# compute total number of hyper-parameter combinations\n",
    "total_combinations = (len(num_hidden_layers_list) *\n",
    "                      len(num_neurons_list) *\n",
    "                      len(activation_list) *\n",
    "                      len(learning_rates) *\n",
    "                      len(optimizer_list))\n",
    "\n",
    "print(\"\\nTotal hyper-parameter combinations:\", total_combinations)  # print total number of combos\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. Grid search loop\n",
    "# (X_train, Y_train, X_val, Y_val, w_train, w_val must be defined above)\n",
    "# ============================================================\n",
    "best_val_loss = np.inf                   # initialize best validation loss with infinity\n",
    "best_history = None                      # variable to store history of best model\n",
    "best_params = None                       # variable to store best hyper-parameters\n",
    "best_model = None                        # variable to store best trained model\n",
    "\n",
    "combo_counter = 0                        # counter to track which combo we are training\n",
    "\n",
    "input_dim = X_train.shape[1]             # number of input features\n",
    "output_dim = Y_train.shape[1]            # number of output targets\n",
    "\n",
    "# loop over all hyper-parameter combinations\n",
    "for num_hidden_layers in num_hidden_layers_list:              # loop over hidden layer counts\n",
    "    for num_neurons in num_neurons_list:                      # loop over neuron counts\n",
    "        for activation_name in activation_list:               # loop over activations\n",
    "            for lr in learning_rates:                         # loop over learning rates\n",
    "                for optimizer_name in optimizer_list:         # loop over optimizers\n",
    "\n",
    "                    combo_counter += 1                        # increase combination counter\n",
    "\n",
    "                    print(\"\\n======================================\")\n",
    "                    print(f\"Training combination {combo_counter}/{total_combinations}\")  # show combo index\n",
    "                    print(f\"Hidden layers: {num_hidden_layers}\")                          # print layer count\n",
    "                    print(f\"Neurons per layer: {num_neurons}\")                            # print neuron count\n",
    "                    print(f\"Activation: {activation_name}\")                               # print activation\n",
    "                    print(f\"Learning rate: {lr}\")                                         # print learning rate\n",
    "                    print(f\"Optimizer: {optimizer_name}\")                                 # print optimizer\n",
    "                    print(\"======================================\")\n",
    "\n",
    "                    # build model with current hyper-parameters\n",
    "                    model = build_model(num_hidden_layers,\n",
    "                                         num_neurons,\n",
    "                                         activation_name,\n",
    "                                         lr,\n",
    "                                         optimizer_name,\n",
    "                                         input_dim,\n",
    "                                         output_dim)\n",
    "\n",
    "                    # define early stopping callback\n",
    "                    early_stop = EarlyStopping(\n",
    "                        monitor='val_loss',           # watch validation loss\n",
    "                        patience=10,                  # stop if no improvement for 10 epochs\n",
    "                        restore_best_weights=True,    # restore weights from best epoch\n",
    "                        verbose=1                     # print message when stopping\n",
    "                    )\n",
    "\n",
    "                    # define learning rate scheduler callback\n",
    "                    lr_scheduler = ReduceLROnPlateau(\n",
    "                        monitor='loss',               # watch training loss\n",
    "                        factor=0.5,                   # reduce learning rate by factor 0.5\n",
    "                        patience=5,                   # wait 5 epochs before reducing LR\n",
    "                        min_lr=1e-6,                  # do not go below this LR\n",
    "                        verbose=1                     # print message when LR changes\n",
    "                    )\n",
    "\n",
    "                    callbacks = [early_stop, lr_scheduler]  # list of callbacks to use\n",
    "\n",
    "                    # train the model with sample weights\n",
    "                    history = model.fit(\n",
    "                        X_train,                     # training features\n",
    "                        Y_train,                     # training targets\n",
    "                        sample_weight=w_train,       # sample weights for training data\n",
    "                        validation_data=(X_val, Y_val, w_val),  # validation data + weights\n",
    "                        epochs=num_epochs,           # maximum number of epochs\n",
    "                        batch_size=batch_size,       # batch size\n",
    "                        callbacks=callbacks,         # callbacks list\n",
    "                        verbose=1                    # show training progress per epoch\n",
    "                    )\n",
    "\n",
    "                    # compute minimum validation loss for this model\n",
    "                    min_val_loss = min(history.history['val_loss'])\n",
    "\n",
    "                    print(f\"Finished combination {combo_counter}. \"\n",
    "                          f\"Best val_loss for this model: {min_val_loss:.6f}\")  # print best val_loss\n",
    "\n",
    "                    # check if this model is better than previous best\n",
    "                    if min_val_loss < best_val_loss:\n",
    "                        print(\">>> New best model found! Updating best model ...\")  # notify improvement\n",
    "                        best_val_loss = min_val_loss          # update best validation loss\n",
    "                        best_history = history                # save training history\n",
    "                        best_params = {                       # save best hyper-parameters\n",
    "                            'num_hidden_layers': num_hidden_layers,\n",
    "                            'num_neurons': num_neurons,\n",
    "                            'activation': activation_name,\n",
    "                            'learning_rate': lr,\n",
    "                            'optimizer': optimizer_name\n",
    "                        }\n",
    "                        best_model = model                    # save best model\n",
    "\n",
    "# ============================================================\n",
    "# 6. Show best hyper-parameter combination\n",
    "# ============================================================\n",
    "print(\"\\n================ BEST MODEL =================\")   # header for best model\n",
    "print(\"Best validation loss:\", best_val_loss)            # print best validation loss\n",
    "print(\"Best hyper-parameters:\")                          # header for params\n",
    "for k, v in best_params.items():                         # loop over param dict\n",
    "    print(f\"  {k}: {v}\")                                 # print each param\n",
    "print(\"=============================================\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. Plot loss and val_loss curves for best model\n",
    "# ============================================================\n",
    "print(\"\\nPlotting loss and val_loss curves for the best model ...\")  # status message\n",
    "\n",
    "plt.figure(figsize=(8, 5))                               # create a new figure\n",
    "plt.plot(best_history.history['loss'], label='Train loss')      # plot training loss\n",
    "plt.plot(best_history.history['val_loss'], label='Validation loss')  # plot validation loss\n",
    "plt.xlabel('Epoch')                                     # label x-axis\n",
    "plt.ylabel('Loss (MSE)')                               # label y-axis\n",
    "plt.title('Training and Validation Loss for Best Model') # set plot title\n",
    "plt.legend()                                           # show legend\n",
    "plt.grid(True)                                         # add grid lines\n",
    "plt.tight_layout()                                     # adjust layout\n",
    "plt.show()                                             # display plot\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. Save the final best model\n",
    "# ============================================================\n",
    "print(\"\\nSaving best model to 'eff_ANN_Tr_v0.h5' ...\")       # status message\n",
    "\n",
    "best_model.save('eff_ANN_Tr_v0.h5')                          # save best model to H5 file\n",
    "\n",
    "print(\"Model saved as 'eff_ANN_Tr_v0.h5'. Done!\")            # final confirmation message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857b5d17-e9bb-402a-b96d-f0964e47485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3                    # AWS SDK for Python\n",
    "import os                       # to check file paths\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Local model file info\n",
    "# -----------------------------\n",
    "local_model_path = \"eff_ANN_Tr_v0.h5\"   # path where you saved the model\n",
    "\n",
    "# check that the file exists before uploading\n",
    "if not os.path.exists(local_model_path):\n",
    "    raise FileNotFoundError(f\"Model file not found: {local_model_path}\")\n",
    "\n",
    "print(f\"Found model file: {local_model_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. S3 bucket and key info\n",
    "# -----------------------------\n",
    "bucket_name = \"ai-bmi-predictor\"  # your S3 bucket name\n",
    "\n",
    "# S3 key (path inside the bucket)\n",
    "s3_key = \"trained-models/efficientnet-models/eff_ANN_Tr_v0.h5\"\n",
    "\n",
    "print(f\"Model will be uploaded to s3://{bucket_name}/{s3_key}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Create S3 client\n",
    "# -----------------------------\n",
    "# This assumes your AWS credentials are configured (env vars, ~/.aws/credentials, or IAM role)\n",
    "s3_client = boto3.client(\"s3\")    # create S3 client\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Upload the file\n",
    "# -----------------------------\n",
    "print(\"Uploading model to S3 ...\")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    Filename=local_model_path,    # local file path\n",
    "    Bucket=bucket_name,           # target S3 bucket\n",
    "    Key=s3_key                    # target S3 key (including folders and filename)\n",
    ")\n",
    "\n",
    "print(\"Upload completed successfully!\")\n",
    "print(f\"File is now at: s3://{bucket_name}/{s3_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed6d84a-e81b-42db-8f9b-0f8028f51162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
