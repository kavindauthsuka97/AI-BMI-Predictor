{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e4b6615-0c14-44bb-b466-ebdc3c05fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a13679a6-4340-4289-83e4-4738faec1b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# -------------------------\n",
    "# RunPod S3 Credentials\n",
    "# -------------------------\n",
    "ACCESS_KEY = \"user_37sKcYrvnk9UXaIY3B3Zr90MH0g\"\n",
    "SECRET_KEY = \"rps_YW72UMRXEMRVC8A407OCL08J8G34U1B3QTNO1ETX18pa1n\"\n",
    "BUCKET = \"e9tcw5eupu\"\n",
    "\n",
    "ENDPOINT_URL = \"https://s3api-eu-ro-1.runpod.io\"\n",
    "REGION = \"eu-ro-1\"  # required but not strictly enforced\n",
    "\n",
    "# -------------------------\n",
    "# S3 Client (IMPORTANT PART)\n",
    "# -------------------------\n",
    "s3 = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=ACCESS_KEY,\n",
    "    aws_secret_access_key=SECRET_KEY,\n",
    "    endpoint_url=ENDPOINT_URL,\n",
    "    region_name=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d190d08e-5ad1-4441-93ed-309777c7c993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¦ Bucket: e9tcw5eupu\n",
      "\n",
      "ðŸ“‚ data/  (355.24 MB)\n",
      "   â””â”€â”€ data/eff_training.csv  [355.24 MB]\n",
      "ðŸ“‚ efficientnet/  (261.71 MB)\n",
      "   â””â”€â”€ efficientnet/eff_ann_version8.h5  [15.08 MB]\n",
      "   â””â”€â”€ efficientnet/efficientnetb7_feature_extractor.h5  [246.63 MB]\n",
      "ðŸ“‚ sapiens/  (8.11 GB)\n",
      "   â””â”€â”€ sapiens/sapiens_2b_goliath_best_goliath_mIoU_8179_epoch_181_torchscript.pt2  [8.11 GB]\n",
      "ðŸ“‚ scalers/  (119.96 KB)\n",
      "   â””â”€â”€ scalers/scaler_robust_features.pkl  [118.36 KB]\n",
      "   â””â”€â”€ scalers/scaler_standard_features.pkl  [593 B]\n",
      "   â””â”€â”€ scalers/scaler_targets.pkl  [1.02 KB]\n",
      "ðŸ“‚ training-artifacts/  (166.35 KB)\n",
      "   â””â”€â”€ training-artifacts/IQR.pkl  [82.87 KB]\n",
      "   â””â”€â”€ training-artifacts/height_mean.pkl  [21 B]\n",
      "   â””â”€â”€ training-artifacts/meadian.pkl  [82.87 KB]\n",
      "   â””â”€â”€ training-artifacts/res_mean.pkl  [291 B]\n",
      "   â””â”€â”€ training-artifacts/res_std.pkl  [291 B]\n",
      "   â””â”€â”€ training-artifacts/std_height.pkl  [21 B]\n",
      "\n",
      "==================================================\n",
      "TOTAL SIZE: 8.71 GB\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def human_size(bytes_size):\n",
    "    if bytes_size >= 1024**3:\n",
    "        return f\"{bytes_size / 1024**3:.2f} GB\"\n",
    "    elif bytes_size >= 1024**2:\n",
    "        return f\"{bytes_size / 1024**2:.2f} MB\"\n",
    "    elif bytes_size >= 1024:\n",
    "        return f\"{bytes_size / 1024:.2f} KB\"\n",
    "    return f\"{bytes_size} B\"\n",
    "\n",
    "\n",
    "def list_tree_with_sizes(bucket):\n",
    "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "\n",
    "    tree = defaultdict(list)\n",
    "    folder_sizes = defaultdict(int)\n",
    "    total_size = 0\n",
    "\n",
    "    for page in paginator.paginate(Bucket=bucket):\n",
    "        for obj in page.get(\"Contents\", []):\n",
    "            key = obj[\"Key\"]\n",
    "            size = obj[\"Size\"]\n",
    "\n",
    "            top_folder = key.split(\"/\")[0]\n",
    "            tree[top_folder].append((key, size))\n",
    "\n",
    "            folder_sizes[top_folder] += size\n",
    "            total_size += size\n",
    "\n",
    "    # PRINT TREE\n",
    "    print(f\"\\nðŸ“¦ Bucket: {bucket}\\n\")\n",
    "\n",
    "    for folder, files in sorted(tree.items()):\n",
    "        print(f\"ðŸ“‚ {folder}/  ({human_size(folder_sizes[folder])})\")\n",
    "        for key, size in files:\n",
    "            print(f\"   â””â”€â”€ {key}  [{human_size(size)}]\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"TOTAL SIZE: {human_size(total_size)}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "list_tree_with_sizes(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d36d0b-a0c6-4358-ac0f-0ff921b53cad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
