{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b7c651-6cc7-4da4-b3b6-3f045fc7f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "bucket = \"ai-bmi-predictor\"\n",
    "key = \"test-data/eff_testingB.csv\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "\n",
    "data = pd.read_csv(obj[\"Body\"])\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e29a2be-ade8-4379-8b03-dce1bd73aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Volume feature engineering\n",
    "# =========================\n",
    "\n",
    "import boto3  # AWS SDK for Python (S3 access)\n",
    "import numpy as np  # fast array/mask operations\n",
    "import pandas as pd  # dataframe ops\n",
    "from PIL import Image  # read PNG images\n",
    "from io import BytesIO  # convert S3 bytes -> file-like object\n",
    "from botocore.exceptions import ClientError  # catch S3 missing-key errors\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG (edit if needed)\n",
    "# -------------------------\n",
    "\n",
    "MASK_BUCKET = \"amazon-bodym\"            # S3 bucket that contains mask images\n",
    "FRONT_MASK_PREFIX = \"testB/mask/\"      # front masks folder (from your 1st screenshot)\n",
    "SIDE_MASK_PREFIX = \"testB/mask_left/\"  # side masks folder (from your 2nd screenshot)\n",
    "PHOTO_ID_COL = \"photo_id\"              # column in your CSV that maps to mask filenames\n",
    "OUTPUT_COL = \"volume\"                  # name for the new feature column\n",
    "\n",
    "# -------------------------\n",
    "# S3 client (reused)\n",
    "# -------------------------\n",
    "\n",
    "print(\"Creating S3 client...\")  # track progress\n",
    "s3 = boto3.client(\"s3\")  # create an S3 client\n",
    "\n",
    "# -----------------------------------------\n",
    "# Helper: standardize photo_id -> filename\n",
    "# -----------------------------------------\n",
    "\n",
    "print(\"Inferring photo_id padding length...\")  # track progress\n",
    "photo_id_series = data[PHOTO_ID_COL].astype(str).fillna(\"\")  # ensure string + no NaN\n",
    "photo_id_series = photo_id_series.str.strip()  # remove whitespace\n",
    "photo_id_series = photo_id_series.str.replace(\".png\", \"\", regex=False)  # drop extension if present\n",
    "photo_id_series = photo_id_series.str.split(\".\", n=1).str[0]  # drop trailing \".0\" etc (common if numeric CSV)\n",
    "pad_len = int(photo_id_series.str.len().max()) if len(photo_id_series) else 0  # infer max length\n",
    "pad_len = max(pad_len, 4)  # default to 4 (matches your example like \"0021\")\n",
    "print(f\"Using zero-pad length = {pad_len}\")  # show chosen padding\n",
    "\n",
    "def normalize_photo_id(photo_id, pad_length):  # define normalizer\n",
    "    pid = str(photo_id).strip()  # convert to string and trim spaces\n",
    "    pid = pid.replace(\".png\", \"\")  # remove .png if included\n",
    "    pid = pid.split(\".\", 1)[0]  # remove any trailing decimals (e.g., \"21.0\" -> \"21\")\n",
    "    if pid.isdigit():  # only pad if it's purely digits\n",
    "        pid = pid.zfill(pad_length)  # left-pad with zeros\n",
    "    return pid  # return normalized id like \"0021\"\n",
    "\n",
    "# -----------------------------------------\n",
    "# Helper: read mask PNG from S3 into boolean\n",
    "# -----------------------------------------\n",
    "\n",
    "def load_mask_bool_from_s3(bucket, key):  # function to load a mask file\n",
    "    print(f\"    Downloading: s3://{bucket}/{key}\")  # track downloads\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)  # fetch object bytes from S3\n",
    "    img = Image.open(BytesIO(obj[\"Body\"].read()))  # open image from bytes\n",
    "    img = img.convert(\"L\")  # convert to grayscale\n",
    "    arr = np.array(img)  # convert to numpy array\n",
    "    mask_bool = arr > 0  # treat any non-zero pixel as \"body\"\n",
    "    return mask_bool  # return boolean mask\n",
    "\n",
    "# -----------------------------------------\n",
    "# Helper: compute mean width across rows\n",
    "# -----------------------------------------\n",
    "\n",
    "def mean_row_width(mask_bool):  # compute average body width across horizontal rows\n",
    "    if mask_bool.size == 0:  # guard: empty image\n",
    "        return np.nan  # cannot compute\n",
    "    row_has_body = mask_bool.any(axis=1)  # rows where body exists\n",
    "    if not row_has_body.any():  # guard: no body pixels\n",
    "        return np.nan  # cannot compute\n",
    "    first = mask_bool.argmax(axis=1)  # first True index per row (0 if none)\n",
    "    last = mask_bool.shape[1] - 1 - np.flip(mask_bool, axis=1).argmax(axis=1)  # last True index per row\n",
    "    widths = (last - first + 1).astype(float)  # width per row (in pixels)\n",
    "    widths[~row_has_body] = np.nan  # ignore rows with no body\n",
    "    return float(np.nanmean(widths))  # average width across valid rows\n",
    "\n",
    "# -----------------------------------------\n",
    "# Helper: compute height from front mask\n",
    "# -----------------------------------------\n",
    "\n",
    "def body_height(mask_bool):  # compute vertical extent (top-to-bottom)\n",
    "    if mask_bool.size == 0:  # guard: empty image\n",
    "        return np.nan  # cannot compute\n",
    "    ys, xs = np.where(mask_bool)  # get coordinates of body pixels\n",
    "    if ys.size == 0:  # guard: no body pixels\n",
    "        return np.nan  # cannot compute\n",
    "    h = (ys.max() - ys.min() + 1).astype(float)  # height in pixels\n",
    "    return float(h)  # return height\n",
    "\n",
    "# -----------------------------------------\n",
    "# Per-record feature computation\n",
    "# -----------------------------------------\n",
    "\n",
    "def compute_volume_for_photo_id(photo_id):  # compute volume + debug parts for one record\n",
    "    pid = normalize_photo_id(photo_id, pad_len)  # normalize id to match PNG filename\n",
    "    front_key = f\"{FRONT_MASK_PREFIX}{pid}.png\"  # build front mask key\n",
    "    side_key = f\"{SIDE_MASK_PREFIX}{pid}.png\"  # build side mask key\n",
    "\n",
    "    try:\n",
    "        front_mask = load_mask_bool_from_s3(MASK_BUCKET, front_key)  # load front mask\n",
    "    except ClientError as e:  # handle missing front file\n",
    "        print(f\"    ERROR: missing front mask for {pid}: {e}\")  # print error\n",
    "        return np.nan, np.nan, np.nan, np.nan  # return NaNs\n",
    "\n",
    "    try:\n",
    "        side_mask = load_mask_bool_from_s3(MASK_BUCKET, side_key)  # load side mask\n",
    "    except ClientError as e:  # handle missing side file\n",
    "        print(f\"    ERROR: missing side mask for {pid}: {e}\")  # print error\n",
    "        return np.nan, np.nan, np.nan, np.nan  # return NaNs\n",
    "\n",
    "    front_w = mean_row_width(front_mask)  # front width (body looks wide from front)\n",
    "    side_w = mean_row_width(side_mask)  # side width (body looks thick from side)\n",
    "    avg_w = (front_w + side_w) / 2.0  # average width as instructed\n",
    "    h = body_height(front_mask)  # height from front mask (vertical extent)\n",
    "\n",
    "    raw_volume = avg_w * h  # volume ≈ average width × height\n",
    "    norm_volume = raw_volume / (h ** 2) if (h is not None and not np.isnan(h) and h > 0) else np.nan  # normalize\n",
    "\n",
    "    return norm_volume, front_w, side_w, h  # return volume + components for debugging\n",
    "\n",
    "# -----------------------------------------\n",
    "# Run across the dataframe and add column(s)\n",
    "# -----------------------------------------\n",
    "\n",
    "print(\"Starting volume computation for all records...\")  # track progress\n",
    "volumes = []  # store normalized volumes\n",
    "front_ws = []  # store front widths (debug)\n",
    "side_ws = []  # store side widths (debug)\n",
    "heights = []  # store heights (debug)\n",
    "\n",
    "n = len(data)  # total rows\n",
    "print(f\"Total rows to process: {n}\")  # show total\n",
    "\n",
    "for i, photo_id in enumerate(data[PHOTO_ID_COL].tolist(), start=1):  # iterate through photo_ids\n",
    "    print(f\"\\nRecord {i}/{n} | photo_id={photo_id}\")  # show per-record progress\n",
    "    v, fw, sw, h = compute_volume_for_photo_id(photo_id)  # compute features\n",
    "    print(f\"    front_width={fw} | side_width={sw} | height={h} | volume={v}\")  # print computed values\n",
    "    volumes.append(v)  # append volume\n",
    "    front_ws.append(fw)  # append front width\n",
    "    side_ws.append(sw)  # append side width\n",
    "    heights.append(h)  # append height\n",
    "\n",
    "    if i % 50 == 0:  # every 50 rows\n",
    "        print(f\"\\nProcessed {i}/{n} records so far...\")  # periodic progress update\n",
    "\n",
    "print(\"\\nAttaching new feature columns to dataframe...\")  # track progress\n",
    "data[OUTPUT_COL] = volumes  # add normalized volume feature\n",
    "data[\"front_width\"] = front_ws  # optional debug column\n",
    "data[\"side_width\"] = side_ws  # optional debug column\n",
    "data[\"mask_height\"] = heights  # optional debug column\n",
    "\n",
    "print(\"Done.\")  # final status\n",
    "print(\"Preview of updated dataframe:\")  # show preview message\n",
    "print(data[[PHOTO_ID_COL, OUTPUT_COL, \"front_width\", \"side_width\", \"mask_height\"]].head())  # preview new columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc18b52-f49b-46f0-aa92-d64868aa6a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the intermediate/debug columns (ignore if they don't exist)\n",
    "cols_to_drop = [\"front_width\", \"side_width\", \"mask_height\"]  # columns to remove\n",
    "print(\"Dropping columns:\", cols_to_drop)  # track process\n",
    "data.drop(columns=cols_to_drop, inplace=True, errors=\"ignore\")  # drop safely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138a041b-c9a6-44ca-b000-08c40ee8a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa39b314-790e-4c4f-9deb-f714b554a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3  # S3 client\n",
    "from io import StringIO  # in-memory text buffer\n",
    "\n",
    "print(\"Preparing to upload updated dataset to S3...\")  # track process\n",
    "\n",
    "s3_out_path = \"s3://ai-bmi-predictor/test-data/eff_testingB_v2.csv\"  # target S3 path\n",
    "print(\"Target:\", s3_out_path)  # show target\n",
    "\n",
    "# ---- parse s3://bucket/key ----\n",
    "out_bucket = s3_out_path.replace(\"s3://\", \"\").split(\"/\", 1)[0]  # extract bucket\n",
    "out_key = s3_out_path.replace(\"s3://\", \"\").split(\"/\", 1)[1]     # extract key\n",
    "print(f\"Parsed -> bucket={out_bucket}, key={out_key}\")  # confirm parsing\n",
    "\n",
    "# ---- write CSV to memory ----\n",
    "csv_buffer = StringIO()  # create in-memory buffer\n",
    "print(\"Serializing dataframe to CSV (in-memory)...\")  # track process\n",
    "data.to_csv(csv_buffer, index=False)  # write dataframe as CSV text\n",
    "csv_body = csv_buffer.getvalue()  # get CSV string content\n",
    "print(f\"CSV size (chars): {len(csv_body):,}\")  # print rough size\n",
    "\n",
    "# ---- upload to S3 ----\n",
    "s3 = boto3.client(\"s3\")  # create S3 client\n",
    "print(\"Uploading to S3...\")  # track process\n",
    "s3.put_object(\n",
    "    Bucket=out_bucket,                 # destination bucket\n",
    "    Key=out_key,                       # destination key\n",
    "    Body=csv_body.encode(\"utf-8\"),     # file bytes\n",
    "    ContentType=\"text/csv\"             # content type\n",
    ")  # upload\n",
    "\n",
    "print(\"Upload complete ✅\")  # done\n",
    "print(f\"Saved to: {s3_out_path}\")  # confirm final path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5e72ef-6900-442e-8101-e1a298080e04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
